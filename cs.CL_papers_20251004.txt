1: Information Seeking for Robust Decision Making under Partial  Observability
Authors: ['Djengo Cyun-Jyun Fang', 'Tsung-Wei Ke']
Summary: Explicit information seeking is essential to human problem-solving inpractical environments characterized by incomplete information and noisydynamics. When the true environmental state is not directly observable, humansseek information to update their internal dynamics and inform futuredecision-making. Although existing Large Language Model (LLM) planning agentshave addressed observational uncertainty, they often overlook discrepanciesbetween their internal dynamics and the actual environment. We introduceInformation Seeking Decision Planner (InfoSeeker), an LLM decision-makingframework that integrates task-oriented planning with information seeking toalign internal dynamics and make optimal decisions under uncertainty in bothagent observations and environmental dynamics. InfoSeeker prompts an LLM toactively gather information by planning actions to validate its understanding,detect environmental changes, or test hypotheses before generating or revisingtask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmarksuite featuring partially observable environments with incomplete observationsand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%absolute performance gain over prior methods without sacrificing sampleefficiency. Moreover, InfoSeeker generalizes across LLMs and outperformsbaselines on established benchmarks such as robotic manipulation and webnavigation. These findings underscore the importance of tightly integratingplanning and information seeking for robust behavior in partially observableenvironments. The project page is available at https://infoseekerllm.github.io
摘要: 在信息不完整且动态嘈杂的实际环境中，显式信息获取对人类解决问题至关重要。当真实环境状态无法直接观测时，人类会通过获取信息来更新其内部动态，并为未来决策提供依据。尽管现有的大型语言模型（LLM）规划智能体已解决了观测不确定性问题，但它们往往忽略了内部动态与实际环境之间的差异。我们提出了信息获取决策规划器（InfoSeeker），这是一种LLM决策框架，它将面向任务的规划与信息获取相结合，以在智能体观测和环境动态均存在不确定性的情况下，对齐内部动态并做出最优决策。InfoSeeker通过规划行动来提示LLM主动收集信息，以验证其理解、检测环境变化或测试假设，然后再生成或修订面向任务的计划。为评估InfoSeeker，我们引入了一个新颖的基准测试套件，其特点是具有不完整观测和不确定动态的部分可观测环境。实验表明，InfoSeeker在保持样本效率的同时，比先前方法实现了74%的绝对性能提升。此外，InfoSeeker能够跨LLM泛化，并在机器人操作和网络导航等既定基准上优于基线模型。这些发现强调了在部分可观测环境中，将规划与信息获取紧密集成对于实现稳健行为的重要性。项目页面可在https://infoseekerllm.github.io获取。
Link: http://arxiv.org/abs/2510.01531v1
Updated: 2025-10-02T00:06:32Z

2: InvThink: Towards AI Safety via Inverse Reasoning
Authors: ['Yubin Kim', 'Taehan Kim', 'Eugene Park', 'Chunjong Park', 'Cynthia Breazeal', 'Daniel McDuff', 'Hae Won Park']
Summary: We present InvThink, a simple yet powerful approach that gives large languagemodels (LLMs) the capability of inverse thinking: reasoning through failuremodes before generating responses. Unlike existing safety alignment methodsthat optimize directly for safe response, InvThink instructs models to 1)enumerate potential harms, 2) analyze their consequences, and 3) generate safeoutputs that proactively avoid these risks. Our method reveals three keyfindings: (i) safety improvements show stronger scaling with model sizecompared to existing safety methods. (ii) InvThink mitigates safety tax; bytraining models to systematically consider failure modes, it preserves generalreasoning capabilities on standard benchmarks. (iii) beyond general safetytasks, InvThink excels in high-stakes domains including external-facing(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,achieving up to 15.7% reduction in harmful responses compared to baselinemethods like SafetyPrompt. We further implement InvThink via supervisedfine-tuning, and reinforcement learning across three LLM families. Theseresults suggest that inverse reasoning provides a scalable and generalizablepath toward safer, more capable language models.
摘要: 我们提出了InvThink，这是一种简单而强大的方法，它赋予大型语言模型（LLMs）逆向思考的能力：在生成响应之前通过失效模式进行推理。与现有直接优化安全响应的安全对齐方法不同，InvThink指示模型1）枚举潜在危害，2）分析其后果，以及3）生成主动避免这些风险的安全输出。我们的方法揭示了三个关键发现：（i）与现有安全方法相比，安全性改进随模型规模呈现出更强的扩展性。（ii）InvThink减轻了安全税；通过训练模型系统地考虑失效模式，它保留了在标准基准上的通用推理能力。（iii）除了通用安全任务外，InvThink在高风险领域表现出色，包括外部导向（医学、金融、法律）和代理（勒索、谋杀）风险场景，与SafetyPrompt等基线方法相比，有害响应减少了高达15.7%。我们进一步通过监督微调以及跨三个大型语言模型系列的强化学习实现了InvThink。这些结果表明，逆向推理为构建更安全、更强大的语言模型提供了一条可扩展且可泛化的路径。
Link: http://arxiv.org/abs/2510.01569v1
Updated: 2025-10-02T01:26:53Z

3: Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query  Autocomplete
Authors: ['Adithya Rajan', 'Xiaoyu Liu', 'Prateek Verma', 'Vibhu Arora']
Summary: We introduce a data-centric approach for mitigating presentation bias inreal-time neural query autocomplete systems through the use of syntheticprefixes. These prefixes are generated from complete user queries collectedduring regular search sessions where autocomplete was not active. This allowsus to enrich the training data for learning to rank models with more diverseand less biased examples. This method addresses the inherent bias in engagementsignals collected from live query autocomplete interactions, where modelsuggestions influence user behavior. Our neural ranker is optimized forreal-time deployment under strict latency constraints and incorporates a richset of features, including query popularity, seasonality, fuzzy match scores,and contextual signals such as department affinity, device type, and verticalalignment with previous user queries. To support efficient training, weintroduce a task-specific simplification of the listwise loss, reducingcomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the queryautocomplete structure of having only one ground-truth selection per prefix.Deployed in a large-scale e-commerce setting, our system demonstratesstatistically significant improvements in user engagement, as measured by meanreciprocal rank and related metrics. Our findings show that synthetic prefixesnot only improve generalization but also provide a scalable path toward biasmitigation in other low-latency ranking tasks, including related searches andquery recommendations.
摘要: 我们提出了一种以数据为中心的方法，通过使用合成前缀来缓解实时神经查询自动补全系统中的呈现偏差。这些前缀是从自动补全功能未激活的常规搜索会话中收集的完整用户查询生成的。这使我们能够用更多样化且偏差更小的示例来丰富学习排序模型的训练数据。该方法解决了从实时查询自动补全交互中收集的参与信号中固有的偏差，其中模型建议会影响用户行为。我们的神经排序器针对严格延迟约束下的实时部署进行了优化，并整合了丰富的特征集，包括查询流行度、季节性、模糊匹配分数以及上下文信号，如部门亲和力、设备类型和与先前用户查询的垂直对齐。为支持高效训练，我们引入了一种针对任务的列表式损失简化方法，通过利用每个前缀仅有一个真实选择的查询自动补全结构，将计算复杂度从$O(n^2)$降至$O(n)$。在大规模电子商务环境中部署后，我们的系统在用户参与度方面表现出统计显著的改进，这通过平均倒数排名和相关指标进行衡量。我们的研究结果表明，合成前缀不仅提升了泛化能力，还为在其他低延迟排序任务（包括相关搜索和查询推荐）中实现偏差缓解提供了可扩展的路径。
Link: http://arxiv.org/abs/2510.01574v1
Updated: 2025-10-02T01:44:44Z

4: Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,  Attentive Compression
Authors: ['Joykirat Singh', 'Justin Chih-Yao Chen', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Akshay Nambi', 'Mohit Bansal']
Summary: Recent thinking models solve complex reasoning tasks by scaling test-timecompute, but this scaling must be allocated in line with task difficulty. Onone hand, short reasoning (underthinking) leads to errors on harder problemsthat require extended reasoning steps; but, excessively long reasoning(overthinking) can be token-inefficient, generating unnecessary steps evenafter reaching a correct intermediate solution. We refer to this asunder-adaptivity, where the model fails to modulate its response lengthappropriately given problems of varying difficulty. To address under-adaptivityand strike a balance between under- and overthinking, we propose TRAAC (ThinkRight with Adaptive, Attentive Compression), an online post-training RL methodthat leverages the model's self-attention over a long reasoning trajectory toidentify important steps and prune redundant ones. TRAAC also estimatesdifficulty and incorporates it into training rewards, thereby learning toallocate reasoning budget commensurate with example difficulty. Our approachimproves accuracy, reduces reasoning steps, and enables adaptive thinkingcompared to base models and other RL baselines. Across a variety of tasks(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absoluteaccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%compared to the base model, and a 7.9% accuracy gain paired with a 29.4% lengthdrop compared to the best RL baseline. TRAAC also shows strong generalization:although our models are trained on math datasets, they show accuracy andefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,and OptimalThinkingBench. Our analysis further verifies that TRAAC providesfine-grained adjustments to thinking budget based on difficulty and that acombination of task-difficulty calibration and attention-based compressionyields gains across diverse tasks.
摘要: 近期的思维模型通过扩展测试时计算来解决复杂推理任务，但这种扩展必须与任务难度相匹配。一方面，过短的推理（思考不足）会导致在需要更多推理步骤的难题上出错；但，过长的推理（思考过度）则在token效率上低下，甚至在得出正确中间解后仍会生成不必要的步骤。我们称之为“适应性不足”，即模型在应对不同难度问题时，无法适当调整其响应长度。为解决适应性不足并在思考不足与思考过度之间取得平衡，我们提出了TRAAC（通过自适应、注意力压缩实现正确思考），这是一种在线后训练强化学习方法，利用模型在长推理轨迹上的自注意力机制来识别重要步骤并修剪冗余步骤。TRAAC还估算任务难度并将其纳入训练奖励，从而学会分配与示例难度相称的推理预算。与基础模型及其他强化学习基线相比，我们的方法提升了准确性、减少了推理步骤，并实现了自适应思维。在多项任务（AIME、AMC、GPQA-D、BBEH）中，TRAAC（Qwen3-4B）相较于基础模型平均绝对准确率提升8.4%，推理长度相对减少36.8%；相较于最佳强化学习基线，准确率提升7.9%，推理长度减少29.4%。TRAAC还展现出强大的泛化能力：尽管我们的模型在数学数据集上训练，但在分布外的非数学数据集（如GPQA-D、BBEH及OptimalThinkingBench）上仍实现了准确率和效率的提升。我们的分析进一步验证，TRAAC能基于难度对思维预算进行细粒度调整，并且任务难度校准与基于注意力的压缩相结合，在多样化任务中均能带来增益。
Link: http://arxiv.org/abs/2510.01581v1
Updated: 2025-10-02T02:00:20Z

5: ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and  Long-Context Reasoning
Authors: ['Haochen You', 'Baojing Liu']
Summary: While Transformer architectures have demonstrated impressive scalabilityacross domains, they continue to face challenges in long-context reasoning,computational efficiency, and structural generalization - largely due to rigidlayer stacking, dense attention, and reliance on positional encodings. Wepresent ReSSFormer, a Recursive Sparse Structured Transformer that integratesthree complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) foriterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)for efficient and focused context selection, and Self-Organizing EncoderStructure (SOES) for position-free structure induction. ReSSFormer replacesconventional depth stacking with recurrent inference, substitutes fullattention with token- and expert-level sparsity, and models latent tokentopology directly from content. Across language modeling, multi-hop QA, andstructure-sensitive tasks, ReSSFormer consistently outperforms strong baselinesunder comparable FLOPs and parameter budgets, highlighting its scalability,efficiency, and structural flexibility.
摘要: 尽管Transformer架构已在各领域展现出令人印象深刻的可扩展性，但其在长上下文推理、计算效率和结构泛化方面仍面临挑战——这主要源于僵化的层级堆叠、密集注意力机制以及对位置编码的依赖。我们提出了ReSSFormer，一种递归稀疏结构化Transformer，整合了三项互补创新：用于有界深度迭代推理的递归推理与记忆单元（R2MU）、实现高效聚焦上下文选择的自适应稀疏注意力模块（ASAM），以及支持无位置结构诱导的自组织编码器结构（SOES）。ReSSFormer用递归推理替代传统深度堆叠，以token级和专家级稀疏性替代全注意力机制，并直接从内容建模潜在token拓扑结构。在语言建模、多跳问答及结构敏感任务中，ReSSFormer在可比FLOPs和参数预算下持续超越强基线模型，凸显了其可扩展性、效率及结构灵活性。
Link: http://arxiv.org/abs/2510.01585v1
Updated: 2025-10-02T02:05:30Z

6: CLUE: Non-parametric Verification from Experience via Hidden-State  Clustering
Authors: ['Zhenwen Liang', 'Ruosen Li', 'Yujun Zhou', 'Linfeng Song', 'Dian Yu', 'Xinya Du', 'Haitao Mi', 'Dong Yu']
Summary: Assessing the quality of Large Language Model (LLM) outputs presents acritical challenge. Previous methods either rely on text-level information(e.g., reward models, majority voting), which can overfit to superficial cues,or on calibrated confidence from token probabilities, which would fail onless-calibrated models. Yet both of these signals are, in fact, partialprojections of a richer source of information: the model's internal hiddenstates. Early layers, closer to token embeddings, preserve semantic and lexicalfeatures that underpin text-based judgments, while later layers increasinglyalign with output logits, embedding confidence-related information. This paperexplores hidden states directly as a unified foundation for verification. Weshow that the correctness of a solution is encoded as a geometrically separablesignature within the trajectory of hidden activations. To validate this, wepresent Clue (Clustering and Experience-based Verification), a deliberatelyminimalist, non-parametric verifier. With no trainable parameters, CLUE onlysummarizes each reasoning trace by an hidden state delta and classifiescorrectness via nearest-centroid distance to ``success'' and ``failure''clusters formed from past experience. The simplicity of this method highlightsthe strength of the underlying signal. Empirically, CLUE consistentlyoutperforms LLM-as-a-judge baselines and matches or exceeds modernconfidence-based methods in reranking candidates, improving both top-1 andmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%(top-maj@16).
摘要: 评估大型语言模型（LLM）输出的质量是一项关键挑战。先前的方法要么依赖于文本层面的信息（例如，奖励模型、多数投票），这可能会过度拟合表面线索；要么依赖于标记概率的校准置信度，而这在那些校准程度较低的模型上会失效。然而，这两种信号实际上都源自一个更丰富的信息源的部分投影：模型的内部隐藏状态。靠近标记嵌入的早期层保留了支撑文本判断的语义和词汇特征，而后期层则逐渐与输出对数概率对齐，从而嵌入与置信度相关的信息。本文直接探索隐藏状态，将其作为验证的统一基础。我们表明，解决方案的正确性在隐藏激活轨迹中被编码为一种几何上可分离的签名。为验证这一点，我们提出了Clue（基于聚类和经验的验证），这是一种刻意设计的极简非参数验证器。CLUE没有可训练参数，仅通过隐藏状态差值对每个推理轨迹进行总结，并通过计算与过往经验形成的“成功”和“失败”簇的最近质心距离来分类正确性。该方法的简洁性凸显了底层信号的强度。实验上，CLUE在性能上持续优于以LLM为评判基准的方法，在重新排序候选答案时匹配或超越了现代基于置信度的方法，并在AIME 24/25和GPQA数据集上同时提升了top-1和多数投票的准确率。尤为突出的是，在AIME 24数据集上使用1.5B模型时，CLUE将准确率从56.7%（majority@64）提升至70.0%（top-maj@16）。
Link: http://arxiv.org/abs/2510.01591v1
Updated: 2025-10-02T02:14:33Z

7: A Comparison of Independent and Joint Fine-tuning Strategies for  Retrieval-Augmented Generation
Authors: ['Neal Gregory Lawton', 'Alfy Samuel', 'Anoop Kumar', 'Daben Liu']
Summary: A Comparison of Independent and Joint Fine-tuning Strategies forRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate andcompare strategies for fine-tuning Retrieval Augmented Generation (RAG)pipelines, including independent fine-tuning, joint fine-tuning, and two-phasefine-tuning. Abstract: Retrieval augmented generation (RAG) is a popularframework for question answering that is powered by two large language models(LLMs): an embedding model that retrieves context documents from a databasethat are relevant to a given question, and a generator model that uses theretrieved context to generate an answer to the question. Both the embedding andgenerator models can be fine-tuned to increase performance of a RAG pipeline ona new task, but multiple fine-tuning strategies exist with different costs andbenefits. In this paper, we evaluate and compare several RAG fine-tuningstrategies, including independent, joint, and two-phase fine-tuning. In ourexperiments, we observe that all of these strategies achieve about equalimprovement in EM and F1 generation quality metrics, although they havesignificantly different computational costs. We conclude the optimalfine-tuning strategy to use depends on whether the training dataset includescontext labels and whether a grid search over the learning rates for theembedding and generator models is required.
摘要: 检索增强生成的独立与联合微调策略比较 下载PDF Neal Gregory Lawton, Alfy Samuel, Anoop Kumar, Daben Liu 发表日期：2025年8月20日，最后修改日期：2025年9月17日 EMNLP2025 Findings会议，出版主席，作者 修订 BibTeX CC BY 4.0 关键词：检索增强生成（RAG），大型语言模型（LLMs），微调，问答，联合微调 TL;DR：我们评估并比较了用于微调检索增强生成（RAG）管道的策略，包括独立微调、联合微调和两阶段微调。 摘要：检索增强生成（RAG）是一种流行的问答框架，由两个大型语言模型（LLMs）驱动：一个嵌入模型，用于从数据库中检索与给定问题相关的上下文文档；一个生成器模型，使用检索到的上下文生成问题的答案。嵌入模型和生成器模型都可以进行微调，以提高RAG管道在新任务上的性能，但存在多种具有不同成本和效益的微调策略。在本文中，我们评估并比较了几种RAG微调策略，包括独立微调、联合微调和两阶段微调。在我们的实验中，我们观察到所有这些策略在EM和F1生成质量指标上都取得了大致相等的改进，尽管它们的计算成本差异显著。我们得出结论，最优微调策略的选择取决于训练数据集是否包含上下文标签，以及是否需要对嵌入模型和生成器模型的学习率进行网格搜索。
Link: http://arxiv.org/abs/2510.01600v1
Updated: 2025-10-02T02:30:28Z

8: Bridging Collaborative Filtering and Large Language Models with Dynamic  Alignment, Multimodal Fusion and Evidence-grounded Explanations
Authors: ['Bo Ma', 'LuYao Liu', 'Simon Lau', 'Chandler Yuan', 'and XueY Cui', 'Rosie Zhang']
Summary: Recent research has explored using Large Language Models for recommendationtasks by transforming user interaction histories and item metadata into textprompts, then having the LLM produce rankings or recommendations. A promisingapproach involves connecting collaborative filtering knowledge to LLMrepresentations through compact adapter networks, which avoids expensivefine-tuning while preserving the strengths of both components. Yet severalchallenges persist in practice: collaborative filtering models often use staticsnapshots that miss rapidly changing user preferences; many real-world itemscontain rich visual and audio content beyond textual descriptions; and currentsystems struggle to provide trustworthy explanations backed by concreteevidence. Our work introduces \model{}, a framework that tackles theselimitations through three key innovations. We develop an online adaptationmechanism that continuously incorporates new user interactions throughlightweight modules, avoiding the need to retrain large models. We create aunified representation that seamlessly combines collaborative signals withvisual and audio features, handling cases where some modalities may beunavailable. Finally, we design an explanation system that groundsrecommendations in specific collaborative patterns and item attributes,producing natural language rationales users can verify. Our approach maintainsthe efficiency of frozen base models while adding minimal computationaloverhead, making it practical for real-world deployment.
摘要: 近期研究探索了将大型语言模型用于推荐任务，方法是将用户交互历史和项目元数据转换为文本提示，然后让大型语言模型生成排序或推荐。一种有前景的方法是通过紧凑的适配器网络将协同过滤知识与大型语言模型的表示相连接，这种方法避免了昂贵的微调，同时保留了两个组件的优势。然而，在实践中仍存在几个挑战：协同过滤模型通常使用静态快照，无法捕捉快速变化的用户偏好；许多现实世界的项目除了文本描述外，还包含丰富的视觉和音频内容；现有系统难以提供由具体证据支持的可信解释。我们的工作引入了\model{}框架，通过三项关键创新来解决这些局限性。我们开发了一种在线适应机制，通过轻量级模块持续整合新的用户交互，避免了重新训练大型模型的需求。我们创建了一种统一表示，无缝地将协同信号与视觉和音频特征相结合，处理某些模态可能不可用的情况。最后，我们设计了一个解释系统，将推荐基于特定的协同模式和项目属性，生成用户可以验证的自然语言理由。我们的方法在保持冻结基础模型效率的同时，仅增加了最小的计算开销，使其适用于实际部署。
Link: http://arxiv.org/abs/2510.01606v1
Updated: 2025-10-02T02:43:24Z

9: PychoBench: Evaluating the Psychology Intelligence of Large Language  Models
Authors: ['Min Zeng']
Summary: Large Language Models (LLMs) have demonstrated remarkable success across awide range of industries, primarily due to their impressive generativeabilities. Yet, their potential in applications requiring cognitive abilities,such as psychological counseling, remains largely untapped. This paperinvestigates the key question: Can LLMs be effectively applied to psychologicalcounseling? To determine whether an LLM can effectively take on the role of apsychological counselor, the first step is to assess whether it meets thequalifications required for such a role, namely the ability to pass the U.S.National Counselor Certification Exam (NCE). This is because, just as a humancounselor must pass a certification exam to practice, an LLM must demonstratesufficient psychological knowledge to meet the standards required for such arole. To address this, we introduce PsychoBench, a benchmark grounded inU.S.national counselor examinations, a licensure test for professionalcounselors that requires about 70% accuracy to pass. PsychoBench comprisesapproximately 2,252 carefully curated single-choice questions, crafted torequire deep understanding and broad enough to cover various sub-disciplines ofpsychology. This benchmark provides a comprehensive assessment of an LLM'sability to function as a counselor. Our evaluation shows that advanced modelssuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passingthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)remain far below it. These results suggest that only frontier LLMs arecurrently capable of meeting counseling exam standards, highlighting both thepromise and the challenges of developing psychology-oriented LLMs.
摘要: 大型语言模型（LLMs）已在众多行业取得了显著成功，这主要归功于其卓越的生成能力。然而，在需要认知能力的应用领域，如心理咨询，其潜力在很大程度上仍未被开发。本文探讨了一个核心问题：LLMs能否有效应用于心理咨询？为判断LLM能否有效胜任心理咨询师的角色，首要步骤是评估其是否具备该角色所需的资质，即通过美国国家咨询师认证考试（NCE）的能力。这是因为，正如人类咨询师必须通过认证考试才能执业，LLM也必须展现出足够的心理学知识以满足该角色的标准要求。为此，我们引入了PsychoBench，这是一个基于美国国家咨询师考试的基准测试，该考试是专业咨询师的执业许可测试，要求约70%的准确率方可通过。PsychoBench包含约2,252道精心策划的单选题，这些题目旨在考察深度理解，且覆盖面广，涵盖心理学的多个子学科。该基准为评估LLM作为咨询师履职能力提供了全面衡量。我们的评估显示，GPT-4o、Llama3.3-70B和Gemma3-27B等先进模型的得分远超及格线，而较小的开源模型（如Qwen2.5-7B、Mistral-7B）则远低于此。结果表明，目前仅有前沿LLMs能够达到心理咨询考试标准，这既凸显了开发面向心理学的LLMs的潜力，也揭示了其面临的挑战。
Link: http://arxiv.org/abs/2510.01611v1
Updated: 2025-10-02T02:49:06Z

10: RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical  Question Answering
Authors: ['Lovely Yeswanth Panchumarthi', 'Sai Prasad Gudari', 'Atharva Negi', 'Praveen Raj Budime', 'Harsit Upadhya']
Summary: The exponential growth of biomedical literature creates significantchallenges for accessing precise medical information. Current biomedicalquestion-answering systems primarily focus on short-form answers, failing toprovide the comprehensive explanations necessary for clinical decision-making.We present RAG-BioQA, a novel framework combining retrieval-augmentedgeneration with domain-specific fine-tuning to produce evidence-based,long-form biomedical answers. Our approach integrates BioBERT embeddings withFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,MonoT5) to optimize context selection before synthesizing evidence through afine-tuned T5 model. Experimental results on the PubMedQA dataset showsignificant improvements over baselines, with our best model achievingsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the stateof accessible, evidence-based biomedical knowledge retrieval.
摘要: 生物医学文献的指数级增长为获取精准医疗信息带来了巨大挑战。当前生物医学问答系统主要聚焦于简短答案，无法提供临床决策所需的全面解释。我们提出了RAG-BioQA，一种结合检索增强生成与领域特定微调的创新框架，用于生成基于证据的长篇生物医学答案。我们的方法将BioBERT嵌入与FAISS索引相结合，并比较了多种重排序策略（BM25、ColBERT、MonoT5）以优化上下文选择，然后通过微调的T5模型整合证据。在PubMedQA数据集上的实验结果表明，我们的方法相较于基线模型取得了显著改进，其中最佳模型在BLEU、ROUGE和METEOR指标上均实现了大幅提升，推动了可获取的、基于证据的生物医学知识检索技术的发展。
Link: http://arxiv.org/abs/2510.01612v1
Updated: 2025-10-02T02:49:09Z

11: Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single  Consumer GPU: Continual Pre-training, SFT, and DPO
Authors: ['Yu-Cheng Chih', 'Ming-Tao Duan', 'Yong-Hao Hou']
Summary: Small Language Models (SLMs) enable cost-effective, on-device andlatency-sensitive AI applications, yet their deployment in Traditional Chinese(TC) remains hindered by token-level instability - models unpredictably emitnon-TC characters or code-switch into other languages. We address thispractical reliability gap by creating PureTC-1B, a three-stage stabilizationpipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned modelreleased by Meta) using parameter-efficient LoRA adapters. Our method combinesContinual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning(SFT) with instruction data, and Direct Preference Optimization (DPO) usingTC-adherence preferences to improve monolingual robustness without full-modelretraining. On a benchmark designed to simulate real-world usage, PureTC-1Bachieves a 51.3% relative reduction (micro-average) in non-TC output tokensversus the base model. On a Named Entity Translation (NET) task, PureTC-1Bfurther reduces incorrect-language tokens by 77.2% relative to Llama-3B and57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainableeven at the 1B scale. The pipeline is reproducible, adapter-only, andhardware-friendly, offering practitioners a practical recipe to enhancelanguage stability for TC and potentially other non-English languages.
摘要: 小型语言模型（SLMs）能够实现经济高效、设备端部署且对延迟敏感的人工智能应用，但其在繁体中文（TC）领域的部署仍受制于词元级不稳定性问题——模型会不可预测地输出非繁体中文字符或切换至其他语言。为解决这一实际可靠性缺口，我们创建了PureTC-1B，这是一个针对Meta发布的开源权重指令微调模型Llama-3.2-1B-Instruct的三阶段稳定化流程，采用参数高效的LoRA适配器实现。我们的方法结合了以繁体中文为中心语料的持续预训练（CPT）、基于指令数据的监督微调（SFT），以及利用繁体中文遵循偏好进行的直接偏好优化（DPO），从而在无需全模型重训练的情况下提升单语鲁棒性。在模拟真实使用场景的基准测试中，PureTC-1B的非繁体中文输出词元相对减少率达51.3%（微平均）。在命名实体翻译（NET）任务中，PureTC-1B相较于Llama-3B和Qwen-1.5B分别进一步降低了77.2%和57.2%的错误语言词元，这表明即使在10亿参数规模下也能实现稳健的繁体中文遵循。该流程具有可复现性、纯适配器特性及硬件友好性，为从业者提供了一套增强繁体中文及其他潜在非英语语言稳定性的实用方案。
Link: http://dx.doi.org/10.5281/zenodo.17240351
Updated: 2025-10-02T02:50:12Z

12: AMAS: Adaptively Determining Communication Topology for LLM-based  Multi-Agent System
Authors: ['Hui Yi Leong', 'Yuheng Li', 'Yuqing Wu', 'Wenwen Ouyang', 'Wei Zhu', 'Jiechao Gao']
Summary: Although large language models (LLMs) have revolutionized natural languageprocessing capabilities, their practical implementation as autonomousmulti-agent systems (MAS) for industrial problem-solving encounters persistentbarriers. Conventional MAS architectures are fundamentally restricted byinflexible, hand-crafted graph topologies that lack contextual responsiveness,resulting in diminished efficacy across varied academic and commercialworkloads. To surmount these constraints, we introduce AMAS, aparadigm-shifting framework that redefines LLM-based MAS through a noveldynamic graph designer. This component autonomously identifies task-specificoptimal graph configurations via lightweight LLM adaptation, eliminating thereliance on monolithic, universally applied structural templates. Instead, AMASexploits the intrinsic properties of individual inputs to intelligently directquery trajectories through task-optimized agent pathways. Rigorous validationacross question answering, mathematical deduction, and code generationbenchmarks confirms that AMAS systematically exceeds state-of-the-artsingle-agent and multi-agent approaches across diverse LLM architectures. Ourinvestigation establishes that context-sensitive structural adaptabilityconstitutes a foundational requirement for high-performance LLM MASdeployments.
摘要: 尽管大型语言模型（LLM）已经彻底改变了自然语言处理能力，但它们作为自主多智能体系统（MAS）用于工业问题解决的实际实施仍面临持续障碍。传统的MAS架构从根本上受到僵化、手工制作的图拓扑结构的限制，这些结构缺乏上下文响应能力，导致其在各种学术和商业工作负载中的功效降低。为了克服这些限制，我们引入了AMAS，这是一个开创性的框架，通过一种新颖的动态图设计器重新定义了基于LLM的MAS。该组件通过轻量级的LLM自适应，自主识别特定任务的最优图配置，从而消除了对单一的、普遍应用的结构模板的依赖。相反，AMAS利用各个输入的内在属性，通过任务优化的智能体路径智能地引导查询轨迹。在问答、数学推理和代码生成基准测试上的严格验证证实，AMAS在各种LLM架构上系统地超越了最先进的单智能体和多智能体方法。我们的研究确立了，上下文敏感的结构适应性是高性能LLM MAS部署的基础要求。
Link: http://arxiv.org/abs/2510.01617v1
Updated: 2025-10-02T02:50:22Z

13: LLM4Rec: Large Language Models for Multimodal Generative Recommendation  with Causal Debiasing
Authors: ['Bo Ma', 'Hang Li', 'ZeHua Hu', 'XiaoFan Gui', 'LuYao Liu', 'Simon Lau']
Summary: Contemporary generative recommendation systems face significant challenges inhandling multimodal data, eliminating algorithmic biases, and providingtransparent decision-making processes. This paper introduces an enhancedgenerative recommendation framework that addresses these limitations throughfive key innovations: multimodal fusion architecture, retrieval-augmentedgeneration mechanisms, causal inference-based debiasing, explainablerecommendation generation, and real-time adaptive learning capabilities. Ourframework leverages advanced large language models as the backbone whileincorporating specialized modules for cross-modal understanding, contextualknowledge integration, bias mitigation, explanation synthesis, and continuousmodel adaptation. Extensive experiments on three benchmark datasets(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistentimprovements in recommendation accuracy, fairness, and diversity compared toexisting approaches. The proposed framework achieves up to 2.3% improvement inNDCG@10 and 1.4% enhancement in diversity metrics while maintainingcomputational efficiency through optimized inference strategies.
摘要: 当代生成式推荐系统在处理多模态数据、消除算法偏见以及提供透明决策过程方面面临重大挑战。本文介绍了一种增强型生成式推荐框架，通过五项关键创新解决这些局限性：多模态融合架构、检索增强生成机制、基于因果推断的去偏技术、可解释推荐生成以及实时自适应学习能力。该框架以先进的大型语言模型为基础，同时集成了用于跨模态理解、上下文知识整合、偏见缓解、解释合成和持续模型适配的专用模块。在三个基准数据集（MovieLens-25M、Amazon-Electronics、Yelp-2023）上的大量实验表明，与现有方法相比，该框架在推荐准确性、公平性和多样性方面均有一致的提升。所提出的框架在NDCG@10指标上实现了高达2.3%的提升，在多样性指标上实现了1.4%的增强，同时通过优化的推理策略保持了计算效率。
Link: http://arxiv.org/abs/2510.01622v1
Updated: 2025-10-02T02:53:05Z

14: Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What  to Use Instead
Authors: ['Feiyang Kang', 'Michael Kuchnik', 'Karthik Padthe', 'Marin Vlastelica', 'Ruoxi Jia', 'Carole-Jean Wu', 'Newsha Ardalani']
Summary: In post-training for reasoning Large Language Models (LLMs), the currentstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as``RL'' below). In this work, we challenge whether high SFT scores translate toimproved performance after RL. We provide extensive counter-examples where thisis not true. We find high SFT scores can be biased toward simpler or morehomogeneous data and are not reliably predictive of subsequent RL gains orscaled-up post-training effectiveness. In some cases, RL training on modelswith improved SFT performance could lead to substantially worse outcomecompared to RL on the base model without SFT. We study alternative metrics andidentify generalization loss on held-out reasoning examples and Pass@large kperformance to provide strong proxies for the RL outcome. We trained hundredsof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensiveevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPUhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiplestate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RLperformance, prediction based on generalization loss and Pass@large k achievessubstantial higher precision, improving $R^2$ coefficient and Spearman's rankcorrelation coefficient by up to 0.5 (2x). This provides strong utility forbroad use cases. For example, in most experiments, we find SFT training onunique examples for a one epoch underperforms training on half examples for twoepochs, either after SFT or SFT-then-RL; With the same SFT budget, trainingonly on short examples may lead to better SFT performance, though, it oftenleads to worse outcome after RL compared to training on examples with varyinglengths. Evaluation tool will be open-sourced.
摘要: 在用于推理的大型语言模型（LLMs）的后训练中，当前实践将LLMs的训练分为两个独立阶段：监督微调（SFT）和基于可验证奖励的强化学习（RLVR，下文简称为“RL”）。在本文中，我们质疑高SFT分数是否能转化为RL后性能的提升。我们提供了大量反例证明情况并非如此。我们发现，高SFT分数可能偏向于更简单或更同质化的数据，且无法可靠地预测后续的RL增益或规模化后训练效果。在某些情况下，对SFT性能提升的模型进行RL训练，其结果可能比在未经过SFT的基础模型上进行RL训练差得多。我们研究了替代性指标，并发现对保留推理样本的泛化损失和Pass@large k性能可作为RL结果的强有力代理指标。我们通过GRPO训练了数百个参数规模达120亿的SFT和RLVR模型，在7个数学基准测试上进行了多达256次重复的广泛评估，耗资超过100万GPU小时。实验涉及Llama3、Mistral-Nemo、Qwen3等多个模型以及多种最先进的SFT/RL数据集。与直接基于RL前性能的预测相比，基于泛化损失和Pass@large k的预测精度显著更高，将R²系数和斯皮尔曼等级相关系数提高了0.5（2倍）。这为广泛的应用场景提供了重要价值。例如，在大多数实验中，我们发现对唯一样本进行一个周期的SFT训练，其效果不如对半数样本进行两个周期的训练，无论是在SFT后还是在SFT-then-RL阶段；在相同的SFT预算下，仅训练短样本可能获得更好的SFT性能，但与训练不同长度样本相比，其RL后的结果往往更差。评估工具将开源。
Link: http://arxiv.org/abs/2510.01624v1
Updated: 2025-10-02T02:57:00Z

15: Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of  Scaling Laws, Benefits, and Pitfalls
Authors: ['Feiyang Kang', 'Newsha Ardalani', 'Michael Kuchnik', 'Youssef Emad', 'Mostafa Elhoushi', 'Shubhabrata Sengupta', 'Shang-Wen Li', 'Ramya Raghavendra', 'Ruoxi Jia', 'Carole-Jean Wu']
Summary: Training data plays a crucial role in Large Language Models (LLM) scaling,yet high quality data is of limited supply. Synthetic data techniques offer apotential path toward sidestepping these limitations. We conduct a large-scaleempirical investigation (>1000 LLMs with >100k GPU hours) using a unifiedprotocol and scaling laws, comparing natural web data, diverse synthetic types(rephrased text, generated textbooks), and mixtures of natural and syntheticdata. Specifically, we found pre-training on rephrased synthetic data\textit{alone} is not faster than pre-training on natural web texts; whilepre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web textscan speed up 5-10x (to reach the same validation loss) at larger data budgets.Pre-training on textbook-style synthetic data \textit{alone} results in notablyhigher loss on many downstream domains especially at small data budgets. "Good"ratios of synthetic data in training data mixtures depend on the model size anddata budget, empirically converging to ~30% for rephrased synthetic data.Larger generator models do not necessarily yield better pre-training data than~8B-param models. These results contribute mixed evidence on "model collapse"during large-scale single-round (n=1) model training on syntheticdata--training on rephrased synthetic data shows no degradation in performancein foreseeable scales whereas training on mixtures of textbook-stylepure-generated synthetic data shows patterns predicted by "model collapse". Ourwork demystifies synthetic data in pre-training, validates its conditionalbenefits, and offers practical guidance.
摘要: 训练数据在大型语言模型（LLM）的扩展中扮演着至关重要的角色，然而高质量数据的供应却十分有限。合成数据技术为规避这些限制提供了一条潜在路径。我们采用统一协议和扩展定律进行了一项大规模实证研究（涉及超过1000个LLM，耗时超过10万GPU小时），比较了自然网络数据、多种合成数据类型（改写文本、生成的教科书）以及自然与合成数据的混合物。具体而言，我们发现单独使用改写合成数据进行预训练的速度并不比使用自然网络文本预训练更快；而在较大数据预算下，将1/3的改写合成数据与2/3的自然网络文本混合进行预训练，可使速度提升5-10倍（以达到相同的验证损失）。单独使用教科书风格的合成数据进行预训练会导致在许多下游领域上的损失显著更高，尤其是在小数据预算情况下。训练数据混合物中合成数据的“良好”比例取决于模型规模和数据预算，实证结果表明改写合成数据的比例收敛于约30%。更大的生成模型并不一定比约80亿参数的模型产生更好的预训练数据。这些结果为“模型崩溃”在合成数据上的大规模单轮（n=1）模型训练过程中提供了混合证据——使用改写合成数据进行训练在可预见的规模下未表现出性能下降，而使用教科书风格纯生成合成数据的混合物进行训练则显示出“模型崩溃”所预测的模式。我们的工作揭示了预训练中合成数据的神秘性，验证了其条件性优势，并提供了实用指导。
Link: http://arxiv.org/abs/2510.01631v1
Updated: 2025-10-02T03:24:42Z

16: NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with  BERT
Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']
Summary: Large Language Models (LLMs) suffer from a range of vulnerabilities thatallow malicious users to solicit undesirable responses through manipulation ofthe input text. These so-called jailbreak prompts are designed to trick the LLMinto circumventing the safety guardrails put in place to keep responsesacceptable to the developer's policies. In this study, we analyse the abilityof different machine learning models to distinguish jailbreak prompts fromgenuine uses, including looking at our ability to identify jailbreaks that usepreviously unseen strategies. Our results indicate that using current datasetsthe best performance is achieved by fine tuning a Bidirectional EncoderRepresentations from Transformers (BERT) model end-to-end for identifyingjailbreaks. We visualise the keywords that distinguish jailbreak from genuineprompts and conclude that explicit reflexivity in prompt structure could be asignal of jailbreak intention.
摘要: 大型语言模型（LLMs）存在一系列漏洞，使得恶意用户能够通过操纵输入文本来诱导不当响应。这些所谓的越狱提示旨在欺骗大型语言模型，使其绕过为保持响应符合开发者政策而设置的安全护栏。在本研究中，我们分析了不同机器学习模型区分越狱提示与真实使用的能力，包括识别采用前所未见策略的越狱提示的能力。我们的结果表明，使用当前数据集，通过端到端微调用于识别越狱提示的来自Transformer的双向编码器表示（BERT）模型，可实现最佳性能。我们可视化区分越狱提示与真实提示的关键词，并得出结论：提示结构中的显式自反性可能是越狱意图的信号。
Link: http://arxiv.org/abs/2510.01644v1
Updated: 2025-10-02T03:55:29Z

17: Position: Privacy Is Not Just Memorization!
Authors: ['Niloofar Mireshghallah', 'Tianshi Li']
Summary: The discourse on privacy risks in Large Language Models (LLMs) hasdisproportionately focused on verbatim memorization of training data, while aconstellation of more immediate and scalable privacy threats remainunderexplored. This position paper argues that the privacy landscape of LLMsystems extends far beyond training data extraction, encompassing risks fromdata collection practices, inference-time context leakage, autonomous agentcapabilities, and the democratization of surveillance through deep inferenceattacks. We present a comprehensive taxonomy of privacy risks across the LLMlifecycle -- from data collection through deployment -- and demonstrate throughcase studies how current privacy frameworks fail to address these multifacetedthreats. Through a longitudinal analysis of 1,322 AI/ML privacy paperspublished at leading conferences over the past decade (2016--2025), we revealthat while memorization receives outsized attention in technical research, themost pressing privacy harms lie elsewhere, where current technical approachesoffer little traction and viable paths forward remain unclear. We call for afundamental shift in how the research community approaches LLM privacy, movingbeyond the narrow focus of current technical solutions and embracinginterdisciplinary approaches that address the sociotechnical nature of theseemerging threats.
摘要: 关于大型语言模型（LLMs）隐私风险的讨论，不成比例地集中于对训练数据的逐字记忆，而一系列更直接且可扩展的隐私威胁却仍未得到充分探索。本立场论文指出，LLM系统的隐私格局远不止于训练数据提取，还包括数据收集实践、推理时上下文泄露、自主代理能力以及通过深度推理攻击实现监控普及化所带来的风险。我们提出了一个涵盖LLM全生命周期（从数据收集到部署）的隐私风险综合分类法，并通过案例研究展示了当前隐私框架如何未能应对这些多方面的威胁。通过对过去十年（2016-2025年）顶级会议上发表的1322篇AI/ML隐私论文进行纵向分析，我们发现，尽管记忆问题在技术研究中受到了过度关注，但最紧迫的隐私危害却存在于其他领域，而当前的技术方法在这些领域收效甚微，可行的前进路径仍不明确。我们呼吁研究界在处理LLM隐私问题上进行根本性转变，超越当前技术解决方案的狭隘视角，采用能够应对这些新兴威胁社会技术性质的跨学科方法。
Link: http://arxiv.org/abs/2510.01645v1
Updated: 2025-10-02T04:02:06Z

18: Learning to Look at the Other Side: A Semantic Probing Study of Word  Embeddings in LLMs with Enabled Bidirectional Attention
Authors: ['Zhaoxin Feng', 'Jianfei Ma', 'Emmanuele Chersoni', 'Xiaojing Zhao', 'Xiaoyi Bao']
Summary: Autoregressive Large Language Models (LLMs) demonstrate exceptionalperformance in language understanding and generation. However, theirapplication in text embedding tasks has been relatively slow, along with theanalysis of their semantic representation in probing tasks, due to theconstraints of the unidirectional attention mechanism.  This paper aims to explore whether such constraints can be overcome byenabling bidirectional attention in LLMs. We tested different variants of theLlama architecture through additional training steps, progressively enablingbidirectional attention and unsupervised/supervised contrastive learning.
摘要: 自回归大型语言模型（LLMs）在语言理解和生成方面表现出卓越的性能。然而，由于单向注意力机制的制约，它们在文本嵌入任务中的应用以及其在探测任务中的语义表示分析均进展相对缓慢。本文旨在探讨通过在LLMs中实现双向注意力，是否能够克服此类制约。我们通过额外的训练步骤对Llama架构的不同变体进行了测试，逐步实现双向注意力以及无监督/监督对比学习。
Link: http://arxiv.org/abs/2510.01652v1
Updated: 2025-10-02T04:18:13Z

19: SoK: Measuring What Matters for Closed-Loop Security Agents
Authors: ['Mudita Khurana', 'Raunak Jain']
Summary: Cybersecurity is a relentless arms race, with AI driven offensive systemsevolving faster than traditional defenses can adapt. Research and toolingremain fragmented across isolated defensive functions, creating blind spotsthat adversaries exploit. Autonomous agents capable of integrating, exploitconfirmation, remediation, and validation into a single closed loop offerpromise, but the field lacks three essentials: a framework defining the agenticcapabilities of security systems across security life cycle, a principledmethod for evaluating closed loop agents, and a benchmark for measuring theirperformance in practice. We introduce CLASP: the Closed-Loop AutonomousSecurity Performance framework which aligns the security lifecycle(reconnaissance, exploitation, root cause analysis, patch synthesis,validation) with core agentic capabilities (planning, tool use, memory,reasoning, reflection & perception) providing a common vocabulary and rubricfor assessing agentic capabilities in security tasks. By applying CLASP to 21representative works, we map where systems demonstrate strengths, and wherecapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,a composite metric quantifying both degree of loop closure and operationaleffectiveness, and outline the requirements for a closed loop benchmark.Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, andmeasurements needed to advance both function level performance and measureclosed loop security agents.
摘要: 网络安全是一场永无止境的军备竞赛，人工智能驱动的攻击系统的发展速度超过了传统防御的适应能力。研究和工具在孤立的防御功能中仍然碎片化，造成了对手可利用的盲点。能够将集成、漏洞利用确认、修复和验证整合到单一闭环中的自主代理展现出潜力，但该领域缺乏三个关键要素：一个定义安全系统在安全生命周期中代理能力的框架，一种评估闭环代理的原则性方法，以及一个衡量其在实践中性能的基准。我们介绍了CLASP：闭环自主安全性能框架，该框架将安全生命周期（侦察、漏洞利用、根本原因分析、补丁合成、验证）与核心代理能力（规划、工具使用、记忆、推理、反思与感知）相结合，为评估安全任务中的代理能力提供了通用词汇和标准。通过将CLASP应用于21个代表性工作，我们绘制了系统展现优势的领域以及能力持续存在的差距。随后，我们定义了闭环能力（CLC）分数，这是一种综合指标，用于量化闭环程度和运营效能，并概述了闭环基准的要求。CLASP和CLC分数共同提供了推进功能级性能和衡量闭环安全代理所需的词汇、诊断和测量方法。
Link: http://arxiv.org/abs/2510.01654v1
Updated: 2025-10-02T04:20:35Z

20: MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue  Summarization
Authors: ['Yinhong Liu', 'Jianfeng He', 'Hang Su', 'Ruixue Lian', 'Yi Nian', 'Jake Vincent', 'Srikanth Vishnubhotla', 'Robinson Piramuthu', 'Saab Mansour']
Summary: Multimodal Dialogue Summarization (MDS) is a critical task with wide-rangingapplications. To support the development of effective MDS models, robustautomatic evaluation methods are essential for reducing both cost and humaneffort. However, such methods require a strong meta-evaluation benchmarkgrounded in human annotations. In this work, we introduce MDSEval, the firstmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,corresponding summaries, and human judgments across eight well-defined qualityaspects. To ensure data quality and richfulness, we propose a novel filteringframework leveraging Mutually Exclusive Key Information (MEKI) acrossmodalities. Our work is the first to identify and formalize key evaluationdimensions specific to MDS. We benchmark state-of-the-art modal evaluationmethods, revealing their limitations in distinguishing summaries from advancedMLLMs and their susceptibility to various bias.
摘要: 多模态对话摘要（MDS）是一项具有广泛应用的关键任务。为支持高效MDS模型的开发，稳健的自动评估方法对于降低成本和减少人力投入至关重要。然而，此类方法需要一个基于人工标注的强大元评估基准作为支撑。在本研究中，我们推出了MDSEval，这是首个面向MDS的元评估基准，包含图像共享对话、相应的摘要以及涵盖八个明确定义质量维度的人类评判。为确保数据质量和丰富性，我们提出了一种新颖的过滤框架，该框架利用了跨模态的互斥关键信息（MEKI）。我们的研究首次识别并形式化了MDS特有的关键评估维度。我们对最先进的模态评估方法进行了基准测试，揭示了它们在区分来自先进多模态大语言模型（MLLMs）的摘要方面的局限性，以及它们对各种偏见的敏感性。
Link: http://arxiv.org/abs/2510.01659v1
Updated: 2025-10-02T04:38:27Z

21: Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness
Authors: ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet']
Summary: Computer-Use Agents (CUAs) are an increasingly deployed class of agents thattake actions on GUIs to accomplish user goals. In this paper, we show that CUAsconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goalsregardless of feasibility, safety, reliability, or context. We characterizethree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)assumptions and decisions under ambiguity, and (iii) contradictory orinfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing thesethree patterns. Built on OSWorld, BLIND-ACT provides realistic environments andemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreementwith human annotations. We use BLIND-ACT to evaluate nine frontier models,including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observinghigh average BGD rates (80.8%) across them. We show that BGD exposes subtlerisks that arise even when inputs are not directly harmful. Whileprompting-based interventions lower BGD levels, substantial risk persists,highlighting the need for stronger training- or inference-time interventions.Qualitative analysis reveals observed failure modes: execution-first bias(focusing on how to act over whether to act), thought-action disconnect(execution diverging from reasoning), and request-primacy (justifying actionsdue to user request). Identifying BGD and introducing BLIND-ACT establishes afoundation for future research on studying and mitigating this fundamental riskand ensuring safe CUA deployment.
摘要: 计算机使用代理（CUA）是一类日益广泛部署的代理，它们通过在图形用户界面（GUI）上执行操作来实现用户目标。在本文中，我们表明CUA持续表现出盲目标导向性（BGD）：一种无论可行性、安全性、可靠性或上下文如何都追求目标的倾向。我们描述了BGD的三个普遍模式：（i）缺乏上下文推理，（ii）在模糊性下的假设和决策，以及（iii）矛盾或不可行的目标。我们开发了BLIND-ACT，这是一个包含90项任务的基准，用于捕捉这三种模式。BLIND-ACT基于OSWorld构建，提供了真实的环境，并采用基于大型语言模型（LLM）的评估器来评估代理行为，与人类标注的一致性达到93.75%。我们使用BLIND-ACT评估了九个前沿模型，包括Claude Sonnet和Opus 4、Computer-Use-Preview以及GPT-5，观察到它们的平均BGD率很高（80.8%）。我们表明，BGD揭示了即使在输入并非直接有害时也会出现的细微风险。虽然基于提示的干预可以降低BGD水平，但重大风险依然存在，这凸显了在训练或推理阶段进行更强干预的必要性。定性分析揭示了观察到的失效模式：执行优先偏见（关注如何行动而非是否行动）、思维-行动脱节（执行与推理不一致）以及请求主导性（因用户请求而为行动辩护）。识别BGD并引入BLIND-ACT为未来研究和缓解这一根本性风险、确保CUA安全部署奠定了基础。
Link: http://arxiv.org/abs/2510.01670v1
Updated: 2025-10-02T04:52:15Z

22: FOR-Prompting: From Objection to Revision via an Asymmetric Prompting  Protocol
Authors: ['He Zhang', 'Anzhou Zhang', 'Jian Dai']
Summary: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)organize internal deliberation but lack an explicit mechanism for externalquestioning that elicits self-revision. We present FOR-Prompting (FromObjection to Revision Prompting), an asymmetric protocol where a Defenderproposes an answer, an Objectioner raises question-style objections with nodirect fixes, and a Host enforces consistency and closure. On GSM8K we observeabout a 22% point gain over single-prompt and accuracy on par with CoT, withmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1judge. FOR-Prompting also corrects mistakes without tools or human supervisionon tricky queries, and improves performance for small-scale model (approx. 19%accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise forsmall models and on personal device use. Beyond factual QA, qualitativeanalyses on open-ended tasks show enhanced exploration and refinement, withdialogue traces that make assumptions and trade-offs explicit. The protocol ismodel agnostic and operates purely at the prompt level through role-structuredturns, so it works with hosted and local models of different sizes withoutretraining, and it supports large-scale study of objection-guided reasoning.
摘要: 诸如思维链（CoT）和思维树（ToT）等推理协议组织内部审议，但缺乏一种明确的外部质疑机制来引发自我修正。我们提出了FOR-Prompting（从异议到修正的提示），这是一种非对称协议，其中辩护者提出答案，反对者提出无直接修正方案的问题式异议，而主持人则强制执行一致性和终结性。在GSM8K数据集上，我们观察到相较于单次提示的准确率提升了约22个百分点，与CoT的准确率相当，并且在由统一的GPT 4.1评判者进行的评估中，推理和连贯性评分高出10%以上。FOR-Prompting还能在无需工具或人工监督的情况下修正复杂查询中的错误，并提升了小规模模型的性能（在GSM8K任务上，Llama3.2:1b模型的准确率提高了约19%），凸显了其在小模型和个人设备使用方面的潜力。除事实性问答外，对开放式任务的定性分析显示了增强的探索和细化能力，其对话轨迹能够明确展示假设和权衡。该协议与模型无关，完全通过角色结构化的轮次在提示层面运作，因此它无需重新训练即可与不同规模托管和本地模型协同工作，并支持对异议引导推理的大规模研究。
Link: http://arxiv.org/abs/2510.01674v1
Updated: 2025-10-02T04:57:58Z

23: How Do Language Models Compose Functions?
Authors: ['Apoorv Khandelwal', 'Ellie Pavlick']
Summary: While large language models (LLMs) appear to be increasingly capable ofsolving compositional tasks, it is an open question whether they do so usingcompositional mechanisms. In this work, we investigate how feedforward LLMssolve two-hop factual recall tasks, which can be expressed compositionally as$g(f(x))$. We first confirm that modern LLMs continue to suffer from the"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.Then, using logit lens on their residual stream activations, we identify twoprocessing mechanisms, one which solves tasks $\textit{compositionally}$,computing $f(x)$ along the way to computing $g(f(x))$, and one which solvesthem $\textit{directly}$, without any detectable signature of the intermediatevariable $f(x)$. Finally, we find that which mechanism is employed appears tobe related to the embedding space geometry, with the idiomatic mechanism beingdominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ inthe embedding spaces. We fully release our data and code at:https://github.com/apoorvkh/composing-functions .
摘要: 尽管大型语言模型（LLM）在解决组合任务方面似乎越来越有能力，但它们是否使用组合机制来解决这些任务仍是一个悬而未决的问题。在这项工作中，我们研究了前馈LLM如何解决两跳事实回忆任务，这类任务可以组合表示为$g(f(x))$。我们首先证实，现代LLM仍然存在“组合性差距”：即它们计算$z = f(x)$和$y =g(z)$的能力，并不必然意味着它们能够计算组合$y = g(f(x))$。然后，通过在其残差流激活上使用logit lens，我们识别出两种处理机制：一种是$\textit{组合式}$地解决任务，即在计算$g(f(x))$的过程中计算$f(x)$；另一种是$\textit{直接式}$地解决任务，且没有任何可检测到的中间变量$f(x)$的痕迹。最后，我们发现，所采用的机制似乎与嵌入空间的几何结构有关，在嵌入空间中存在从$x$到$g(f(x))$的线性映射的情况下，直接式机制占据主导地位。我们已在以下网址完整公开了我们的数据和代码：https://github.com/apoorvkh/composing-functions 。
Link: http://arxiv.org/abs/2510.01685v1
Updated: 2025-10-02T05:21:34Z

24: Improving AGI Evaluation: A Data Science Perspective
Authors: ['John Hawkins']
Summary: Evaluation of potential AGI systems and methods is difficult due to thebreadth of the engineering goal. We have no methods for perfect evaluation ofthe end state, and instead measure performance on small tests designed toprovide directional indication that we are approaching AGI. In this work weargue that AGI evaluation methods have been dominated by a design philosophythat uses our intuitions of what intelligence is to create synthetic tasks,that have performed poorly in the history of AI. Instead we argue for analternative design philosophy focused on evaluating robust task execution thatseeks to demonstrate AGI through competence. This perspective is developed fromcommon practices in data science that are used to show that a system can bereliably deployed. We provide practical examples of what this would mean forAGI evaluation.
摘要: 由于工程目标的广泛性，对潜在通用人工智能系统及方法的评估十分困难。我们尚无完美评估最终状态的方法，转而通过设计小型测试来衡量性能，以提供我们正接近通用人工智能的方向性指示。在本文中，我们认为通用人工智能评估方法长期被一种设计理念所主导，即依赖我们对智能的直觉来创建合成任务，而这类任务在人工智能历史上表现不佳。我们主张采用一种替代设计理念，聚焦于评估稳健的任务执行能力，并通过胜任力来展现通用人工智能。这一观点源于数据科学中用于证明系统可被可靠部署的常见实践。我们为通用人工智能评估的具体含义提供了实际案例。
Link: http://arxiv.org/abs/2510.01687v1
Updated: 2025-10-02T05:27:29Z

25: Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation
Authors: ['Seungseop Lim', 'Gibaeg Kim', 'Wooseok Han', 'Jean Seo', 'Hyunkyung Lee', 'Jaehyo Yoo', 'Eunho Yang']
Summary: Recent advances in Large Language Models (LLMs) have brought significantimprovements to various service domains, including chatbots and medicalpre-consultation applications. In the healthcare domain, the most commonapproach for adapting LLMs to multi-turn dialogue generation is SupervisedFine-Tuning (SFT). However, datasets for SFT in tasks like medicalpre-consultation typically exhibit a skewed turn-count distribution. Trainingon such data induces a novel failure mechanism we term **Format Inertia**,where models tend to generate repetitive, format-correct, but diagnosticallyuninformative questions in long medical dialogues. To mitigate this observedfailure mechanism, we adopt a simple, data-centric method that rebalances theturn-count distribution of the training dataset. Experimental results show thatour approach substantially alleviates Format Inertia in medicalpre-consultation.
摘要: 大型语言模型（LLM）的最新进展为包括聊天机器人和医疗预咨询应用在内的各个服务领域带来了显著改进。在医疗领域，将大型语言模型应用于多轮对话生成的最常见方法是监督式微调（SFT）。然而，用于医疗预咨询等任务的监督式微调数据集通常呈现出轮次数量分布不均的现象。在此类数据上进行训练会引发一种我们称之为“**格式惯性**”的新型失效机制，即在漫长的医疗对话中，模型倾向于生成重复的、格式正确但诊断信息量不足的问题。为缓解这一观察到的失效机制，我们采用了一种简单的、以数据为中心的方法，即重新平衡训练数据集的轮次数量分布。实验结果表明，我们的方法显著减轻了医疗预咨询中的格式惯性问题。
Link: http://arxiv.org/abs/2510.01688v1
Updated: 2025-10-02T05:29:38Z

26: What MLLMs Learn about When they Learn about Multimodal Reasoning:  Perception, Reasoning, or their Integration?
Authors: ['Jiwan Chung', 'Neel Joshi', 'Pratyusha Sharma', 'Youngjae Yu', 'Vibhav Vineet']
Summary: Multimodal reasoning models have recently shown promise on challengingdomains such as olympiad-level geometry, yet their evaluation remains dominatedby aggregate accuracy, a single score that obscures where and how models areimproving. We introduce MathLens, a benchmark designed to disentangle thesubskills of multimodal reasoning while preserving the complexity oftextbook-style geometry problems. The benchmark separates performance intothree components: Perception: extracting information from raw inputs,Reasoning: operating on available information, and Integration: selectingrelevant perceptual evidence and applying it within reasoning. To support eachtest, we provide annotations: visual diagrams, textual descriptions to evaluatereasoning in isolation, controlled questions that require both modalities, andprobes for fine-grained perceptual skills, all derived from symbolicspecifications of the problems to ensure consistency and robustness. Ouranalysis reveals that different training approaches have uneven effects: First,reinforcement learning chiefly strengthens perception, especially whensupported by textual supervision, while textual SFT indirectly improvesperception through reflective reasoning. Second, reasoning improves only intandem with perception. Third, integration remains the weakest capacity, withresidual errors concentrated there once other skills advance. Finally,robustness diverges: RL improves consistency under diagram variation, whereasmultimodal SFT reduces it through overfitting. We will release all data andexperimental logs.
摘要: 多模态推理模型最近在奥林匹克竞赛级别的几何等具有挑战性的领域展现出了潜力，然而其评估仍主要依赖于总体准确率这一单一分数，该分数掩盖了模型改进的具体位置和方式。我们推出了MathLens，这是一个旨在解构多模态推理子技能，同时保留教科书式几何问题复杂性的基准。该基准将性能分为三个组成部分：感知（从原始输入中提取信息）、推理（对可用信息进行操作）和整合（选择相关的感知证据并在推理中应用）。为支持每项测试，我们提供了注释：视觉图表、用于独立评估推理的文本描述、需要两种模态的受控问题，以及用于细粒度感知技能的探针，所有这些都源自问题的符号化规范，以确保一致性和稳健性。我们的分析揭示了不同训练方法的不均衡影响：首先，强化学习主要增强感知能力，尤其在有文本监督支持时；而文本监督微调则通过反思性推理间接提升感知能力。其次，推理能力仅在感知能力同步提升时才会改进。第三，整合能力仍然是最薄弱的环节，当其他技能进步后，残留错误会集中于此。最后，稳健性表现各异：强化学习提升了图表变化下的一致性，而多模态监督微调则因过拟合而降低了这种一致性。我们将发布所有数据和实验日志。
Link: http://arxiv.org/abs/2510.01719v1
Updated: 2025-10-02T06:58:29Z

27: Machine-interpretable Engineering Design Standards for Valve  Specification
Authors: ['Anders Gjerver', 'Rune Frostad', 'Vedrana Barisic', 'Melinda Hodkiewicz', 'Caitlin Woods', 'Mihaly Fekete', 'Arild Braathen Torjusen', 'Johan Wilhelm Kluwer']
Summary: Engineering design processes use technical specifications and must complywith standards. Product specifications, product type data sheets, and designstandards are still mainly document-centric despite the ambition to digitalizeindustrial work. In this paper, we demonstrate how to transform informationheld in engineering design standards into modular, reusable,machine-interpretable ontologies and use the ontologies in quality assurance ofthe plant design and equipment selection process. We use modelling patterns tocreate modular ontologies for knowledge captured in the text and in frequentlyreferenced tables in International Standards for piping, material and valvedesign. These modules are exchangeable, as stored in a W3C compliant format,and interoperable as they are aligned with the top-level ontology ISO DIS23726-3: Industrial Data Ontology (IDO).  We test these ontologies, created based on international material and pipingstandards and industry norms, on a valve selection process. Valves areinstantiated in semantic asset models as individuals along with a semanticrepresentation of the environmental condition at their location on the asset.We create "functional location tags" as OWL individuals that become instancesof OWL class Valve Data Sheet (VDS) specified valves. Similarly we createinstances of manufacturer product type. Our approach enables automatedvalidation that a specific VDS is compliant with relevant industry standards.Using semantic reasoning and executable design rules, we also determine whetherthe product type meets the valve specification. Creation of shared, reusableIDO-based modular ontologies for design standards enables semantic reasoning tobe applied to equipment selection processes and demonstrates the potential ofthis approach for Standards Bodies wanting to transition to digitized SmartStandards.
摘要: 工程设计流程使用技术规范，并且必须符合标准。尽管工业工作数字化的愿望强烈，但产品规格、产品类型数据表和设计标准仍然主要以文档为中心。在本文中，我们展示了如何将工程设计标准中包含的信息转换为模块化、可重用、机器可解释的本体，并使用这些本体在工厂设计和设备选择流程中进行质量保证。我们使用建模模式为国际管道、材料和阀门设计标准中捕获的文本以及频繁引用的表格中的知识创建模块化本体。这些模块以符合W3C的格式存储，因此可交换；并且它们与顶层本体ISO DIS 23726-3：工业数据本体（IDO）保持一致，因此具有互操作性。我们基于国际材料和管道标准以及行业规范创建的这些本体，在一个阀门选择流程中进行了测试。阀门连同其在资产所在位置的环境条件的语义表示，作为个体实例化在语义资产模型中。我们创建了“功能位置标签”作为OWL个体，这些个体成为符合OWL类阀门数据表（VDS）规定的阀门的实例。同样，我们创建了制造商产品类型的实例。我们的方法能够自动验证特定的VDS是否符合相关的行业标准。通过使用语义推理和可执行的设计规则，我们还能确定产品类型是否满足阀门规格。为设计标准创建共享的、基于IDO的可重用模块化本体，使得语义推理能够应用于设备选择流程，并展示了这种方法对于希望过渡到数字化智能标准的标准机构的潜力。
Link: http://arxiv.org/abs/2510.01736v1
Updated: 2025-10-02T07:20:37Z

28: Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware  Refusal in Factual Tasks
Authors: ['Wenbo Pan', 'Jie Xu', 'Qiguang Chen', 'Junhao Dong', 'Libo Qin', 'Xinfeng Li', 'Haining Yu', 'Xiaohua Jia']
Summary: Large Language Models (LLMs) should refuse to answer questions beyond theirknowledge. This capability, which we term knowledge-aware refusal, is crucialfor factual reliability. However, existing metrics fail to faithfully measurethis ability. On the one hand, simple refusal-based metrics are biased byrefusal rates and yield inconsistent scores when models exhibit differentrefusal tendencies. On the other hand, existing calibration metrics areproxy-based, capturing the performance of auxiliary calibration processesrather than the model's actual refusal behavior. In this work, we propose theRefusal Index (RI), a principled metric that measures how accurately LLMsrefuse questions they do not know. We define RI as Spearman's rank correlationbetween refusal probability and error probability. To make RI practicallymeasurable, we design a lightweight two-pass evaluation method that efficientlyestimates RI from observed refusal rates across two standard evaluation runs.Extensive experiments across 16 models and 5 datasets demonstrate that RIaccurately quantifies a model's intrinsic knowledge-aware refusal capability infactual tasks. Notably, RI remains stable across different refusal rates andprovides consistent model rankings independent of a model's overall accuracyand refusal rates. More importantly, RI provides insight into an important butpreviously overlooked aspect of LLM factuality: while LLMs achieve highaccuracy on factual tasks, their refusal behavior can be unreliable andfragile. This finding highlights the need to complement traditional accuracymetrics with the Refusal Index for comprehensive factuality evaluation.
摘要: 大型语言模型（LLM）应拒绝回答超出其知识范围的问题。我们称此能力为知识感知拒绝，它对事实可靠性至关重要。然而，现有指标未能忠实衡量这种能力。一方面，简单的基于拒绝的指标受拒绝率影响，当模型表现出不同拒绝倾向时，会产生不一致的评分。另一方面，现有的校准指标是基于代理的，捕捉的是辅助校准过程的性能，而非模型的实际拒绝行为。在这项工作中，我们提出了拒绝指数（RI），一种原则性指标，用于衡量LLM准确拒绝其未知问题的能力。我们将RI定义为拒绝概率与错误概率之间的斯皮尔曼等级相关系数。为使RI可实际测量，我们设计了一种轻量级两遍评估方法，该方法通过两个标准评估运行中观察到的拒绝率高效估算RI。在16个模型和5个数据集上的大量实验表明，RI能准确量化模型在事实任务中的内在知识感知拒绝能力。值得注意的是，RI在不同拒绝率下保持稳定，并提供与模型整体准确率和拒绝率无关的一致模型排名。更重要的是，RI揭示了LLM事实性中一个重要但此前被忽视的方面：尽管LLM在事实任务中实现了高准确率，但其拒绝行为可能不可靠且脆弱。这一发现凸显了在综合事实性评估中，需用拒绝指数补充传统准确率指标的必要性。
Link: http://arxiv.org/abs/2510.01782v1
Updated: 2025-10-02T08:20:36Z

29: Comparison of Unsupervised Metrics for Evaluating Judicial Decision  Extraction
Authors: ['Ivan Leonidovich Litvak', 'Anton Kostin', 'Fedor Lashkin', 'Tatiana Maksiyan', 'Sergey Lagutin']
Summary: The rapid advancement of artificial intelligence in legal natural languageprocessing demands scalable methods for evaluating text extraction fromjudicial decisions. This study evaluates 16 unsupervised metrics, includingnovel formulations, to assess the quality of extracting seven semantic blocksfrom 1,000 anonymized Russian judicial decisions, validated against 7,168expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,semantic, structural, pseudo-ground truth, and legal-specific categories,operate without pre-annotated ground truth. Bootstrapped correlations, Lin'sconcordance correlation coefficient (CCC), and mean absolute error (MAE) revealthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negativecorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, LinCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, usinggpt-4.1-mini via g4f, suggests limited specialization for legal textse. Thesefindings highlight that unsupervised metrics, including LLM-based approaches,enable scalable screening but, with moderate correlations and low CCC values,cannot fully replace human judgment in high-stakes legal contexts. This workadvances legal NLP by providing annotation-free evaluation tools, withimplications for judicial analytics and ethical AI deployment.
摘要: 人工智能在法律自然语言处理领域的快速发展，要求对从司法判决中提取文本的方法进行可扩展评估。本研究评估了16种无监督指标（包括新型公式），以衡量从1000份匿名化俄罗斯司法判决中提取七个语义块的质量，并通过7168份专家评审（采用1-5分李克特量表）进行验证。这些指标涵盖基于文档、语义、结构、伪真实标签及法律专用类别，无需预先标注的真实标签即可运行。通过自举相关性分析、林氏一致性相关系数（CCC）和平均绝对误差（MAE）发现：术语频率一致性（Pearson r=0.540，Lin CCC=0.512，MAE=0.127）与覆盖率/区块完整性（Pearson r=0.513，Lin CCC=0.443，MAE=0.139）与专家评分最吻合，而法律术语密度（Pearson r=-0.479，Lin CCC=-0.079，MAE=0.394）呈现强负相关。基于g4f调用的gpt-4.1-mini生成的LLM评估得分（均值=0.849，Pearson r=0.382，Lin CCC=0.325，MAE=0.197）表现出中等一致性，但其性能表明对法律文本的专业适配有限。研究结果表明，包括LLM方法在内的无监督指标虽能实现可扩展筛查，但因相关性中等且CCC值较低，在高风险法律场景中尚无法完全取代人工判断。本研究通过提供无需标注的评估工具推进了法律NLP发展，对司法分析及伦理AI部署具有启示意义。
Link: http://arxiv.org/abs/2510.01792v1
Updated: 2025-10-02T08:32:16Z

30: Detecting LLM-Generated Spam Reviews by Integrating Language Model  Embeddings and Graph Neural Network
Authors: ['Xin Liu', 'Rongwu Xu', 'Xinyi Jia', 'Jason Liao', 'Jiao Sun', 'Ling Huang', 'Wei Xu']
Summary: The rise of large language models (LLMs) has enabled the generation of highlypersuasive spam reviews that closely mimic human writing. These reviews posesignificant challenges for existing detection systems and threaten thecredibility of online platforms. In this work, we first create three realisticLLM-generated spam review datasets using three distinct LLMs, each guided byproduct metadata and genuine reference reviews. Evaluations by GPT-4.1 confirmthe high persuasion and deceptive potential of these reviews. To address thisthreat, we propose FraudSquad, a hybrid detection model that integrates textembeddings from a pre-trained language model with a gated graph transformer forspam node classification. FraudSquad captures both semantic and behavioralsignals without relying on manual feature engineering or massive trainingresources. Experiments show that FraudSquad outperforms state-of-the-artbaselines by up to 44.22% in precision and 43.01% in recall on threeLLM-generated datasets, while also achieving promising results on twohuman-written spam datasets. Furthermore, FraudSquad maintains a modest modelsize and requires minimal labeled training data, making it a practical solutionfor real-world applications. Our contributions include new synthetic datasets,a practical detection framework, and empirical evidence highlighting theurgency of adapting spam detection to the LLM era. Our code and datasets areavailable at: https://anonymous.4open.science/r/FraudSquad-5389/.
摘要: 大型语言模型（LLMs）的兴起催生了极具说服力的垃圾评论，这些评论能高度模仿人类写作。这些评论对现有检测系统构成重大挑战，并威胁到在线平台的可信度。在这项工作中，我们首先利用三种不同的LLMs，在产品元数据和真实参考评论的指导下，创建了三个真实的LLM生成垃圾评论数据集。GPT-4.1的评估证实了这些评论具有高度的说服力和欺骗潜力。为应对这一威胁，我们提出了FraudSquad，这是一种混合检测模型，将预训练语言模型的文本嵌入与门控图Transformer相结合，用于垃圾节点分类。FraudSquad无需依赖手动特征工程或大量训练资源，即可同时捕捉语义和行为信号。实验表明，在三个LLM生成的数据集上，FraudSquad的精确率比最先进的基线模型高出44.22%，召回率高出43.01%，同时在两个人类编写的垃圾数据集上也取得了令人鼓舞的结果。此外，FraudSquad保持了适中的模型规模，且仅需最少的标记训练数据，使其成为实际应用的可行解决方案。我们的贡献包括新的合成数据集、一个实用的检测框架以及经验证据，凸显了将垃圾检测适应LLM时代的紧迫性。我们的代码和数据集可在以下网址获取：https://anonymous.4open.science/r/FraudSquad-5389/。
Link: http://arxiv.org/abs/2510.01801v1
Updated: 2025-10-02T08:42:35Z

31: Sparse Query Attention (SQA): A Computationally Efficient Attention  Mechanism with Query Heads Reduction
Authors: ['Adam Filipek']
Summary: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)mechanism, has become the de facto standard for state-of-the-art models inartificial intelligence. However, the quadratic computational complexity of MHAwith respect to sequence length presents a significant barrier to scaling,particularly for applications involving long contexts. Prevailing solutions,such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), haveeffectively addressed the memory bandwidth bottleneck that dominatesautoregressive inference latency by sharing Key and Value projections. Whilehighly successful, these methods do not reduce the fundamental number offloating-point operations (FLOPs) required for the attention score computation,which remains a critical bottleneck for training and full-sequence processing.This paper introduces Sparse Query Attention (SQA), a novel attentionarchitecture that pursues an alternative and complementary optimization path.Instead of reducing Key/Value heads, SQA reduces the number of Query heads.This architectural modification directly decreases the computational complexityof the attention mechanism by a factor proportional to the reduction in queryheads, thereby lowering the overall FLOPs. This work presents the theoreticalfoundation of SQA, its mathematical formulation, and a family of architecturalvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstratethat SQA can achieve significant throughput improvements of up to 3x incomputation-bound scenarios such as model pre-training, fine-tuning, andencoder-based tasks, with only a minimal impact on model quality in preliminarysmallscale experiments. SQA was discovered serendipitously during thedevelopment of the upcoming Reactive Transformer architecture, suggesting itspotential as a powerful tool for building more efficient and scalable models
摘要: Transformer架构以多头注意力机制为基础，已成为人工智能领域先进模型的事实标准。然而，多头注意力机制在序列长度上的二次方计算复杂度，构成了其扩展性的重大障碍，尤其对于涉及长上下文的应用而言。主流解决方案，如多查询注意力（MQA）和分组查询注意力（GQA），通过共享键和值的投影，有效解决了主导自回归推理延迟的内存带宽瓶颈。尽管这些方法取得了巨大成功，但它们并未减少注意力分数计算所需的基本浮点运算次数（FLOPs），而这仍然是训练和全序列处理的关键瓶颈。本文引入了稀疏查询注意力（SQA），这是一种新颖的注意力架构，它追求一条替代且互补的优化路径。SQA并非减少键/值头的数量，而是减少查询头的数量。这种架构修改直接将注意力机制的计算复杂度降低了与查询头减少量成正比的倍数，从而降低了整体的浮点运算次数。本工作阐述了SQA的理论基础、数学公式以及一系列架构变体。在长序列（32k-200k个词元）上的实证基准测试表明，在模型预训练、微调和基于编码器的任务等计算密集型场景中，SQA可实现高达3倍的显著吞吐量提升，且在初步的小规模实验中对模型质量的影响极小。SQA是在开发即将推出的响应式Transformer架构期间偶然发现的，这表明它有望成为构建更高效、可扩展模型的强大工具。
Link: http://arxiv.org/abs/2510.01817v1
Updated: 2025-10-02T09:01:38Z

32: Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical  Errors
Authors: ['Dane Williamson', 'Yangfeng Ji', 'Matthew Dwyer']
Summary: Large Language Models (LLMs) demonstrate strong mathematical problem-solvingabilities but frequently fail on problems that deviate syntactically from theirtraining distribution. We identify a systematic failure mode, syntactic blindspots, in which models misapply familiar reasoning strategies to problems thatare semantically straightforward but phrased in unfamiliar ways. These errorsare not due to gaps in mathematical competence, but rather reflect a brittlecoupling between surface form and internal representation. To test this, werephrase incorrectly answered questions using syntactic templates drawn fromcorrect examples. These rephrasings, which preserve semantics while reducingstructural complexity, often lead to correct answers. We quantify syntacticcomplexity using a metric based on Dependency Locality Theory (DLT), and showthat higher DLT scores are associated with increased failure rates acrossmultiple datasets. Our findings suggest that many reasoning errors stem fromstructural misalignment rather than conceptual difficulty, and thatsyntax-aware interventions can reveal and mitigate these inductive failures.
摘要: 大型语言模型（LLMs）展现出强大的数学问题解决能力，但在句法上偏离其训练分布的问题上却常常失败。我们识别出一种系统性的失败模式，即句法盲点，指模型将熟悉的推理策略错误地应用于语义直白但表述方式陌生的问题。这些错误并非源于数学能力的不足，而是反映了表层形式与内部表征之间的脆弱耦合。为验证这一点，我们使用从正确示例中提取的句法模板，对错误回答的问题进行重新表述。这些重新表述在保留语义的同时降低了结构复杂度，往往能引出正确答案。我们采用基于依存局部理论（DLT）的指标来量化句法复杂度，并证明在多个数据集中，较高的DLT得分与更高的失败率相关。我们的研究结果表明，许多推理错误源于结构失调而非概念难度，而注重句法的干预手段可以揭示并缓解这些归纳失败。
Link: http://arxiv.org/abs/2510.01831v1
Updated: 2025-10-02T09:26:26Z

33: SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with  Reinforcement Learning
Authors: ['Shicheng Liu', 'Kai Sun', 'Lisheng Fu', 'Xilun Chen', 'Xinyuan Zhang', 'Zhaojiang Lin', 'Rulin Shao', 'Yue Liu', 'Anuj Kumar', 'Wen-tau Yih', 'Xin Luna Dong']
Summary: Semi-structured content in HTML tables, lists, and infoboxes accounts for asubstantial share of factual data on the web, yet the formatting complicatesusage, and reliably extracting structured information from them remainschallenging. Existing methods either lack generalization or areresource-intensive due to per-page LLM inference. In this paper, we introduceSCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novelreinforcement learning framework that leverages layout similarity acrosswebpages within the same site as a reward signal. Instead of processing eachpage individually, SCRIBES generates reusable extraction scripts that can beapplied to groups of structurally similar webpages. Our approach furtherimproves by iteratively training on synthetic annotations from in-the-wildCommonCrawl data. Experiments show that our approach outperforms strongbaselines by over 13% in script quality and boosts downstream questionanswering accuracy by more than 4% for GPT-4o, enabling scalable andresource-efficient web information extraction.
摘要: HTML表格、列表和信息框中的半结构化内容占据了网络上事实数据的绝大部分，然而其格式复杂性增加了使用难度，并使得从中可靠地提取结构化信息仍然具有挑战性。现有方法要么缺乏泛化能力，要么由于需要对每个页面进行大语言模型推理而资源密集。在本文中，我们介绍了SCRIBES（基于脚本的网页级半结构化内容提取），这是一种新颖的强化学习框架，它利用同一网站内网页之间的布局相似性作为奖励信号。SCRIBES并非单独处理每个页面，而是生成可重复使用的提取脚本，这些脚本可应用于结构相似的网页组。我们的方法通过在来自真实世界的CommonCrawl数据的合成标注上进行迭代训练而得到进一步改进。实验表明，我们的方法在脚本质量上比强大的基线高出13%以上，并将GPT-4o的下游问答准确率提升了4%以上，从而实现了可扩展且资源高效的网页信息提取。
Link: http://arxiv.org/abs/2510.01832v1
Updated: 2025-10-02T09:27:15Z

34: Plan Then Action:High-Level Planning Guidance Reinforcement Learning for  LLM Reasoning
Authors: ['Zhihao Dou', 'Qinjian Zhao', 'Zhongwei Wan', 'Dinggen Zhang', 'Weida Wang', 'Towsif Raiyan', 'Benteng Chen', 'Qingtao Pan', 'Yang Ouyang', 'Zhiqiang Gao', 'Shufei Zhang', 'Sumon Biswas']
Summary: Large language models (LLMs) have demonstrated remarkable reasoning abilitiesin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,due to their autoregressive token-level generation, the reasoning process islargely constrained to local decision-making and lacks global planning. Thislimitation frequently results in redundant, incoherent, or inaccuratereasoning, which significantly degrades overall performance. Existingapproaches, such as tree-based algorithms and reinforcement learning (RL),attempt to address this issue but suffer from high computational costs andoften fail to produce optimal reasoning trajectories. To tackle this challenge,we propose Plan-Then-Action Enhanced Reasoning with Group Relative PolicyOptimization PTA-GRPO, a two-stage framework designed to improve bothhigh-level planning and fine-grained CoT reasoning. In the first stage, weleverage advanced LLMs to distill CoT into compact high-level guidance, whichis then used for supervised fine-tuning (SFT). In the second stage, weintroduce a guidance-aware RL method that jointly optimizes the final outputand the quality of high-level guidance, thereby enhancing reasoningeffectiveness. We conduct extensive experiments on multiple mathematicalreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, acrossdiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, andLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistentlyachieves stable and significant improvements across different models and tasks,validating its effectiveness and generalization.
摘要: 大型语言模型（LLMs）在复杂任务中展现出了卓越的推理能力，通常依赖于思维链（CoT）推理。然而，由于其自回归的逐词生成机制，推理过程在很大程度上局限于局部决策，缺乏全局规划。这一限制常常导致推理过程冗余、不连贯或不准确，从而显著降低了整体性能。现有方法，如基于树的算法和强化学习（RL），试图解决这一问题，但往往面临高昂的计算成本，且难以生成最优的推理轨迹。为应对这一挑战，我们提出了“计划-行动增强推理与群体相对策略优化”（PTA-GRPO），这是一个旨在提升高层规划和细粒度思维链推理的两阶段框架。在第一阶段，我们利用先进的LLMs将思维链提炼为紧凑的高层指导，并用于监督微调（SFT）。在第二阶段，我们引入了一种感知指导的强化学习方法，联合优化最终输出和高层指导的质量，从而增强推理效果。我们在多个数学推理基准测试上进行了广泛实验，包括MATH、AIME2024、AIME2025和AMC，涵盖了Qwen2.5-7B-Instruct、Qwen3-8B、Qwen3-14B和LLaMA3.2-3B等多种基础模型。实验结果表明，PTA-GRPO在不同模型和任务上均实现了稳定且显著的改进，验证了其有效性和泛化能力。
Link: http://arxiv.org/abs/2510.01833v1
Updated: 2025-10-02T09:28:13Z

35: Model Merging to Maintain Language-Only Performance in Developmentally  Plausible Multimodal Models
Authors: ['Ece Takmaz', 'Lisa Bylinina', 'Jakub Dotlacil']
Summary: State-of-the-art vision-and-language models consist of many parameters andlearn from enormous datasets, surpassing the amounts of linguistic data thatchildren are exposed to as they acquire a language. This paper presents ourapproach to the multimodal track of the BabyLM challenge addressing thisdiscrepancy. We develop language-only and multimodal models in low-resourcesettings using developmentally plausible datasets, with our multimodal modelsoutperforming previous BabyLM baselines. One finding in the multimodal languagemodel literature is that these models tend to underperform in\textit{language-only} tasks. Therefore, we focus on maintaining language-onlyabilities in multimodal models. To this end, we experiment with \textit{modelmerging}, where we fuse the parameters of multimodal models with those oflanguage-only models using weighted linear interpolation. Our resultscorroborate the findings that multimodal models underperform in language-onlybenchmarks that focus on grammar, and model merging with text-only models canhelp alleviate this problem to some extent, while maintaining multimodalperformance.
摘要: 最先进的视觉与语言模型包含大量参数，并从海量数据集中学习，其数据量远超儿童在语言习得过程中所接触到的语言数据量。本文提出了一种旨在解决这一差异的BabyLM挑战多模态赛道的方法。我们使用符合发展规律的数据集，在低资源环境下开发了仅语言模型和多模态模型，其中我们的多模态模型表现优于之前的BabyLM基线模型。多模态语言模型文献中的一个发现是，这些模型在仅语言任务上往往表现不佳。因此，我们专注于在多模态模型中保持仅语言能力。为此，我们尝试了模型合并技术，即通过加权线性插值将多模态模型的参数与仅语言模型的参数进行融合。我们的结果证实了多模态模型在专注于语法的仅语言基准测试中表现不佳的发现，而与纯文本模型进行模型合并可以在一定程度上缓解这一问题，同时保持多模态性能。
Link: http://arxiv.org/abs/2510.01845v1
Updated: 2025-10-02T09:38:25Z

36: REPAIR: Robust Editing via Progressive Adaptive Intervention and  Reintegration
Authors: ['Yisu Wang', 'Ming Wang', 'Haoyuan Song', 'Wenjie Huang', 'Chaozheng Wang', 'Yi Xie', 'Xuming Ran']
Summary: Post-training for large language models (LLMs) is constrained by the highcost of acquiring new knowledge or correcting errors and by the unintended sideeffects that frequently arise from retraining. To address these issues, weintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention andReintegration), a lifelong editing framework designed to support precise andlow-cost model updates while preserving non-target knowledge. REPAIR mitigatesthe instability and conflicts of large-scale sequential edits through aclosed-loop feedback mechanism coupled with dynamic memory management.Furthermore, by incorporating frequent knowledge fusion and enforcing stronglocality guards, REPAIR effectively addresses the shortcomings of traditionaldistribution-agnostic approaches that often overlook unintended ripple effects.Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%across multiple model families and significantly reduces knowledge forgetting.This work introduces a robust framework for developing reliable, scalable, andcontinually evolving LLMs.
摘要: 大型语言模型（LLM）的后训练受到获取新知识或纠正错误的高成本以及再训练中频繁出现的意外副作用的制约。为解决这些问题，我们提出了REPAIR（通过渐进式自适应干预与再整合实现鲁棒编辑），这是一种终身编辑框架，旨在支持精确且低成本的模型更新，同时保留非目标知识。REPAIR通过结合闭环反馈机制与动态内存管理，缓解了大规模顺序编辑中的不稳定性和冲突。此外，通过融入频繁的知识融合并实施强局部性保护，REPAIR有效解决了传统分布不可知方法常忽略意外涟漪效应的缺陷。我们的实验表明，REPAIR在多个模型家族中将编辑准确性提升了10%-30%，并显著减少了知识遗忘。这项工作为开发可靠、可扩展且持续进化的LLM引入了一个鲁棒框架。
Link: http://arxiv.org/abs/2510.01879v1
Updated: 2025-10-02T10:35:39Z

37: Constrained Adaptive Rejection Sampling
Authors: ['Paweł Parys', 'Sairam Vaidya', 'Taylor Berg-Kirkpatrick', "Loris D'Antoni"]
Summary: Language Models (LMs) are increasingly used in applications where generatedoutputs must satisfy strict semantic or syntactic constraints. Existingapproaches to constrained generation fall along a spectrum: greedy constraineddecoding methods enforce validity during decoding but distort the LM'sdistribution, while rejection sampling (RS) preserves fidelity but wastescomputation by discarding invalid outputs. Both extremes are problematic indomains such as program fuzzing, where both validity and diversity of samplesare essential. We present Constrained Adaptive Rejection Sampling (CARS), anapproach that strictly improves the sample-efficiency of RS withoutdistributional distortion. CARS begins with unconstrained LM sampling andadaptively rules out constraint-violating continuations by recording them in atrie and subtracting their probability mass from future draws. This adaptivepruning ensures that prefixes proven invalid are never revisited, acceptancerates improve monotonically, and the resulting samples exactly follow theconstrained distribution. In experiments on a variety of domains -- e.g.,program fuzzing and molecular generation -- CARS consistently achieves higherefficiency -- measured in the number of LM forward passes per valid sample --while also producing stronger sample diversity than both GCD and methods thatapproximate the LM's distribution.
摘要: 语言模型（LMs）在生成输出必须满足严格语义或句法约束的应用中日益普及。现有的约束生成方法可分为两类：贪婪约束解码方法在解码过程中强制保证有效性，但会扭曲语言模型的分布；而拒绝采样（RS）则保持保真度，但通过丢弃无效输出浪费了计算资源。在程序模糊测试等领域，这两类极端方法均存在问题，因为样本的有效性和多样性都至关重要。我们提出了约束自适应拒绝采样（CARS），该方法在不扭曲分布的前提下，严格提升了拒绝采样的样本效率。CARS从无约束的语言模型采样开始，通过将违反约束的后续内容记录在字典树中并从后续抽取中减去其概率质量，自适应地排除这些内容。这种自适应修剪确保了已证明无效的前缀不会被重复访问，接受率单调提升，且生成的样本严格遵循约束分布。在程序模糊测试和分子生成等多个领域的实验中，CARS在效率（以每个有效样本的语言模型前向传播次数衡量）和样本多样性方面，均持续优于贪婪约束解码方法以及近似语言模型分布的方法。
Link: http://arxiv.org/abs/2510.01902v1
Updated: 2025-10-02T11:17:26Z

38: Enhancing Large Language Model Reasoning with Reward Models: An  Analytical Survey
Authors: ['Qiyuan Liu', 'Hao Xu', 'Xuhong Chen', 'Wei Chen', 'Yee Whye Teh', 'Ning Miao']
Summary: Reward models (RMs) play a critical role in enhancing the reasoningperformance of LLMs. For example, they can provide training signals to finetuneLLMs during reinforcement learning (RL) and help select the best answer frommultiple candidates during inference. In this paper, we provide a systematicintroduction to RMs, along with a comprehensive survey of their applications inLLM reasoning. We first review fundamental concepts of RMs, including theirarchitectures, training methodologies, and evaluation techniques. Then, weexplore their key applications: (1) guiding generation and selecting optimaloutputs during LLM inference, (2) facilitating data synthesis and iterativeself-improvement for LLMs, and (3) providing training signals in RL-basedfinetuning. Finally, we address critical open questions regarding theselection, generalization, evaluation, and enhancement of RMs, based onexisting research and our own empirical findings. Our analysis aims to provideactionable insights for the effective deployment and advancement of RMs for LLMreasoning.
摘要: 奖励模型（RM）在提升大型语言模型（LLM）的推理性能方面发挥着关键作用。例如，在强化学习（RL）期间，它们可以提供训练信号来微调LLM，并在推理过程中帮助从多个候选答案中选择最佳答案。在本文中，我们对RM进行了系统性的介绍，并全面调研了它们在LLM推理中的应用。我们首先回顾了RM的基本概念，包括其架构、训练方法和评估技术。接着，我们探讨了它们的关键应用：（1）在LLM推理过程中引导生成并选择最优输出，（2）促进LLM的数据合成和迭代自我改进，以及（3）在基于强化学习的微调中提供训练信号。最后，基于现有研究和我们自己的实证发现，我们探讨了关于RM的选择、泛化、评估和提升的关键开放性问题。我们的分析旨在为有效部署和推进用于LLM推理的RM提供可行的见解。
Link: http://arxiv.org/abs/2510.01925v1
Updated: 2025-10-02T11:42:17Z

39: Inverse Language Modeling towards Robust and Grounded LLMs
Authors: ['Davide Gabrielli', 'Simone Sestito', 'Iacopo Masi']
Summary: The current landscape of defensive mechanisms for LLMs is fragmented andunderdeveloped, unlike prior work on classifiers. To further promoteadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), aunified framework that simultaneously 1) improves the robustness of LLMs toinput perturbations, and, at the same time, 2) enables native grounding byinverting model outputs to identify potentially toxic or unsafe input triggers.ILM transforms LLMs from static generators into analyzable and robust systems,potentially helping RED teaming. ILM can lay the foundation for next-generationLLMs that are not only robust and grounded but also fundamentally morecontrollable and trustworthy. The code is publicly available atgithub.com/davegabe/pag-llm.
摘要: 当前，大语言模型的防御机制格局是碎片化且欠发达的，这与先前关于分类器的研究不同。为进一步提升大语言模型的对抗鲁棒性，我们提出了逆向语言建模，这是一个统一框架，它同时 1) 增强大语言模型对输入扰动的鲁棒性，并且 2) 通过反转模型输出来识别潜在的有害或不安全输入触发器，从而实现原生溯源。逆向语言建模将大语言模型从静态生成器转变为可分析和鲁棒的系统，可能有助于红队测试。逆向语言建模能够为下一代大语言模型奠定基础，使其不仅鲁棒且可溯源，而且从根本上更具可控性和可信度。代码已在 github.com/davegabe/pag-llm 公开。
Link: http://arxiv.org/abs/2510.01929v1
Updated: 2025-10-02T11:47:18Z

40: Veri-R1: Toward Precise and Faithful Claim Verification via Online  Reinforcement Learning
Authors: ['Qi He', 'Cheng Qian', 'Xiusi Chen', 'Bingxiang He', 'Yi R.', 'Fung', 'Heng Ji']
Summary: Claim verification with large language models (LLMs) has recently attractedconsiderable attention, owing to their superior reasoning capabilities andtransparent verification pathways compared to traditional answer-onlyjudgments. Online claim verification requires iterative evidence retrieval andreasoning, yet existing approaches mainly rely on prompt engineering orpredesigned reasoning workflows without offering a unified training paradigm toimprove necessary skills. Therefore, we introduce Veri-R1, an onlinereinforcement learning (RL) framework that enables an LLM to interact with asearch engine and to receive reward signals that explicitly shape its planning,retrieval, and reasoning behaviors. The dynamic interaction between models andretrieval systems more accurately reflects real-world verification scenariosand fosters comprehensive verification skills. Empirical results show thatVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, oftensurpassing larger-scale counterparts. Ablation studies further reveal theimpact of reward components and the link between output logits and labelaccuracy. Our results highlight the effectiveness of online RL for precise andfaithful claim verification and provide a foundation for future research. Werelease our code to support community progress in LLM empowered claimverification.
摘要: 利用大型语言模型（LLM）进行事实核查近来备受关注，因其相较于传统的仅给出答案的判断方式，具有卓越的推理能力和透明的核查路径。在线事实核查需要迭代式的证据检索与推理，然而现有方法主要依赖于提示工程或预设计的推理工作流，未能提供一个统一的训练范式来提升必要的技能。为此，我们提出了Veri-R1，一种在线强化学习（RL）框架，使LLM能够与搜索引擎交互，并接收显式塑造其规划、检索和推理行为的奖励信号。模型与检索系统之间的动态交互更准确地反映了真实世界的核查场景，并培养了全面的核查技能。实证结果表明，Veri-R1将联合准确率提升了高达30%，证据分数翻倍，且性能常常超越更大规模的模型。消融研究进一步揭示了奖励组件的影响以及输出logits与标签准确率之间的关联。我们的研究结果突显了在线强化学习在精确且忠实的事实核查中的有效性，并为未来研究奠定了基础。我们公开了代码以支持社区在LLM赋能的事实核查领域取得进展。
Link: http://arxiv.org/abs/2510.01932v1
Updated: 2025-10-02T11:49:48Z

41: Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,  Argument, and Topic Annotations
Authors: ['Adina Nicola Dobrinoiu', 'Ana Cristiana Marcu', 'Amir Homayounirad', 'Luciano Cavalcante Siebert', 'Enrico Liscio']
Summary: Our interpretation of value concepts is shaped by our socioculturalbackground and lived experiences, and is thus subjective. Recognizingindividual value interpretations is important for developing AI systems thatcan align with diverse human perspectives and avoid bias toward majorityviewpoints. To this end, we investigate whether a language model can predictindividual value interpretations by leveraging multi-dimensional subjectiveannotations as a proxy for their interpretive lens. That is, we evaluatewhether providing examples of how an individual annotates Sentiment, Emotion,Argument, and Topics (SEAT dimensions) helps a language model in predictingtheir value interpretations. Our experiment across different zero- and few-shotsettings demonstrates that providing all SEAT dimensions simultaneously yieldssuperior performance compared to individual dimensions and a baseline where noinformation about the individual is provided. Furthermore, individualvariations across annotators highlight the importance of accounting for theincorporation of individual subjective annotators. To the best of ourknowledge, this controlled setting, although small in size, is the firstattempt to go beyond demographics and investigate the impact of annotationbehavior on value prediction, providing a solid foundation for futurelarge-scale validation.
摘要: 我们对价值概念的理解，受到我们的社会文化背景和生活经历的影响，因此是主观的。认识到个体价值解读的重要性，对于开发能够与多样化人类视角保持一致并避免偏向多数观点的人工智能系统至关重要。为此，我们探究了一个语言模型能否通过利用多维主观注释作为其解读视角的代理来预测个体的价值解读。也就是说，我们评估了提供个体如何标注情感、情绪、论证和主题（SEAT维度）的示例，是否能帮助语言模型预测其价值解读。我们在不同零样本和少样本设置下的实验表明，同时提供所有SEAT维度，相较于单独提供各维度或不提供任何个体信息的基线，能取得更优的性能。此外，注释者之间的个体差异凸显了考虑个体主观注释者整合的重要性。据我们所知，这一受控设置尽管规模较小，却是首次超越人口统计学范畴，探究注释行为对价值预测的影响，为未来大规模验证奠定了坚实的基础。
Link: http://arxiv.org/abs/2510.01976v1
Updated: 2025-10-02T12:51:33Z

42: Exploring Database Normalization Effects on SQL Generation
Authors: ['Ryosuke Kohita']
Summary: Schema design, particularly normalization, is a critical yet often overlookedfactor in natural language to SQL (NL2SQL) systems. Most prior researchevaluates models on fixed schemas, overlooking the influence of design onperformance. We present the first systematic study of schema normalization'simpact, evaluating eight leading large language models on synthetic andreal-world datasets with varied normalization levels. We construct controlledsynthetic datasets with formal normalization (1NF-3NF) and real academic paperdatasets with practical schemes. Our results show that denormalized schemasoffer high accuracy on simple retrieval queries, even with cost-effectivemodels in zero-shot settings. In contrast, normalized schemas (2NF/3NF)introduce challenges such as errors in base table selection and join typeprediction; however, these issues are substantially mitigated by providingfew-shot examples. For aggregation queries, normalized schemas yielded betterperformance, mainly due to their robustness against the data duplication andNULL value issues that cause errors in denormalized schemas. These findingssuggest that the optimal schema design for NL2SQL applications depends on thetypes of queries to be supported. Our study demonstrates the importance ofconsidering schema design when developing NL2SQL interfaces and integratingadaptive schema selection for real-world scenarios.
摘要: 模式设计，特别是规范化，是自然语言转SQL（NL2SQL）系统中一个关键却常被忽视的因素。以往的大多数研究在固定模式上评估模型，忽略了设计对性能的影响。我们首次对模式规范化的影响进行了系统性研究，在具有不同规范化水平的合成数据集和真实世界数据集上评估了八种领先的大型语言模型。我们构建了具有正式规范化（1NF-3NF）的受控合成数据集和具有实用模式的真实学术论文数据集。我们的研究结果表明，非规范化模式在简单检索查询上提供高准确性，即使在零样本设置下使用经济高效的模型也是如此。相比之下，规范化模式（2NF/3NF）带来了诸如基表选择错误和连接类型预测错误等挑战；然而，通过提供少样本示例，这些问题得到了显著缓解。对于聚合查询，规范化模式表现更佳，主要归因于它们对数据重复和NULL值问题的鲁棒性，这些问题在非规范化模式中会导致错误。这些发现表明，NL2SQL应用的最佳模式设计取决于需要支持的查询类型。我们的研究证明了在开发NL2SQL接口时考虑模式设计的重要性，并为真实场景整合自适应模式选择提供了依据。
Link: http://arxiv.org/abs/2510.01989v1
Updated: 2025-10-02T13:11:30Z

43: LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and  Target
Authors: ['Md Arid Hasan', 'Firoj Alam', 'Md Fahad Hossain', 'Usman Naseem', 'Syed Ishtiaque Ahmed']
Summary: Online social media platforms are central to everyday communication andinformation seeking. While these platforms serve positive purposes, they alsoprovide fertile ground for the spread of hate speech, offensive language, andbullying content targeting individuals, organizations, and communities. Suchcontent undermines safety, participation, and equity online. Reliable detectionsystems are therefore needed, especially for low-resource languages wheremoderation tools are limited. In Bangla, prior work has contributed resourcesand models, but most are single-task (e.g., binary hate/offense) with limitedcoverage of multi-facet signals (type, severity, target). We address these gapsby introducing the first multi-task Bangla hate-speech dataset,BanglaMultiHate, one of the largest manually annotated corpus to date. Buildingon this resource, we conduct a comprehensive, controlled comparison spanningclassical baselines, monolingual pretrained models, and LLMs under zero-shotprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in alow-resource setting and reveal a consistent trend: although LoRA-tuned LLMsare competitive with BanglaBERT, culturally and linguistically groundedpretraining remains critical for robust performance. Together, our dataset andfindings establish a stronger benchmark for developing culturally alignedmoderation tools in low-resource contexts. For reproducibility, we will releasethe dataset and all related scripts.
摘要: 在线社交媒体平台是日常沟通和信息获取的核心。尽管这些平台具有积极用途，但也为针对个人、组织和社区的仇恨言论、冒犯性语言及霸凌内容的传播提供了沃土。此类内容破坏了网络环境的安全性、参与度和公平性。因此，可靠的检测系统必不可少，尤其是对于审核工具有限的低资源语言。在孟加拉语领域，先前的研究已贡献了资源和模型，但大多数是单任务（例如二元仇恨/冒犯分类），对多维度信号（类型、严重程度、目标）的覆盖有限。我们通过引入首个多任务孟加拉语仇恨言论数据集BanglaMultiHate来解决这些空白，该数据集是迄今为止规模最大的手动标注语料库之一。基于此资源，我们进行了全面的受控比较，涵盖经典基线模型、单语预训练模型以及在零样本提示和LoRA微调下的LLM。我们的实验评估了LLM在低资源环境下的适应性，并揭示了一致趋势：尽管经过LoRA微调的LLM与BanglaBERT具有竞争力，但基于文化和语言特征的预训练对于实现稳健性能仍至关重要。总之，我们的数据集和发现为在低资源环境中开发文化适配的审核工具建立了更坚实的基准。为确保可复现性，我们将发布该数据集及所有相关脚本。
Link: http://arxiv.org/abs/2510.01995v1
Updated: 2025-10-02T13:17:11Z

44: Style Over Story: A Process-Oriented Study of Authorial Creativity in  Large Language Models
Authors: ['Donghoon Jung', 'Jiwoo Choi', 'Songeun Chae', 'Seohyon Jung']
Summary: Evaluations of large language models (LLMs)' creativity have focusedprimarily on the quality of their outputs rather than the processes that shapethem. This study takes a process-oriented approach, drawing on narratology toexamine LLMs as computational authors. We introduce constraint-baseddecision-making as a lens for authorial creativity. Using controlled promptingto assign authorial personas, we analyze the creative preferences of themodels. Our findings show that LLMs consistently emphasize Style over otherelements, including Character, Event, and Setting. By also probing thereasoning the models provide for their choices, we show that distinctiveprofiles emerge across models and argue that our approach provides a novelsystematic tool for analyzing AI's authorial creativity.
摘要: 对大型语言模型（LLMs）创造力的评估，主要聚焦于其输出的质量，而非塑造这些输出的过程。本研究采用过程导向的方法，借鉴叙事学将LLMs视为计算作者进行分析。我们引入基于约束的决策作为审视作者创造力的视角。通过使用受控提示来分配作者角色，我们分析了模型的创造性偏好。研究结果显示，LLMs始终将风格置于其他元素（包括角色、事件和场景）之上。此外，通过探究模型为其选择提供的推理依据，我们发现不同模型间呈现出独特的特征，并论证了我们的方法为分析AI的作者创造力提供了一种新颖的系统化工具。
Link: http://arxiv.org/abs/2510.02025v1
Updated: 2025-10-02T13:57:14Z

45: Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming  Tool Usage
Authors: ['Siddhant Arora', 'Haidar Khan', 'Kai Sun', 'Xin Luna Dong', 'Sajal Choudhary', 'Seungwhan Moon', 'Xinyuan Zhang', 'Adithya Sagar', 'Surya Teja Appini', 'Kaushik Patnaik', 'Sanat Sharma', 'Shinji Watanabe', 'Anuj Kumar', 'Ahmed Aly', 'Yue Liu', 'Florian Metze', 'Zhaojiang Lin']
Summary: End-to-end speech-in speech-out dialogue systems are emerging as a powerfulalternative to traditional ASR-LLM-TTS pipelines, generating more natural,expressive responses with significantly lower latency. However, these systemsremain prone to hallucinations due to limited factual grounding. Whiletext-based dialogue systems address this challenge by integrating tools such asweb search and knowledge graph APIs, we introduce the first approach to extendtool use directly into speech-in speech-out systems. A key challenge is thattool integration substantially increases response latency, disruptingconversational flow. To mitigate this, we propose Streaming Retrieval-AugmentedGeneration (Streaming RAG), a novel framework that reduces user-perceivedlatency by predicting tool queries in parallel with user speech, even beforethe user finishes speaking. Specifically, we develop a post-training pipelinethat teaches the model when to issue tool calls during ongoing speech and howto generate spoken summaries that fuse audio queries with retrieved textresults, thereby improving both accuracy and responsiveness. To evaluate ourapproach, we construct AudioCRAG, a benchmark created by converting queriesfrom the publicly available CRAG dataset into speech form. Experimental resultsdemonstrate that our streaming RAG approach increases QA accuracy by up to 200%relative (from 11.1% to 34.2% absolute) and further enhances user experience byreducing tool use latency by 20%. Importantly, our streaming RAG approach ismodality-agnostic and can be applied equally to typed input, paving the way formore agentic, real-time AI assistants.
摘要: 端到端的语音输入语音输出对话系统正成为传统ASR-LLM-TTS管道的一种有力替代方案，能够生成更自然、更具表现力的响应，且延迟显著降低。然而，由于事实依据有限，这些系统仍然容易出现幻觉问题。虽然基于文本的对话系统通过整合网络搜索和知识图谱API等工具来解决这一挑战，但我们首次提出了一种将工具使用直接扩展至语音输入语音输出系统的方法。一个关键挑战在于，工具集成会大幅增加响应延迟，从而破坏对话的流畅性。为缓解此问题，我们提出了流式检索增强生成（Streaming RAG），这是一种新颖的框架，通过在用户语音播放期间并行预测工具查询（甚至在用户结束讲话之前），来降低用户感知的延迟。具体而言，我们开发了一个后训练流程，教导模型在持续语音过程中何时发起工具调用，以及如何生成融合音频查询与检索文本结果的语音摘要，从而同时提升准确性和响应速度。为评估我们的方法，我们构建了AudioCRAG，这是一个通过将公开可用的CRAG数据集中的查询转换为语音形式而创建的基准。实验结果表明，我们的流式RAG方法将问答准确性提高了高达200%的相对值（绝对值从11.1%提升至34.2%），并通过将工具使用延迟降低20%进一步改善了用户体验。重要的是，我们的流式RAG方法具有模态不可知性，可同等应用于键入输入，为更智能的实时AI助手铺平了道路。
Link: http://arxiv.org/abs/2510.02044v1
Updated: 2025-10-02T14:18:20Z

46: Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken  Dialogue Systems
Authors: ['Siddhant Arora', 'Jinchuan Tian', 'Hayato Futami', 'Jiatong Shi', 'Yosuke Kashiwagi', 'Emiru Tsunoo', 'Shinji Watanabe']
Summary: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activitydetection (VAD) for turn-taking, but VAD fails to distinguish between pausesand turn completions. Duplex SDS models address this by predicting outputcontinuously, including silence tokens, thus removing the need for explicitVAD. However, they often have complex dual-channel architecture and lag behindcascaded models in semantic reasoning. To overcome these challenges, we proposeSCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternatingbetween processing fixed-duration user input and generating responses in ablockwise manner. Using frame-level alignments, we create intermediatetargets-aligned user transcripts and system responses for each block.Experiments show that our approach produces more coherent and interpretableresponses than existing duplex methods while supporting lower-latency andoverlapping interactions compared to turn-by-turn systems.
摘要: 大多数端到端（E2E）口语对话系统（SDS）依赖语音活动检测（VAD）进行话轮转换，但VAD无法区分停顿和话轮结束。双工SDS模型通过持续预测输出来解决这一问题，包括静音令牌，从而消除了对显式VAD的需求。然而，它们通常具有复杂的双通道架构，并且在语义推理方面落后于级联模型。为了克服这些挑战，我们提出了SCoT：一种用于双工SDS的流式思维链（CoT）框架，以分块方式交替处理固定时长的用户输入和生成响应。利用帧级对齐，我们为每个块创建了对齐中间目标的用户转录文本和系统响应。实验表明，与现有双工方法相比，我们的方法能生成更连贯且可解释的响应，同时支持比逐轮系统更低的延迟和重叠交互。
Link: http://arxiv.org/abs/2510.02066v1
Updated: 2025-10-02T14:33:05Z

47: Do AI Models Perform Human-like Abstract Reasoning Across Modalities?
Authors: ['Claas Beger', 'Ryan Yi', 'Shuhao Fu', 'Arseny Moskvichev', 'Sarah W. Tsai', 'Sivasankaran Rajamanickam', 'Melanie Mitchell']
Summary: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGIbenchmark, but does that mean state-of-the-art models recognize and reason withthe abstractions that the task creators intended? We investigate models'abstraction abilities on ConceptARC. We evaluate models under settings thatvary the input modality (textual vs. visual), whether the model is permitted touse external Python tools, and, for reasoning models, the amount of reasoningeffort. In addition to measuring output accuracy, we perform fine-grainedevaluation of the natural-language rules that models generate to explain theirsolutions. This dual evaluation lets us assess whether models solve tasks usingthe abstractions ConceptARC was designed to elicit, rather than relying onsurface-level patterns. Our results show that, while some models usingtext-based representations match human output accuracy, the best models' rulesare often based on surface-level ``shortcuts'' and capture intendedabstractions far less often than humans. Thus their capabilities for generalabstract reasoning may be overestimated by evaluations based on accuracy alone.In the visual modality, AI models' output accuracy drops sharply, yet ourrule-level analysis reveals that models might be underestimated, as they stillexhibit a substantial share of rules that capture intended abstractions, butare often unable to correctly apply these rules. In short, our results showthat models still lag humans in abstract reasoning, and that using accuracyalone to evaluate abstract reasoning on ARC-like tasks may overestimateabstract-reasoning capabilities in textual modalities and underestimate it invisual modalities. We believe that our evaluation framework offers a morefaithful picture of multimodal models' abstract reasoning abilities and a moreprincipled way to track progress toward human-like, abstraction-centeredintelligence.
摘要: OpenAI的o3-preview推理模型在ARC-AGI基准测试上超越了人类准确率，但这是否意味着最先进的模型能够识别并运用任务创建者所设想的抽象概念进行推理？我们在ConceptARC上对模型的抽象能力进行了研究。我们在不同的设置下评估模型，包括改变输入模态（文本与视觉）、是否允许模型使用外部Python工具，以及对于推理模型，推理投入的程度。除了衡量输出准确率外，我们还对模型为解释其解决方案而生成的自然语言规则进行了细粒度评估。这种双重评估使我们能够判断模型是否运用了ConceptARC旨在激发的抽象概念来解决问题，而非依赖表面模式。我们的结果显示，尽管一些使用文本表示的模型达到了与人类相当的输出准确率，但最佳模型的规则往往基于表面“捷径”，其捕捉预期抽象概念的频率远低于人类。因此，仅基于准确率的评估可能会高估模型的通用抽象推理能力。在视觉模态下，AI模型的输出准确率急剧下降，但我们的规则层面分析揭示，模型的能力可能被低估，因为它们仍表现出相当比例的捕捉预期抽象概念的规则，只是常常无法正确应用这些规则。总之，我们的结果显示模型在抽象推理方面仍落后于人类，并且仅用准确率来评估ARC类任务上的抽象推理能力，可能会在文本模态下高估、在视觉模态下低估这种能力。我们相信，我们的评估框架为多模态模型的抽象推理能力提供了更真实的图景，也为追踪向类人的、以抽象为中心的智能的进展提供了一种更有原则的方法。
Link: http://arxiv.org/abs/2510.02125v1
Updated: 2025-10-02T15:35:10Z

48: The Disparate Impacts of Speculative Decoding
Authors: ['Jameson Sandler', 'Ahmet Üstün', 'Marco Romanelli', 'Sara Hooker', 'Ferdinando Fioretto']
Summary: The practice of speculative decoding, whereby inference is probabilisticallysupported by a smaller, cheaper, ``drafter'' model, has become a standardtechnique for systematically reducing the decoding time of large languagemodels. This paper conducts an analysis of speculative decoding through thelens of its potential disparate speed-up rates across tasks. Crucially, thepaper shows that speed-up gained from speculative decoding is not uniformlydistributed across tasks, consistently diminishing for under-fit, and oftenunderrepresented tasks. To better understand this phenomenon, we derive ananalysis to quantify this observed ``unfairness'' and draw attention to thefactors that motivate such disparate speed-ups to emerge. Further, guided bythese insights, the paper proposes a mitigation strategy designed to reducespeed-up disparities and validates the approach across several model pairs,revealing on average a 12% improvement in our fairness metric.
摘要: 投机解码（speculative decoding）的做法，即通过一个更小、更便宜的“草稿”模型以概率方式支持推理，已成为系统性地减少大型语言模型解码时间的标准技术。本文从其在不同任务上可能存在的不同加速率的角度，对投机解码进行了分析。关键在于，本文表明，从投机解码中获得的加速在任务间并非均匀分布，对于欠拟合（under-fit）且通常代表性不足的任务，加速效果会持续减弱。为了更好地理解这一现象，我们推导出一种分析方法来量化这种观察到的“不公平性”，并揭示了促使此类不同加速率出现的因素。此外，在这些见解的指导下，本文提出了一种旨在减少加速差异的缓解策略，并在多个模型组合上验证了该方法，结果显示我们的公平性指标平均提升了12%。
Link: http://arxiv.org/abs/2510.02128v1
Updated: 2025-10-02T15:38:57Z

49: RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with  Self-Penalization
Authors: ['Zhaoning Yu', 'Will Su', 'Leitian Tao', 'Haozhu Wang', 'Aashu Singh', 'Hanchao Yu', 'Jianyu Wang', 'Hongyang Gao', 'Weizhe Yuan', 'Jason Weston', 'Ping Yu', 'Jing Xu']
Summary: Reinforcement learning with human-annotated data has boosted chain-of-thoughtreasoning in large reasoning models, but these gains come at high costs inlabeled data while faltering on harder tasks. A natural next step isexperience-driven learning, where models improve without curated labels byadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning withSelf-restraint), a self-penalizing RL framework that converts the absence ofgold labels into a useful learning signal. Instead of overcommitting tospurious majority votes, RESTRAIN exploits signals from the model's entireanswer distribution: penalizing overconfident rollouts and low-consistencyexamples while preserving promising reasoning chains. The self-penalizationmechanism integrates seamlessly into policy optimization methods such as GRPO,enabling continual self-improvement without supervision. On challengingreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent onGPQA-Diamond, nearly matching gold-label training while using no gold labels.These results demonstrate that RESTRAIN establishes a scalable path towardstronger reasoning without gold labels.
摘要: 利用人类标注数据进行强化学习，提升了大型推理模型的思维链推理能力，但这些提升在标注数据方面成本高昂，且在面对更难的任务时表现不佳。一个自然的下一步是经验驱动学习，即模型通过适应无标签数据来改进，而无需经过精心筛选的标签。我们提出了RESTRAIN（带自我约束的强化学习），这是一种自惩罚强化学习框架，能将黄金标签的缺失转化为有用的学习信号。RESTRAIN不依赖于虚假的多数投票，而是利用模型整个答案分布的信号：惩罚过于自信的展开和低一致性样本，同时保留有前景的推理链。自惩罚机制能够无缝集成到GRPO等策略优化方法中，实现无需监督的持续自我提升。在具有挑战性的推理基准测试中，RESTRAIN仅使用无标签数据就取得了显著提升。在Qwen3-4B-Base和OctoThinker Hybrid-8B-Base上，它将AIME25上的Pass@1指标提升了高达140.7%，MMLU_STEM上提升了36.2%，GPQA-Diamond上提升了19.6%，几乎与使用黄金标签的训练效果相当，且完全没有使用黄金标签。这些结果表明，RESTRAIN为在无黄金标签的情况下实现更强的推理能力开辟了一条可扩展的路径。
Link: http://arxiv.org/abs/2510.02172v1
Updated: 2025-10-02T16:24:01Z

50: Learning to Reason for Hallucination Span Detection
Authors: ['Hsuan Su', 'Ting-Yao Hu', 'Hema Swetha Koppula', 'Kundan Krishna', 'Hadi Pouransari', 'Cheng-Yu Hsieh', 'Cem Koc', 'Joseph Yitan Cheng', 'Oncel Tuzel', 'Raviteja Vemulapalli']
Summary: Large language models (LLMs) often generate hallucinations -- unsupportedcontent that undermines reliability. While most prior works frame hallucinationdetection as a binary task, many real-world applications require identifyinghallucinated spans, which is a multi-step decision making process. Thisnaturally raises the question of whether explicit reasoning can help thecomplex task of detecting hallucination spans. To answer this question, wefirst evaluate pretrained models with and without Chain-of-Thought (CoT)reasoning, and show that CoT reasoning has the potential to generate at leastone correct answer when sampled multiple times. Motivated by this, we proposeRL4HS, a reinforcement learning framework that incentivizes reasoning with aspan-level reward function. RL4HS builds on Group Relative Policy Optimizationand introduces Class-Aware Policy Optimization to mitigate reward imbalanceissue. Experiments on the RAGTruth benchmark (summarization, questionanswering, data-to-text) show that RL4HS surpasses pretrained reasoning modelsand supervised fine-tuning, demonstrating the necessity of reinforcementlearning with span-level rewards for detecting hallucination spans.
摘要: 大型语言模型（LLMs）经常产生幻觉——即缺乏依据的内容，这会削弱其可靠性。尽管此前大多数研究将幻觉检测视为一项二元任务，但许多实际应用需要识别出具体的幻觉片段，这是一个多步骤的决策过程。这自然引出一个问题：显式推理是否有助于检测幻觉片段这一复杂任务。为回答此问题，我们首先评估了预训练模型在有无思维链（CoT）推理时的表现，并表明当多次采样时，CoT推理有潜力生成至少一个正确答案。受此启发，我们提出了RL4HS，一种通过片段级奖励函数激励推理的强化学习框架。RL4HS基于群体相对策略优化构建，并引入类别感知策略优化以缓解奖励不平衡问题。在RAGTruth基准测试（涵盖摘要生成、问答、数据到文本任务）上的实验表明，RL4HS超越了预训练推理模型和监督微调方法，证明了采用片段级奖励的强化学习对于检测幻觉片段的必要性。
Link: http://arxiv.org/abs/2510.02173v1
Updated: 2025-10-02T16:24:28Z

51: A Rigorous Benchmark with Multidimensional Evaluation for Deep Research  Agents: From Answers to Reports
Authors: ['Yang Yao', 'Yixu Wang', 'Yuxuan Zhang', 'Yi Lu', 'Tianle Gu', 'Lingyu Li', 'Dingyi Zhao', 'Keming Wu', 'Haozhe Wang', 'Ping Nie', 'Yan Teng', 'Yingchun Wang']
Summary: Artificial intelligence is undergoing the paradigm shift from closed languagemodels to interconnected agent systems capable of external perception andinformation integration. As a representative embodiment, Deep Research Agents(DRAs) systematically exhibit the capabilities for task decomposition,cross-source retrieval, multi-stage reasoning, and structured output, whichmarkedly enhance performance on complex and open-ended tasks. However, existingbenchmarks remain deficient in evaluation dimensions, response formatting, andscoring mechanisms, limiting their capacity to assess such systems effectively.This paper introduces a rigorous benchmark and a multidimensional evaluationframework tailored to DRAs and report-style responses. The benchmark comprises214 expert-curated challenging queries distributed across 10 broad thematicdomains, each accompanied by manually constructed reference bundles to supportcomposite evaluation. The framework enables comprehensive evaluation oflong-form reports generated by DRAs, incorporating integrated scoring metricsfor semantic quality, topical focus, and retrieval trustworthiness. Extensiveexperimentation confirms the superior performance of mainstream DRAs overweb-search-tool-augmented reasoning models, yet reveals considerable scope forfurther improvement. This study provides a robust foundation for capabilityassessment, architectural refinement, and paradigm advancement in DRA systems.
摘要: 人工智能正在经历从封闭语言模型到具备外部感知和信息整合能力的互联代理系统的范式转变。作为典型代表，深度研究代理（DRA）系统性地展现出任务分解、跨源检索、多阶段推理和结构化输出等能力，显著提升了其在复杂开放式任务上的表现。然而，现有基准在评估维度、响应格式和评分机制方面仍存在不足，限制了其有效评估此类系统的能力。本文介绍了一个专为DRA和报告式响应设计的严格基准及多维评估框架。该基准包含214个由专家精心策划的挑战性查询，涵盖10个广泛主题领域，每个查询均配有手动构建的参考包以支持综合评估。该框架能够对DRA生成的长篇报告进行全面评估，涵盖语义质量、主题聚焦度和检索可信度的综合评分指标。大量实验证实，主流DRA在性能上优于配备网络搜索工具的推理模型，但同时也揭示了其进一步改进的巨大空间。本研究为DRA系统的能力评估、架构优化和范式演进提供了坚实基础。
Link: http://arxiv.org/abs/2510.02190v1
Updated: 2025-10-02T16:40:02Z

52: ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge  Graph Exploration Utilities
Authors: ['Felix Brei', 'Lorenz Bühmann', 'Johannes Frey', 'Daniel Gerber', 'Lars-Peter Meyer', 'Claus Stadler', 'Kirill Bulert']
Summary: Interacting with knowledge graphs can be a daunting task for people without abackground in computer science since the query language that is used (SPARQL)has a high barrier of entry. Large language models (LLMs) can lower thatbarrier by providing support in the form of Text2SPARQL translation. In thispaper we introduce a generalized method based on SPINACH, an LLM backed agentthat translates natural language questions to SPARQL queries not in a singleshot, but as an iterative process of exploration and execution. We describe theoverall architecture and reasoning behind our design decisions, and alsoconduct a thorough analysis of the agent behavior to gain insights into futureareas for targeted improvements. This work was motivated by the Text2SPARQLchallenge, a challenge that was held to facilitate improvements in theText2SPARQL domain.
摘要: 对于没有计算机科学背景的人来说，与知识图谱交互可能是一项艰巨的任务，因为所使用的查询语言（SPARQL）入门门槛很高。大型语言模型（LLM）可以通过提供Text2SPARQL翻译支持来降低这一门槛。在本文中，我们介绍了一种基于SPINACH的通用方法，这是一个由大型语言模型支持的代理，它将自然语言问题转换为SPARQL查询，并非一次性完成，而是作为一个探索和执行的迭代过程。我们描述了整体架构以及设计决策背后的原因，并对代理行为进行了深入分析，以洞察未来需要针对性改进的领域。这项工作的动机源于Text2SPARQL挑战赛，该挑战赛旨在促进Text2SPARQL领域的改进。
Link: http://arxiv.org/abs/2510.02200v1
Updated: 2025-10-02T16:49:27Z

53: Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in  VLM-Powered Mobile-Use Agents
Authors: ['Lingzhong Dong', 'Ziqi Zhou', 'Shuaibo Yang', 'Haiyue Sheng', 'Pengzhou Cheng', 'Zongru Wu', 'Zheng Wu', 'Gongshen Liu', 'Zhuosheng Zhang']
Summary: Mobile-use agents powered by vision-language models (VLMs) have shown greatpotential in interpreting natural language instructions and generatingcorresponding actions based on mobile graphical user interface. Recent studiessuggest that incorporating chain-of-thought (CoT) reasoning tends to improvethe execution accuracy. However, existing evaluations emphasize executionaccuracy while neglecting whether CoT reasoning aligns with ground-truthactions. This oversight fails to assess potential reasoning-execution gaps,which in turn foster over-trust: users relying on seemingly plausible CoTs mayunknowingly authorize harmful actions, potentially resulting in financial lossor trust crisis. In this work, we introduce a new evaluation framework todiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment(GTA), which measures whether the action implied by a CoT matches theground-truth action. By combining GTA with the standard Exact Match (EM)metric, we jointly assess both the reasoning accuracy and execution accuracy.This joint perspective reveals two types of reasoning-execution gaps: (i)Execution Gap (EG), where the reasoning correctly identifies the correct actionbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds butreasoning process conflicts with the actual execution. Experimental resultsacross a wide range of mobile interaction tasks reveal that reasoning-executiongaps are prevalent, with execution gaps occurring more frequently thanreasoning gaps. Moreover, while scaling up model size reduces the overall gap,sizable execution gaps persist even in the largest models. Further analysisshows that our framework reliably reflects systematic EG/RG patterns instate-of-the-art models. These findings offer concrete diagnostics and supportthe development of more trustworthy mobile-use agents.
摘要: 由视觉语言模型（VLM）驱动的移动端智能体在解读自然语言指令并基于移动图形用户界面生成相应操作方面展现出巨大潜力。近期研究表明，引入思维链（CoT）推理有助于提升执行准确率。然而，现有评估方法侧重于执行准确率，却忽视了CoT推理是否与真实操作一致。这一疏漏导致无法评估潜在的推理-执行差异，进而引发过度信任问题：用户可能因看似合理的CoT而无意中授权有害操作，从而造成经济损失或信任危机。本研究提出了一种新的评估框架以诊断推理-执行差异。其核心在于真实对齐度（GTA），用于衡量CoT隐含的操作是否与真实操作匹配。通过将GTA与标准精确匹配（EM）指标结合，我们联合评估推理准确率和执行准确率。这种联合视角揭示了两种推理-执行差异类型：（i）执行差异（EG），即推理正确识别操作但执行失败；（ii）推理差异（RG），即执行成功但推理过程与实际执行冲突。在广泛移动交互任务中的实验结果表明，推理-执行差异普遍存在，且执行差异的发生频率高于推理差异。此外，尽管扩大模型规模可缩小整体差异，但即使在最大模型中仍存在显著执行差异。进一步分析显示，该框架能可靠反映前沿模型的系统性EG/RG模式。这些发现为开发更可信的移动端智能体提供了具体诊断依据和技术支持。
Link: http://arxiv.org/abs/2510.02204v1
Updated: 2025-10-02T16:51:19Z

54: StockBench: Can LLM Agents Trade Stocks Profitably In Real-world  Markets?
Authors: ['Yanxu Chen', 'Zijun Yao', 'Yantao Liu', 'Jin Ye', 'Jianing Yu', 'Lei Hou', 'Juanzi Li']
Summary: Large language models (LLMs) have recently demonstrated strong capabilitiesas autonomous agents, showing promise in reasoning, tool use, and sequentialdecision-making. While prior benchmarks have evaluated LLM agents in domainssuch as software engineering and scientific discovery, the finance domainremains underexplored, despite its direct relevance to economic value andhigh-stakes decision-making. Existing financial benchmarks primarily teststatic knowledge through question answering, but they fall short of capturingthe dynamic and iterative nature of trading. To address this gap, we introduceStockBench, a contamination-free benchmark designed to evaluate LLM agents inrealistic, multi-month stock trading environments. Agents receive daily marketsignals -- including prices, fundamentals, and news -- and must make sequentialbuy, sell, or hold decisions. Performance is assessed using financial metricssuch as cumulative return, maximum drawdown, and the Sortino ratio. Ourevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) andopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLMagents struggle to outperform the simple buy-and-hold baseline, several modelsdemonstrate the potential to deliver higher returns and manage risk moreeffectively. These findings highlight both the challenges and opportunities indeveloping LLM-powered financial agents, showing that excelling at staticfinancial knowledge tasks does not necessarily translate into successfultrading strategies. We release StockBench as an open-source resource to supportreproducibility and advance future research in this domain.
摘要: 大型语言模型（LLMs）最近已展现出作为自主代理的强大能力，在推理、工具使用和顺序决策方面表现出巨大潜力。尽管先前的基准测试已在软件工程和科学发现等领域对LLM代理进行了评估，但金融领域尽管与经济价值和高风险决策直接相关，却仍未得到充分探索。现有的金融基准主要通过问答测试静态知识，但它们未能捕捉交易活动的动态性和迭代性。为填补这一空白，我们推出了StockBench，这是一个无污染的基准测试，旨在评估LLM代理在现实、多月的股票交易环境中的表现。代理每日接收市场信号——包括价格、基本面数据和新闻——并必须做出顺序性的买入、卖出或持有决策。绩效通过累计回报率、最大回撤率和索提诺比率等金融指标进行评估。我们对最先进的专有模型（如GPT-5、Claude-4）和开放权重模型（如Qwen3、Kimi-K2、GLM-4.5）的评估显示，尽管大多数LLM代理难以超越简单的买入并持有基准，但有几个模型展现出提供更高回报和更有效管理风险的潜力。这些发现凸显了开发LLM驱动的金融代理所面临的挑战和机遇，表明在静态金融知识任务上的出色表现并不一定转化为成功的交易策略。我们将StockBench作为开源资源发布，以支持可复现性并推动该领域的未来研究。
Link: http://arxiv.org/abs/2510.02209v1
Updated: 2025-10-02T16:54:57Z

55: More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for  Diverse Exploration
Authors: ['Xiaoyang Yuan', 'Yujuan Ding', 'Yi Bin', 'Wenqi Shao', 'Jinyu Cai', 'Jingkuan Song', 'Yang Yang', 'Hengtao Shen']
Summary: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigmfor enhancing the reasoning ability in Large Language Models (LLMs). However,prevailing methods primarily rely on self-exploration or a single off-policyteacher to elicit long chain-of-thought (LongCoT) reasoning, which mayintroduce intrinsic model biases and restrict exploration, ultimately limitingreasoning diversity and performance. Drawing inspiration from multi-teacherstrategies in knowledge distillation, we introduce Adaptive Multi-GuidancePolicy Optimization (AMPO), a novel framework that adaptively leveragesguidance from multiple proficient teacher models, but only when the on-policymodel fails to generate correct solutions. This "guidance-on-demand" approachexpands exploration while preserving the value of self-discovery. Moreover,AMPO incorporates a comprehension-based selection mechanism, prompting thestudent to learn from the reasoning paths that it is most likely to comprehend,thus balancing broad exploration with effective exploitation. Extensiveexperiments show AMPO substantially outperforms a strong baseline (GRPO), witha 4.3% improvement on mathematical reasoning tasks and 12.2% onout-of-distribution tasks, while significantly boosting Pass@k performance andenabling more diverse exploration. Notably, using four peer-sized teachers, ourmethod achieves comparable results to approaches that leverage a single, morepowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstratea more efficient and scalable path to superior reasoning and generalizability.Our code is available at https://github.com/SII-Enigma/AMPO.
摘要: 可验证奖励强化学习（RLVR）是一种增强大型语言模型（LLMs）推理能力的前沿范式。然而，现有方法主要依赖于自我探索或单一离策略教师来生成长链式思维（LongCoT）推理，这可能会引入固有的模型偏差并限制探索，最终限制推理的多样性和性能。受知识蒸馏中多教师策略的启发，我们提出了自适应多引导策略优化（AMPO），这是一个新颖的框架，它仅在在线策略模型无法生成正确解决方案时，才自适应地利用多个熟练教师模型的指导。这种“按需引导”的方法在扩展探索的同时保留了自我发现的价值。此外，AMPO还融合了一种基于理解的选择机制，促使学生模型从其最有可能理解的推理路径中学习，从而在广泛探索和有效利用之间取得平衡。大量实验表明，AMPO显著优于强大的基线模型（GRPO），在数学推理任务上提升了4.3%，在分布外任务上提升了12.2%，同时显著提高了Pass@k性能并实现了更多样化的探索。值得注意的是，使用四个规模相当的教师模型，我们的方法取得了与利用单一更强大教师模型（例如DeepSeek-R1）和更多数据的方法相当的结果。这些结果证明了一条通往卓越推理和泛化能力的更高效、可扩展的路径。我们的代码可在https://github.com/SII-Enigma/AMPO获取。
Link: http://arxiv.org/abs/2510.02227v1
Updated: 2025-10-02T17:14:00Z

56: The Reasoning Boundary Paradox: How Reinforcement Learning Constrains  Language Models
Authors: ['Phuc Minh Nguyen', 'Chinh D. La', 'Duy M. H. Nguyen', 'Nitesh V. Chawla', 'Binh T. Nguyen', 'Khoa D. Doan']
Summary: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a keymethod for improving Large Language Models' reasoning capabilities, yet recentevidence suggests it may paradoxically shrink the reasoning boundary ratherthan expand it. This paper investigates the shrinkage issue of RLVR byanalyzing its learning dynamics and reveals two critical phenomena that explainthis failure. First, we expose negative interference in RLVR, where learning tosolve certain training problems actively reduces the likelihood of correctsolutions for others, leading to the decline of Pass@$k$ performance, or theprobability of generating a correct solution within $k$ attempts. Second, weuncover the winner-take-all phenomenon: RLVR disproportionately reinforcesproblems with high likelihood, correct solutions, under the base model, whilesuppressing other initially low-likelihood ones. Through extensive theoreticaland empirical analysis on multiple mathematical reasoning benchmarks, we showthat this effect arises from the inherent on-policy sampling in standard RLobjectives, causing the model to converge toward narrow solution strategies.Based on these insights, we propose a simple yet effective data curationalgorithm that focuses RLVR learning on low-likelihood problems, achievingnotable improvement in Pass@$k$ performance. Our code is available athttps://github.com/mail-research/SELF-llm-interference.
摘要: 可验证奖励强化学习（RLVR）已成为提升大语言模型推理能力的关键方法，但近期证据表明，它可能反而会缩小而非扩展推理边界。本文通过分析RLVR的学习动态来研究其缩小问题，并揭示了两个解释这一失败的关键现象。首先，我们揭示了RLVR中的负干扰现象，即学习解决某些训练问题会主动降低其他问题的正确解决概率，导致Pass@$k$性能（即在$k$次尝试中生成正确解的概率）下降。其次，我们发现了赢家通吃现象：RLVR会不成比例地强化基础模型下高概率正确解的问题，同时抑制其他初始低概率的问题。通过对多个数学推理基准进行广泛的理论和实证分析，我们表明这种效应源于标准强化学习目标中固有的在线策略采样，导致模型向狭窄的解决策略收敛。基于这些见解，我们提出了一种简单而有效的数据筛选算法，将RLVR学习聚焦于低概率问题，从而在Pass@$k$性能上取得了显著提升。我们的代码可在https://github.com/mail-research/SELF-llm-interference获取。
Link: http://arxiv.org/abs/2510.02230v1
Updated: 2025-10-02T17:17:27Z

57: Enhanced Arabic-language cyberbullying detection: deep embedding and  transformer (BERT) approaches
Authors: ['Ebtesam Jaber Aljohani', 'Wael M. S. Yafoo']
Summary: Recent technological advances in smartphones and communications, includingthe growth of such online platforms as massive social media networks such as X(formerly known as Twitter) endangers young people and their emotionalwell-being by exposing them to cyberbullying, taunting, and bullying content.Most proposed approaches for automatically detecting cyberbullying have beendeveloped around the English language, and methods for detectingArabic-language cyberbullying are scarce. Methods for detecting Arabic-languagecyberbullying are especially scarce. This paper aims to enhance theeffectiveness of methods for detecting cyberbullying in Arabic-languagecontent. We assembled a dataset of 10,662 X posts, pre-processed the data, andused the kappa tool to verify and enhance the quality of our annotations. Weconducted four experiments to test numerous deep learning models forautomatically detecting Arabic-language cyberbullying. We first tested a longshort-term memory (LSTM) model and a bidirectional long short-term memory(Bi-LSTM) model with several experimental word embeddings. We also tested theLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder fromrepresentations (BERT) and then tested them on a different experimental modelsBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTMwith FastText embedding word performed even better, achieving 98% accuracy. Asa result, the outcomes are generalize
摘要: 智能手机和通信领域的最新技术进步，包括诸如X（前身为Twitter）等大型社交媒体网络等在线平台的增长，通过使年轻人接触网络欺凌、嘲弄和欺凌性内容，对年轻人及其情感福祉构成威胁。大多数自动检测网络欺凌的提议方法都是围绕英语开发的，而检测阿拉伯语网络欺凌的方法则十分匮乏。检测阿拉伯语网络欺凌的方法尤其稀缺。本文旨在提高检测阿拉伯语内容中网络欺凌的方法的有效性。我们汇编了一个包含10,662条X帖子的数据集，对数据进行了预处理，并使用kappa工具来验证和增强我们注释的质量。我们进行了四项实验，以测试多种用于自动检测阿拉伯语网络欺凌的深度学习模型。我们首先测试了一个长短期记忆（LSTM）模型和一个双向长短期记忆（Bi-LSTM）模型，并使用了多种实验性词嵌入。我们还测试了结合了一种新颖的预训练双向编码器（BERT）的LSTM和Bi-LSTM模型，然后再次在另一个实验模型BERT上对它们进行了测试。LSTM-BERT和Bi-LSTM-BERT达到了97%的准确率。结合FastText词嵌入的Bi-LSTM模型表现更佳，达到了98%的准确率。因此，这些结果具有泛化性。
Link: http://dx.doi.org/10.11591/ijai.v14.i3.pp2258-2269
Updated: 2025-10-02T17:20:02Z

58: Study on LLMs for Promptagator-Style Dense Retriever Training
Authors: ['Daniel Gwon', 'Nour Jedidi', 'Jimmy Lin']
Summary: Promptagator demonstrated that Large Language Models (LLMs) with few-shotprompts can be used as task-specific query generators for fine-tuningdomain-specialized dense retrieval models. However, the original Promptagatorapproach relied on proprietary and large-scale LLMs which users may not haveaccess to or may be prohibited from using with sensitive data. In this work, westudy the impact of open-source LLMs at accessible scales ($\leq$14Bparameters) as an alternative. Our results demonstrate that open-source LLMs assmall as 3B parameters can serve as effective Promptagator-style querygenerators. We hope our work will inform practitioners with reliablealternatives for synthetic data generation and give insights to maximizefine-tuning results for domain-specific applications.
摘要: Promptagator研究表明，带有少样本提示的大型语言模型（LLM）可用作任务特定的查询生成器，用于微调领域专用的密集检索模型。然而，原始的Promptagator方法依赖于专有且大规模的LLM，用户可能无法访问这些模型，或因涉及敏感数据而被禁止使用。在本研究中，我们探讨了开源LLM在可访问规模（参数量≤140亿）下的影响，作为替代方案。我们的结果表明，参数量小至30亿的开源LLM也能作为有效的Promptagator风格查询生成器。我们希望这项工作能为从业者提供合成数据生成的可靠替代方案，并为优化领域特定应用的微调结果提供洞见。
Link: http://arxiv.org/abs/2510.02241v1
Updated: 2025-10-02T17:29:51Z

59: AccurateRAG: A Framework for Building Accurate Retrieval-Augmented  Question-Answering Applications
Authors: ['Linh The Nguyen', 'Chi Tran', 'Dung Ngoc Nguyen', 'Van-Cuong Pham', 'Hoang Ngo', 'Dat Quoc Nguyen']
Summary: We introduce AccurateRAG -- a novel framework for constructinghigh-performance question-answering applications based on retrieval-augmentedgeneration (RAG). Our framework offers a pipeline for development efficiencywith tools for raw dataset processing, fine-tuning data generation, textembedding & LLM fine-tuning, output evaluation, and building RAG systemslocally. Experimental results show that our framework outperforms previousstrong baselines and obtains new state-of-the-art question-answeringperformance on benchmark datasets.
摘要: 我们介绍 AccurateRAG——一种基于检索增强生成（RAG）构建高性能问答应用的新型框架。我们的框架提供了一个提升开发效率的流水线，包含用于原始数据集处理、微调数据生成、文本嵌入与大语言模型微调、输出评估以及本地构建 RAG 系统的工具。实验结果表明，我们的框架优于以往强大的基线模型，并在基准数据集上取得了最新的顶尖问答性能。
Link: http://arxiv.org/abs/2510.02243v1
Updated: 2025-10-02T17:30:08Z

60: ExGRPO: Learning to Reason from Experience
Authors: ['Runzhe Zhan', 'Yafu Li', 'Zhi Wang', 'Xiaoye Qu', 'Dongrui Liu', 'Jing Shao', 'Derek F. Wong', 'Yu Cheng']
Summary: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigmfor improving the reasoning ability of large language models. However, standardon-policy training discards rollout experiences after a single update, leadingto computational inefficiency and instability. While prior work on RL hashighlighted the benefits of reusing past experience, the role of experiencecharacteristics in shaping learning dynamics of large reasoning models remainsunderexplored. In this paper, we are the first to investigate what makes areasoning experience valuable and identify rollout correctness and entropy aseffective indicators of experience value. Based on these insights, we proposeExGRPO (Experiential Group Relative Policy Optimization), a framework thatorganizes and prioritizes valuable experiences, and employs a mixed-policyobjective to balance exploration with experience exploitation. Experiments onfive backbone models (1.5B-8B parameters) show that ExGRPO consistentlyimproves reasoning performance on mathematical/general benchmarks, with anaverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPOstabilizes training on both stronger and weaker models where on-policy methodsfail. These results highlight principled experience management as a keyingredient for efficient and scalable RLVR.
摘要: 可验证奖励强化学习（RLVR）是提升大语言模型推理能力的一种新兴范式。然而，标准的在策略训练在单次更新后即丢弃推演经验，导致计算效率低下和不稳定性。尽管先前关于强化学习的研究强调了复用过往经验的优势，但经验特征在塑造大型推理模型学习动态中的作用仍未得到充分探索。在本文中，我们首次探究了推理经验的价值所在，并确定推演正确性和熵是经验价值的有效指标。基于这些见解，我们提出了ExGRPO（经验组相对策略优化），该框架对有价值经验进行组织和优先级排序，并采用混合策略目标以平衡探索与经验利用。在五个基础模型（15亿至80亿参数）上的实验表明，ExGRPO在数学/通用基准测试上持续提升推理性能，较在策略RLVR平均提升3.5/7.6个百分点。此外，ExGRPO在更强和更弱模型上均能稳定训练，而在策略方法则无法做到。这些结果凸显了原则性的经验管理是实现高效且可扩展RLVR的关键要素。
Link: http://arxiv.org/abs/2510.02245v1
Updated: 2025-10-02T17:31:30Z

61: Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative  Entropy Regulation
Authors: ['Tianyi Jiang', 'Yi Bin', 'Yujuan Ding', 'Kainian Zhu', 'Fei Ma', 'Jingkuan Song', 'Heng Tao Shen']
Summary: Large Language Models (LLMs) have demonstrated remarkable reasoning abilitieson complex problems using long Chain-of-Thought (CoT) reasoning. However, theyoften suffer from overthinking, meaning generating unnecessarily lengthyreasoning steps for simpler problems. This issue may degrade the efficiency ofthe models and make them difficult to adapt the reasoning depth to thecomplexity of problems. To address this, we introduce a novel metric TokenEntropy Cumulative Average (TECA), which measures the extent of explorationthroughout the reasoning process. We further propose a novel reasoning paradigm-- Explore Briefly, Then Decide -- with an associated Cumulative EntropyRegulation (CER) mechanism. This paradigm leverages TECA to help the modeldynamically determine the optimal point to conclude its thought process andprovide a final answer, thus achieving efficient reasoning. Experimentalresults across diverse mathematical benchmarks show that our approachsubstantially mitigates overthinking without sacrificing problem-solvingability. With our thinking paradigm, the average response length decreases byup to 71% on simpler datasets, demonstrating the effectiveness of our method increating a more efficient and adaptive reasoning process.
摘要: 大型语言模型（LLMs）通过长链式思维（CoT）推理，在复杂问题上展现了卓越的推理能力。然而，它们常常会陷入过度思考，即针对简单问题生成不必要的冗长推理步骤。这一问题可能降低模型的效率，并使其难以根据问题的复杂性调整推理深度。为解决此问题，我们提出了一种新的度量指标——令牌熵累积平均值（TECA），用于衡量推理过程中的探索程度。我们进一步提出了一种新的推理范式——“简要探索，然后决策”，并辅以累积熵调节（CER）机制。该范式利用TECA帮助模型动态确定结束思维过程并提供最终答案的最佳时机，从而实现高效推理。在多个数学基准测试中的实验结果表明，我们的方法在显著缓解过度思考的同时，并未牺牲问题解决能力。采用我们的思维范式后，在较简单数据集上的平均响应长度减少了高达71%，证明了我们的方法在创建更高效、更具适应性的推理过程方面的有效性。
Link: http://arxiv.org/abs/2510.02249v1
Updated: 2025-10-02T17:36:50Z

62: The Unreasonable Effectiveness of Scaling Agents for Computer Use
Authors: ['Gonzalo Gonzalez-Pumariega', 'Vincent Tu', 'Chih-Lun Lee', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang']
Summary: Computer-use agents (CUAs) hold promise for automating everyday digitaltasks, but their unreliability and high variance hinder their application tolong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a methodthat scales over agents by generating multiple rollouts and selecting amongthem using behavior narratives that describe the agents' rollouts. It enablesboth wide exploration and principled trajectory selection, substantiallyimproving robustness and success rates. On OSWorld, our bBoN scaling methodestablishes a new state of the art (SoTA) at 69.9%, significantly outperformingprior methods and approaching human-level performance at 72%, withcomprehensive ablations validating key design choices. We further demonstratestrong generalization results to different operating systems onWindowsAgentArena and AndroidWorld. Crucially, our results highlight theunreasonable effectiveness of scaling CUAs, when you do it right: effectivescaling requires structured trajectory understanding and selection, and bBoNprovides a practical framework to achieve this.
摘要: 计算机使用代理（CUA）有望实现日常数字任务的自动化，但其不可靠性和高变异性阻碍了它们在长周期复杂任务中的应用。我们提出了行为最佳N次采样（bBoN）方法，该方法通过生成多个轨迹并使用描述代理轨迹的行为叙事在它们之间进行选择，从而实现代理的规模化。它既支持广泛探索，又支持原则性的轨迹选择，显著提高了鲁棒性和成功率。在OSWorld上，我们的bBoN规模化方法以69.9%的成绩创造了新的最先进水平（SoTA），显著优于先前的方法，并接近72%的人类水平表现，全面的消融实验验证了关键设计选择。我们还在WindowsAgentArena和AndroidWorld上展示了其在不同操作系统上的强大泛化能力。至关重要的是，我们的研究结果凸显了当方法得当、正确实施时，规模化CUA的惊人有效性：有效的规模化需要结构化的轨迹理解和选择，而bBoN为实现这一目标提供了一个实用框架。
Link: http://arxiv.org/abs/2510.02250v1
Updated: 2025-10-02T17:37:08Z

63: RLAD: Training LLMs to Discover Abstractions for Solving Reasoning  Problems
Authors: ['Yuxiao Qu', 'Anikait Singh', 'Yoonho Lee', 'Amrith Setlur', 'Ruslan Salakhutdinov', 'Chelsea Finn', 'Aviral Kumar']
Summary: Reasoning requires going beyond pattern matching or memorization of solutionsto identify and implement "algorithmic procedures" that can be used to deduceanswers to hard problems. Doing so requires realizing the most relevantprimitives, intermediate results, or shared procedures, and building upon them.While RL post-training on long chains of thought ultimately aims to uncoverthis kind of algorithmic behavior, most reasoning traces learned by largemodels fail to consistently capture or reuse procedures, instead drifting intoverbose and degenerate exploration. To address more effective reasoning, weintroduce reasoning abstractions: concise natural language descriptions ofprocedural and factual knowledge that guide the model toward learningsuccessful reasoning. We train models to be capable of proposing multipleabstractions given a problem, followed by RL that incentivizes building asolution while using the information provided by these abstractions. Thisresults in a two-player RL training paradigm, abbreviated as RLAD, that jointlytrains an abstraction generator and a solution generator. This setupeffectively enables structured exploration, decouples learning signals ofabstraction proposal and solution generation, and improves generalization toharder problems. We also show that allocating more test-time compute togenerating abstractions is more beneficial for performance than generating moresolutions at large test budgets, illustrating the role of abstractions inguiding meaningful exploration.
摘要: 推理需要超越模式匹配或对解决方案的简单记忆，以识别并实施可用于推导难题答案的“算法程序”。这要求我们意识到最相关的基元、中间结果或共享程序，并在此基础上进行构建。尽管在长思维链上进行强化学习的后训练最终旨在揭示这种算法行为，但大型模型学到的大多数推理轨迹无法持续地捕获或重用这些程序，反而容易陷入冗长且退化的探索之中。为解决更有效的推理问题，我们引入了推理抽象：即对程序性和事实性知识的简洁自然语言描述，用以指导模型学习成功的推理。我们训练模型使其能够针对给定问题提出多种抽象，随后通过强化学习激励模型在利用这些抽象所提供信息的基础上构建解决方案。这形成了一个双人强化学习训练范式，简称为RLAD，它联合训练一个抽象生成器和一个解决方案生成器。该设置有效地实现了结构化探索，将抽象提议与解决方案生成的学习信号解耦，并提升了对更难问题的泛化能力。我们还证明，在较大的测试预算下，将更多的测试时间计算分配给生成抽象，比生成更多的解决方案对性能提升更为有益，这说明了抽象在引导有意义探索中的作用。
Link: http://arxiv.org/abs/2510.02263v1
Updated: 2025-10-02T17:44:23Z

64: InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in  Tool-Augmented Agents
Authors: ['Yaxin Du', 'Yuanshuo Zhang', 'Xiyuan Yang', 'Yifan Zhou', 'Cheng Wang', 'Gongyi Zou', 'Xianghe Pang', 'Wenhao Wang', 'Menglan Chen', 'Shuo Tang', 'Zhiyu Li', 'Siheng Chen']
Summary: Information seeking is a fundamental requirement for humans. However,existing LLM agents rely heavily on open-web search, which exposes twofundamental weaknesses: online content is noisy and unreliable, and manyreal-world tasks require precise, domain-specific knowledge unavailable fromthe web. The emergence of the Model Context Protocol (MCP) now allows agents tointerface with thousands of specialized tools, seemingly resolving thislimitation. Yet it remains unclear whether agents can effectively leverage suchtools -- and more importantly, whether they can integrate them withgeneral-purpose search to solve complex tasks. Therefore, we introduceInfoMosaic-Bench, the first benchmark dedicated to multi-source informationseeking in tool-augmented agents. Covering six representative domains(medicine, finance, maps, video, web, and multi-domain integration),InfoMosaic-Bench requires agents to combine general-purpose search withdomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalablepipeline that grounds task conditions in verified tool outputs, enforcescross-source dependencies, and filters out shortcut cases solvable by triviallookup. This design guarantees both reliability and non-triviality. Experimentswith 14 state-of-the-art LLM agents reveal three findings: (i) web informationalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% passrate; (ii) domain tools provide selective but inconsistent benefits, improvingsome domains while degrading others; and (iii) 22.4% of failures arise fromincorrect tool usage or selection, highlighting that current LLMs stillstruggle with even basic tool handling.
摘要: 信息获取是人类的基本需求。然而，现有的大型语言模型代理严重依赖开放网络搜索，这暴露了两个根本性弱点：网络内容嘈杂且不可靠，并且许多现实世界的任务需要网络无法提供的精确、特定领域的知识。模型上下文协议（MCP）的出现现在允许代理与数千种专用工具进行交互，似乎解决了这一限制。但目前尚不清楚代理能否有效利用这些工具——更重要的是，它们能否将这些工具与通用搜索相结合以解决复杂任务。因此，我们引入了InfoMosaic-Bench，这是第一个专门用于工具增强型代理多源信息获取的基准测试。InfoMosaic-Bench涵盖六个代表性领域（医学、金融、地图、视频、网络和多领域集成），要求代理将通用搜索与特定领域工具相结合。任务通过InfoMosaic-Flow合成，这是一个可扩展的流程，它将任务条件基于经过验证的工具输出，强制执行跨源依赖关系，并过滤掉可通过简单查找解决的捷径案例。这种设计确保了可靠性和非平凡性。对14种最先进的大型语言模型代理的实验揭示了三个发现：（i）仅靠网络信息是不够的，GPT-5仅达到38.2%的准确率和67.5%的通过率；（ii）领域工具提供选择性但不一致的益处，改进了某些领域，同时降低了其他领域的性能；（iii）22.4%的失败源于工具使用或选择不当，这凸显了当前大型语言模型即使在基本工具处理方面仍然存在困难。
Link: http://arxiv.org/abs/2510.02271v1
Updated: 2025-10-02T17:48:03Z

65: Parallel Scaling Law: Unveiling Reasoning Generalization through A  Cross-Linguistic Perspective
Authors: ['Wen Yang', 'Junhong Wu', 'Chong Li', 'Chengqing Zong', 'Jiajun Zhang']
Summary: Recent advancements in Reinforcement Post-Training (RPT) have significantlyenhanced the capabilities of Large Reasoning Models (LRMs), sparking increasedinterest in the generalization of RL-based reasoning. While existing work hasprimarily focused on investigating its generalization across tasks ormodalities, this study proposes a novel cross-linguistic perspective toinvestigate reasoning generalization. This raises a crucial question:$\textit{Does the reasoning capability achieved from English RPT effectivelytransfer to other languages?}$ We address this by systematically evaluatingEnglish-centric LRMs on multilingual reasoning benchmarks and introducing ametric to quantify cross-lingual transferability. Our findings reveal thatcross-lingual transferability varies significantly across initial model, targetlanguage, and training paradigm. Through interventional studies, we find thatmodels with stronger initial English capabilities tend to over-rely onEnglish-specific patterns, leading to diminished cross-lingual generalization.To address this, we conduct a thorough parallel training study. Experimentalresults yield three key findings: $\textbf{First-Parallel Leap}$, a substantialleap in performance when transitioning from monolingual to just a singleparallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealingthat cross-lingual reasoning transfer follows a power-law with the number oftraining parallel languages. Moreover, we identify the discrepancy betweenactual monolingual performance and the power-law prediction as$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMsfail to fully generalize across languages. Our study challenges the assumptionthat LRM reasoning mirrors human cognition, providing critical insights for thedevelopment of more language-agnostic LRMs.
摘要: 强化后训练（RPT）的最新进展显著提升了大型推理模型（LRM）的能力，从而激发了对基于强化学习的推理泛化问题的日益关注。尽管现有工作主要集中于研究其在不同任务或模态间的泛化能力，但本研究提出了一种新颖的跨语言视角来探究推理的泛化性。这引出了一个关键问题：通过英语强化后训练获得的推理能力能否有效迁移至其他语言？我们通过在多语言推理基准上系统性评估以英语为中心的LRM，并引入一种量化跨语言迁移能力的指标来解答这一问题。研究结果表明，跨语言迁移能力因初始模型、目标语言和训练范式的不同而存在显著差异。通过干预性研究，我们发现初始英语能力更强的模型往往过度依赖英语特有的模式，从而导致跨语言泛化能力下降。为解决这一问题，我们开展了一项全面的平行训练研究。实验结果得出三个关键发现：首先是“并行飞跃”，即从单语训练转向仅添加一种平行语言时性能的显著提升；其次是可预测的“并行扩展定律”，表明跨语言推理迁移遵循与训练语言数量相关的幂律关系。此外，我们将实际单语性能与幂律预测之间的差异定义为“单语泛化差距”，这表明以英语为中心的LRM未能完全实现跨语言泛化。本研究挑战了LRM推理与人类认知相似的假设，为开发更具语言普适性的LRM提供了关键见解。
Link: http://arxiv.org/abs/2510.02272v1
Updated: 2025-10-02T17:49:49Z

66: Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming  Attacks
Authors: ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth']
Summary: Despite recent rapid progress in AI safety, current large language modelsremain vulnerable to adversarial attacks in multi-turn interaction settings,where attackers strategically adapt their prompts across conversation turns andpose a more critical yet realistic challenge. Existing approaches that discoversafety vulnerabilities either rely on manual red-teaming with human experts oremploy automated methods using pre-defined templates and human-curated attackdata, with most focusing on single-turn attacks. However, these methods did notexplore the vast space of possible multi-turn attacks, failing to considernovel attack trajectories that emerge from complex dialogue dynamics andstrategic conversation planning. This gap is particularly critical given recentfindings that LLMs exhibit significantly higher vulnerability to multi-turnattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policyreinforcement learning framework integrated with tree search that autonomouslydiscovers diverse multi-turn attack strategies by treating the dialogue as asequential decision-making problem, enabling systematic exploration withoutmanually curated data. Through extensive experiments, our approach not onlyachieves more than 25.9% higher ASR across 10 target models compared toprevious state-of-the-art approaches, but also effectively uncovers new attackstrategies by learning optimal dialogue policies that maximize attack successacross multiple turns.
摘要: 尽管人工智能安全领域近期取得了快速进展，但当前的大型语言模型在多轮交互环境中仍然容易受到对抗性攻击。在这种环境中，攻击者会策略性地在对话轮次中调整其提示，从而构成一个更为关键且现实的挑战。现有发现安全漏洞的方法要么依赖于人类专家进行手动红队测试，要么采用基于预定义模板和人工筛选攻击数据的自动化方法，且大多数方法仅关注单轮攻击。然而，这些方法未能探索广阔的多轮攻击空间，忽略了由复杂对话动态和策略性对话规划所产生的新型攻击路径。鉴于近期研究发现大型语言模型对多轮攻击的脆弱性显著高于单轮攻击，这一差距尤为关键。我们提出了DialTree-RPO，这是一种集成树搜索的在线策略强化学习框架，通过将对话视为序列决策问题，自主发现多样化的多轮攻击策略，从而无需人工筛选数据即可实现系统性探索。通过大量实验，我们的方法不仅在10个目标模型上实现了比以往最先进方法高25.9%以上的攻击成功率，还通过学习最大化多轮攻击成功率的最佳对话策略，有效揭示了新的攻击策略。
Link: http://arxiv.org/abs/2510.02286v1
Updated: 2025-10-02T17:57:05Z

67: From Behavioral Performance to Internal Competence: Interpreting  Vision-Language Models with VLM-Lens
Authors: ['Hala Sheta', 'Eric Huang', 'Shuyu Wu', 'Ilia Alenabi', 'Jiajun Hong', 'Ryker Lin', 'Ruoxi Ning', 'Daniel Wei', 'Jialin Yang', 'Jiawei Zhou', 'Ziqiao Ma', 'Freda Shi']
Summary: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,analysis, and interpretation of vision-language models (VLMs) by supporting theextraction of intermediate outputs from any layer during the forward pass ofopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface thatabstracts away model-specific complexities and supports user-friendly operationacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs andtheir over 30 variants, and is extensible to accommodate new models withoutchanging the core logic.  The toolkit integrates easily with various interpretability and analysismethods. We demonstrate its usage with two simple analytical experiments,revealing systematic differences in the hidden representations of VLMs acrosslayers and target concepts. VLM-Lens is released as an open-sourced project toaccelerate community efforts in understanding and improving VLMs.
摘要: 我们推出了VLM-Lens，这是一个旨在实现对视觉语言模型（VLM）进行系统性基准测试、分析和解释的工具包，它支持在开源VLMs的前向传播过程中从任何层提取中间输出。VLM-Lens提供了一个统一的、可通过YAML配置的接口，该接口抽象了模型特定的复杂性，并支持在多种VLMs之间进行用户友好的操作。它目前支持16种最先进的基准VLMs及其30多种变体，并且具有可扩展性，能够在不改变核心逻辑的情况下容纳新模型。该工具包可以轻松与各种可解释性和分析方法集成。我们通过两个简单的分析实验演示了其用法，揭示了VLMs在不同层和目标概念上的隐藏表征中存在的系统性差异。VLM-Lens作为一个开源项目发布，以加速社区在理解和改进VLMs方面的努力。
Link: http://arxiv.org/abs/2510.02292v1
Updated: 2025-10-02T17:58:41Z

68: F2LLM Technical Report: Matching SOTA Embedding Performance with 6  Million Open-Source Data
Authors: ['Ziyin Zhang', 'Zihan Liao', 'Hang Yu', 'Peng Di', 'Rui Wang']
Summary: We introduce F2LLM - Foundation to Feature Large Language Models, a suite ofstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlikeprevious top-ranking embedding models that require massive contrastivepretraining, sophisticated training pipelines, and costly synthetic trainingdata, F2LLM is directly finetuned from foundation models on 6 millionquery-document-negative tuples curated from open-source, non-syntheticdatasets, striking a strong balance between training cost, model size, andembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2ndamong models with approximately 4B parameters and 7th overall, while F2LLM-1.7Branks 1st among models in the 1B-2B size range. To facilitate future researchin the field, we release the models, training dataset, and code, positioningF2LLM as a strong, reproducible, and budget-friendly baseline for future works.
摘要: 我们推出F2LLM——从基础到特征的大型语言模型，这是一套先进的嵌入模型，包含三种尺寸：0.6B、1.7B和4B。与以往需要大规模对比预训练、复杂训练流程和高成本合成训练数据的顶级嵌入模型不同，F2LLM直接在从开源、非合成数据集中精心挑选的600万查询-文档-负样本元组上对基础模型进行微调，在训练成本、模型大小和嵌入性能之间实现了出色的平衡。在MTEB英文排行榜上，F2LLM-4B在参数量约4B的模型中排名第二，总体排名第七；而F2LLM-1.7B在1B-2B尺寸范围内的模型中位居第一。为促进该领域的未来研究，我们发布了模型、训练数据集和代码，将F2LLM定位为未来研究工作中一个强大、可复现且经济实惠的基线模型。
Link: http://arxiv.org/abs/2510.02294v1
Updated: 2025-10-02T17:58:49Z

69: Interactive Training: Feedback-Driven Neural Network Optimization
Authors: ['Wentao Zhang', 'Yang Young Lu', 'Yuntian Deng']
Summary: Traditional neural network training typically follows fixed, predefinedoptimization recipes, lacking the flexibility to dynamically respond toinstabilities or emerging training issues. In this paper, we introduceInteractive Training, an open-source framework that enables real-time,feedback-driven intervention during neural network training by human experts orautomated AI agents. At its core, Interactive Training uses a control server tomediate communication between users or agents and the ongoing training process,allowing users to dynamically adjust optimizer hyperparameters, training data,and model checkpoints. Through three case studies, we demonstrate thatInteractive Training achieves superior training stability, reduced sensitivityto initial hyperparameters, and improved adaptability to evolving user needs,paving the way toward a future training paradigm where AI agents autonomouslymonitor training logs, proactively resolve instabilities, and optimize trainingdynamics.
摘要: 传统的神经网络训练通常遵循固定的、预定义的优化方案，缺乏动态响应不稳定性或新出现的训练问题的灵活性。在本文中，我们介绍了交互式训练，这是一个开源框架，使人类专家或自动化AI代理能够在神经网络训练过程中进行实时的、反馈驱动的干预。其核心在于，交互式训练使用一个控制服务器来调解用户或代理与正在进行的训练过程之间的通信，允许用户动态调整优化器超参数、训练数据和模型检查点。通过三个案例研究，我们证明了交互式训练实现了卓越的训练稳定性、降低了对初始超参数的敏感性，并提高了对不断变化的用户需求的适应性，为未来的训练范式铺平了道路，在这种范式下，AI代理将自主监控训练日志，主动解决不稳定性问题，并优化训练动态。
Link: http://arxiv.org/abs/2510.02297v1
Updated: 2025-10-02T17:59:00Z

70: Drawing Conclusions from Draws: Rethinking Preference Semantics in  Arena-Style LLM Evaluation
Authors: ['Raphael Tang', 'Crystina Zhang', 'Wenyan Li', 'Carmen Lai', 'Pontus Stenetorp', 'Yao Lu']
Summary: In arena-style evaluation of large language models (LLMs), two LLMs respondto a user query, and the user chooses the winning response or deems the"battle" a draw, resulting in an adjustment to the ratings of both models. Theprevailing approach for modeling these rating dynamics is to view battles astwo-player game matches, as in chess, and apply the Elo rating system and itsderivatives. In this paper, we critically examine this paradigm. Specifically,we question whether a draw genuinely means that the two models are equal andhence whether their ratings should be equalized. Instead, we conjecture thatdraws are more indicative of query difficulty: if the query is too easy, thenboth models are more likely to succeed equally. On three real-world arenadatasets, we show that ignoring rating updates for draws yields a 1-3% relativeincrease in battle outcome prediction accuracy (which includes draws) for allfour rating systems studied. Further analyses suggest that draws occur more forqueries rated as very easy and those as highly objective, with risk ratios of1.37 and 1.35, respectively. We recommend future rating systems to reconsiderexisting draw semantics and to account for query properties in rating updates.
摘要: 在大语言模型（LLM）的竞技场式评估中，两个大语言模型会对用户查询做出响应，用户选择获胜的响应或判定“对战”为平局，从而调整两个模型的评分。当前用于建模这些评分动态的主流方法是将对战视为双人游戏比赛，如同国际象棋，并应用Elo评分系统及其衍生系统。在本文中，我们批判性地审视了这一范式。具体而言，我们质疑平局是否真正意味着两个模型实力相当，进而其评分是否应被拉平。相反，我们推测平局更多反映了查询的难度：若查询过于简单，则两个模型均等成功的可能性更大。在三个真实世界的竞技场数据集上，我们证明，对于所研究的四种评分系统，忽略平局的评分更新可使对战结果预测准确率（包含平局）相对提升1-3%。进一步分析表明，平局在被评为非常简单和高度客观的查询中更为常见，其风险比分别为1.37和1.35。我们建议未来的评分系统重新审视现有的平局语义，并在评分更新中考虑查询的属性。
Link: http://arxiv.org/abs/2510.02306v1
Updated: 2025-10-02T17:59:41Z


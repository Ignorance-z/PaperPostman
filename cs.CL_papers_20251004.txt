1: Information Seeking for Robust Decision Making under Partial  Observability
Authors: ['Djengo Cyun-Jyun Fang', 'Tsung-Wei Ke']
Summary: Explicit information seeking is essential to human problem-solving inpractical environments characterized by incomplete information and noisydynamics. When the true environmental state is not directly observable, humansseek information to update their internal dynamics and inform futuredecision-making. Although existing Large Language Model (LLM) planning agentshave addressed observational uncertainty, they often overlook discrepanciesbetween their internal dynamics and the actual environment. We introduceInformation Seeking Decision Planner (InfoSeeker), an LLM decision-makingframework that integrates task-oriented planning with information seeking toalign internal dynamics and make optimal decisions under uncertainty in bothagent observations and environmental dynamics. InfoSeeker prompts an LLM toactively gather information by planning actions to validate its understanding,detect environmental changes, or test hypotheses before generating or revisingtask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmarksuite featuring partially observable environments with incomplete observationsand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%absolute performance gain over prior methods without sacrificing sampleefficiency. Moreover, InfoSeeker generalizes across LLMs and outperformsbaselines on established benchmarks such as robotic manipulation and webnavigation. These findings underscore the importance of tightly integratingplanning and information seeking for robust behavior in partially observableenvironments. The project page is available at https://infoseekerllm.github.io
Link: http://arxiv.org/abs/2510.01531v1
Updated: 2025-10-02T00:06:32Z

2: InvThink: Towards AI Safety via Inverse Reasoning
Authors: ['Yubin Kim', 'Taehan Kim', 'Eugene Park', 'Chunjong Park', 'Cynthia Breazeal', 'Daniel McDuff', 'Hae Won Park']
Summary: We present InvThink, a simple yet powerful approach that gives large languagemodels (LLMs) the capability of inverse thinking: reasoning through failuremodes before generating responses. Unlike existing safety alignment methodsthat optimize directly for safe response, InvThink instructs models to 1)enumerate potential harms, 2) analyze their consequences, and 3) generate safeoutputs that proactively avoid these risks. Our method reveals three keyfindings: (i) safety improvements show stronger scaling with model sizecompared to existing safety methods. (ii) InvThink mitigates safety tax; bytraining models to systematically consider failure modes, it preserves generalreasoning capabilities on standard benchmarks. (iii) beyond general safetytasks, InvThink excels in high-stakes domains including external-facing(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,achieving up to 15.7% reduction in harmful responses compared to baselinemethods like SafetyPrompt. We further implement InvThink via supervisedfine-tuning, and reinforcement learning across three LLM families. Theseresults suggest that inverse reasoning provides a scalable and generalizablepath toward safer, more capable language models.
Link: http://arxiv.org/abs/2510.01569v1
Updated: 2025-10-02T01:26:53Z

3: Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query  Autocomplete
Authors: ['Adithya Rajan', 'Xiaoyu Liu', 'Prateek Verma', 'Vibhu Arora']
Summary: We introduce a data-centric approach for mitigating presentation bias inreal-time neural query autocomplete systems through the use of syntheticprefixes. These prefixes are generated from complete user queries collectedduring regular search sessions where autocomplete was not active. This allowsus to enrich the training data for learning to rank models with more diverseand less biased examples. This method addresses the inherent bias in engagementsignals collected from live query autocomplete interactions, where modelsuggestions influence user behavior. Our neural ranker is optimized forreal-time deployment under strict latency constraints and incorporates a richset of features, including query popularity, seasonality, fuzzy match scores,and contextual signals such as department affinity, device type, and verticalalignment with previous user queries. To support efficient training, weintroduce a task-specific simplification of the listwise loss, reducingcomputational complexity from $O(n^2)$ to $O(n)$ by leveraging the queryautocomplete structure of having only one ground-truth selection per prefix.Deployed in a large-scale e-commerce setting, our system demonstratesstatistically significant improvements in user engagement, as measured by meanreciprocal rank and related metrics. Our findings show that synthetic prefixesnot only improve generalization but also provide a scalable path toward biasmitigation in other low-latency ranking tasks, including related searches andquery recommendations.
Link: http://arxiv.org/abs/2510.01574v1
Updated: 2025-10-02T01:44:44Z

4: Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,  Attentive Compression
Authors: ['Joykirat Singh', 'Justin Chih-Yao Chen', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Akshay Nambi', 'Mohit Bansal']
Summary: Recent thinking models solve complex reasoning tasks by scaling test-timecompute, but this scaling must be allocated in line with task difficulty. Onone hand, short reasoning (underthinking) leads to errors on harder problemsthat require extended reasoning steps; but, excessively long reasoning(overthinking) can be token-inefficient, generating unnecessary steps evenafter reaching a correct intermediate solution. We refer to this asunder-adaptivity, where the model fails to modulate its response lengthappropriately given problems of varying difficulty. To address under-adaptivityand strike a balance between under- and overthinking, we propose TRAAC (ThinkRight with Adaptive, Attentive Compression), an online post-training RL methodthat leverages the model's self-attention over a long reasoning trajectory toidentify important steps and prune redundant ones. TRAAC also estimatesdifficulty and incorporates it into training rewards, thereby learning toallocate reasoning budget commensurate with example difficulty. Our approachimproves accuracy, reduces reasoning steps, and enables adaptive thinkingcompared to base models and other RL baselines. Across a variety of tasks(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absoluteaccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%compared to the base model, and a 7.9% accuracy gain paired with a 29.4% lengthdrop compared to the best RL baseline. TRAAC also shows strong generalization:although our models are trained on math datasets, they show accuracy andefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,and OptimalThinkingBench. Our analysis further verifies that TRAAC providesfine-grained adjustments to thinking budget based on difficulty and that acombination of task-difficulty calibration and attention-based compressionyields gains across diverse tasks.
Link: http://arxiv.org/abs/2510.01581v1
Updated: 2025-10-02T02:00:20Z

5: ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and  Long-Context Reasoning
Authors: ['Haochen You', 'Baojing Liu']
Summary: While Transformer architectures have demonstrated impressive scalabilityacross domains, they continue to face challenges in long-context reasoning,computational efficiency, and structural generalization - largely due to rigidlayer stacking, dense attention, and reliance on positional encodings. Wepresent ReSSFormer, a Recursive Sparse Structured Transformer that integratesthree complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) foriterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)for efficient and focused context selection, and Self-Organizing EncoderStructure (SOES) for position-free structure induction. ReSSFormer replacesconventional depth stacking with recurrent inference, substitutes fullattention with token- and expert-level sparsity, and models latent tokentopology directly from content. Across language modeling, multi-hop QA, andstructure-sensitive tasks, ReSSFormer consistently outperforms strong baselinesunder comparable FLOPs and parameter budgets, highlighting its scalability,efficiency, and structural flexibility.
Link: http://arxiv.org/abs/2510.01585v1
Updated: 2025-10-02T02:05:30Z

6: CLUE: Non-parametric Verification from Experience via Hidden-State  Clustering
Authors: ['Zhenwen Liang', 'Ruosen Li', 'Yujun Zhou', 'Linfeng Song', 'Dian Yu', 'Xinya Du', 'Haitao Mi', 'Dong Yu']
Summary: Assessing the quality of Large Language Model (LLM) outputs presents acritical challenge. Previous methods either rely on text-level information(e.g., reward models, majority voting), which can overfit to superficial cues,or on calibrated confidence from token probabilities, which would fail onless-calibrated models. Yet both of these signals are, in fact, partialprojections of a richer source of information: the model's internal hiddenstates. Early layers, closer to token embeddings, preserve semantic and lexicalfeatures that underpin text-based judgments, while later layers increasinglyalign with output logits, embedding confidence-related information. This paperexplores hidden states directly as a unified foundation for verification. Weshow that the correctness of a solution is encoded as a geometrically separablesignature within the trajectory of hidden activations. To validate this, wepresent Clue (Clustering and Experience-based Verification), a deliberatelyminimalist, non-parametric verifier. With no trainable parameters, CLUE onlysummarizes each reasoning trace by an hidden state delta and classifiescorrectness via nearest-centroid distance to ``success'' and ``failure''clusters formed from past experience. The simplicity of this method highlightsthe strength of the underlying signal. Empirically, CLUE consistentlyoutperforms LLM-as-a-judge baselines and matches or exceeds modernconfidence-based methods in reranking candidates, improving both top-1 andmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%(top-maj@16).
Link: http://arxiv.org/abs/2510.01591v1
Updated: 2025-10-02T02:14:33Z

7: A Comparison of Independent and Joint Fine-tuning Strategies for  Retrieval-Augmented Generation
Authors: ['Neal Gregory Lawton', 'Alfy Samuel', 'Anoop Kumar', 'Daben Liu']
Summary: A Comparison of Independent and Joint Fine-tuning Strategies forRetrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate andcompare strategies for fine-tuning Retrieval Augmented Generation (RAG)pipelines, including independent fine-tuning, joint fine-tuning, and two-phasefine-tuning. Abstract: Retrieval augmented generation (RAG) is a popularframework for question answering that is powered by two large language models(LLMs): an embedding model that retrieves context documents from a databasethat are relevant to a given question, and a generator model that uses theretrieved context to generate an answer to the question. Both the embedding andgenerator models can be fine-tuned to increase performance of a RAG pipeline ona new task, but multiple fine-tuning strategies exist with different costs andbenefits. In this paper, we evaluate and compare several RAG fine-tuningstrategies, including independent, joint, and two-phase fine-tuning. In ourexperiments, we observe that all of these strategies achieve about equalimprovement in EM and F1 generation quality metrics, although they havesignificantly different computational costs. We conclude the optimalfine-tuning strategy to use depends on whether the training dataset includescontext labels and whether a grid search over the learning rates for theembedding and generator models is required.
Link: http://arxiv.org/abs/2510.01600v1
Updated: 2025-10-02T02:30:28Z

8: Bridging Collaborative Filtering and Large Language Models with Dynamic  Alignment, Multimodal Fusion and Evidence-grounded Explanations
Authors: ['Bo Ma', 'LuYao Liu', 'Simon Lau', 'Chandler Yuan', 'and XueY Cui', 'Rosie Zhang']
Summary: Recent research has explored using Large Language Models for recommendationtasks by transforming user interaction histories and item metadata into textprompts, then having the LLM produce rankings or recommendations. A promisingapproach involves connecting collaborative filtering knowledge to LLMrepresentations through compact adapter networks, which avoids expensivefine-tuning while preserving the strengths of both components. Yet severalchallenges persist in practice: collaborative filtering models often use staticsnapshots that miss rapidly changing user preferences; many real-world itemscontain rich visual and audio content beyond textual descriptions; and currentsystems struggle to provide trustworthy explanations backed by concreteevidence. Our work introduces \model{}, a framework that tackles theselimitations through three key innovations. We develop an online adaptationmechanism that continuously incorporates new user interactions throughlightweight modules, avoiding the need to retrain large models. We create aunified representation that seamlessly combines collaborative signals withvisual and audio features, handling cases where some modalities may beunavailable. Finally, we design an explanation system that groundsrecommendations in specific collaborative patterns and item attributes,producing natural language rationales users can verify. Our approach maintainsthe efficiency of frozen base models while adding minimal computationaloverhead, making it practical for real-world deployment.
Link: http://arxiv.org/abs/2510.01606v1
Updated: 2025-10-02T02:43:24Z

9: PychoBench: Evaluating the Psychology Intelligence of Large Language  Models
Authors: ['Min Zeng']
Summary: Large Language Models (LLMs) have demonstrated remarkable success across awide range of industries, primarily due to their impressive generativeabilities. Yet, their potential in applications requiring cognitive abilities,such as psychological counseling, remains largely untapped. This paperinvestigates the key question: Can LLMs be effectively applied to psychologicalcounseling? To determine whether an LLM can effectively take on the role of apsychological counselor, the first step is to assess whether it meets thequalifications required for such a role, namely the ability to pass the U.S.National Counselor Certification Exam (NCE). This is because, just as a humancounselor must pass a certification exam to practice, an LLM must demonstratesufficient psychological knowledge to meet the standards required for such arole. To address this, we introduce PsychoBench, a benchmark grounded inU.S.national counselor examinations, a licensure test for professionalcounselors that requires about 70% accuracy to pass. PsychoBench comprisesapproximately 2,252 carefully curated single-choice questions, crafted torequire deep understanding and broad enough to cover various sub-disciplines ofpsychology. This benchmark provides a comprehensive assessment of an LLM'sability to function as a counselor. Our evaluation shows that advanced modelssuch as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passingthreshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)remain far below it. These results suggest that only frontier LLMs arecurrently capable of meeting counseling exam standards, highlighting both thepromise and the challenges of developing psychology-oriented LLMs.
Link: http://arxiv.org/abs/2510.01611v1
Updated: 2025-10-02T02:49:06Z

10: RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical  Question Answering
Authors: ['Lovely Yeswanth Panchumarthi', 'Sai Prasad Gudari', 'Atharva Negi', 'Praveen Raj Budime', 'Harsit Upadhya']
Summary: The exponential growth of biomedical literature creates significantchallenges for accessing precise medical information. Current biomedicalquestion-answering systems primarily focus on short-form answers, failing toprovide the comprehensive explanations necessary for clinical decision-making.We present RAG-BioQA, a novel framework combining retrieval-augmentedgeneration with domain-specific fine-tuning to produce evidence-based,long-form biomedical answers. Our approach integrates BioBERT embeddings withFAISS indexing and compares various re-ranking strategies (BM25, ColBERT,MonoT5) to optimize context selection before synthesizing evidence through afine-tuned T5 model. Experimental results on the PubMedQA dataset showsignificant improvements over baselines, with our best model achievingsubstantial gains across BLEU, ROUGE, and METEOR metrics, advancing the stateof accessible, evidence-based biomedical knowledge retrieval.
Link: http://arxiv.org/abs/2510.01612v1
Updated: 2025-10-02T02:49:09Z

11: Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single  Consumer GPU: Continual Pre-training, SFT, and DPO
Authors: ['Yu-Cheng Chih', 'Ming-Tao Duan', 'Yong-Hao Hou']
Summary: Small Language Models (SLMs) enable cost-effective, on-device andlatency-sensitive AI applications, yet their deployment in Traditional Chinese(TC) remains hindered by token-level instability - models unpredictably emitnon-TC characters or code-switch into other languages. We address thispractical reliability gap by creating PureTC-1B, a three-stage stabilizationpipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned modelreleased by Meta) using parameter-efficient LoRA adapters. Our method combinesContinual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning(SFT) with instruction data, and Direct Preference Optimization (DPO) usingTC-adherence preferences to improve monolingual robustness without full-modelretraining. On a benchmark designed to simulate real-world usage, PureTC-1Bachieves a 51.3% relative reduction (micro-average) in non-TC output tokensversus the base model. On a Named Entity Translation (NET) task, PureTC-1Bfurther reduces incorrect-language tokens by 77.2% relative to Llama-3B and57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainableeven at the 1B scale. The pipeline is reproducible, adapter-only, andhardware-friendly, offering practitioners a practical recipe to enhancelanguage stability for TC and potentially other non-English languages.
Link: http://dx.doi.org/10.5281/zenodo.17240351
Updated: 2025-10-02T02:50:12Z

12: AMAS: Adaptively Determining Communication Topology for LLM-based  Multi-Agent System
Authors: ['Hui Yi Leong', 'Yuheng Li', 'Yuqing Wu', 'Wenwen Ouyang', 'Wei Zhu', 'Jiechao Gao']
Summary: Although large language models (LLMs) have revolutionized natural languageprocessing capabilities, their practical implementation as autonomousmulti-agent systems (MAS) for industrial problem-solving encounters persistentbarriers. Conventional MAS architectures are fundamentally restricted byinflexible, hand-crafted graph topologies that lack contextual responsiveness,resulting in diminished efficacy across varied academic and commercialworkloads. To surmount these constraints, we introduce AMAS, aparadigm-shifting framework that redefines LLM-based MAS through a noveldynamic graph designer. This component autonomously identifies task-specificoptimal graph configurations via lightweight LLM adaptation, eliminating thereliance on monolithic, universally applied structural templates. Instead, AMASexploits the intrinsic properties of individual inputs to intelligently directquery trajectories through task-optimized agent pathways. Rigorous validationacross question answering, mathematical deduction, and code generationbenchmarks confirms that AMAS systematically exceeds state-of-the-artsingle-agent and multi-agent approaches across diverse LLM architectures. Ourinvestigation establishes that context-sensitive structural adaptabilityconstitutes a foundational requirement for high-performance LLM MASdeployments.
Link: http://arxiv.org/abs/2510.01617v1
Updated: 2025-10-02T02:50:22Z

13: LLM4Rec: Large Language Models for Multimodal Generative Recommendation  with Causal Debiasing
Authors: ['Bo Ma', 'Hang Li', 'ZeHua Hu', 'XiaoFan Gui', 'LuYao Liu', 'Simon Lau']
Summary: Contemporary generative recommendation systems face significant challenges inhandling multimodal data, eliminating algorithmic biases, and providingtransparent decision-making processes. This paper introduces an enhancedgenerative recommendation framework that addresses these limitations throughfive key innovations: multimodal fusion architecture, retrieval-augmentedgeneration mechanisms, causal inference-based debiasing, explainablerecommendation generation, and real-time adaptive learning capabilities. Ourframework leverages advanced large language models as the backbone whileincorporating specialized modules for cross-modal understanding, contextualknowledge integration, bias mitigation, explanation synthesis, and continuousmodel adaptation. Extensive experiments on three benchmark datasets(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistentimprovements in recommendation accuracy, fairness, and diversity compared toexisting approaches. The proposed framework achieves up to 2.3% improvement inNDCG@10 and 1.4% enhancement in diversity metrics while maintainingcomputational efficiency through optimized inference strategies.
Link: http://arxiv.org/abs/2510.01622v1
Updated: 2025-10-02T02:53:05Z

14: Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What  to Use Instead
Authors: ['Feiyang Kang', 'Michael Kuchnik', 'Karthik Padthe', 'Marin Vlastelica', 'Ruoxi Jia', 'Carole-Jean Wu', 'Newsha Ardalani']
Summary: In post-training for reasoning Large Language Models (LLMs), the currentstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as``RL'' below). In this work, we challenge whether high SFT scores translate toimproved performance after RL. We provide extensive counter-examples where thisis not true. We find high SFT scores can be biased toward simpler or morehomogeneous data and are not reliably predictive of subsequent RL gains orscaled-up post-training effectiveness. In some cases, RL training on modelswith improved SFT performance could lead to substantially worse outcomecompared to RL on the base model without SFT. We study alternative metrics andidentify generalization loss on held-out reasoning examples and Pass@large kperformance to provide strong proxies for the RL outcome. We trained hundredsof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensiveevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPUhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiplestate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RLperformance, prediction based on generalization loss and Pass@large k achievessubstantial higher precision, improving $R^2$ coefficient and Spearman's rankcorrelation coefficient by up to 0.5 (2x). This provides strong utility forbroad use cases. For example, in most experiments, we find SFT training onunique examples for a one epoch underperforms training on half examples for twoepochs, either after SFT or SFT-then-RL; With the same SFT budget, trainingonly on short examples may lead to better SFT performance, though, it oftenleads to worse outcome after RL compared to training on examples with varyinglengths. Evaluation tool will be open-sourced.
Link: http://arxiv.org/abs/2510.01624v1
Updated: 2025-10-02T02:57:00Z

15: Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of  Scaling Laws, Benefits, and Pitfalls
Authors: ['Feiyang Kang', 'Newsha Ardalani', 'Michael Kuchnik', 'Youssef Emad', 'Mostafa Elhoushi', 'Shubhabrata Sengupta', 'Shang-Wen Li', 'Ramya Raghavendra', 'Ruoxi Jia', 'Carole-Jean Wu']
Summary: Training data plays a crucial role in Large Language Models (LLM) scaling,yet high quality data is of limited supply. Synthetic data techniques offer apotential path toward sidestepping these limitations. We conduct a large-scaleempirical investigation (>1000 LLMs with >100k GPU hours) using a unifiedprotocol and scaling laws, comparing natural web data, diverse synthetic types(rephrased text, generated textbooks), and mixtures of natural and syntheticdata. Specifically, we found pre-training on rephrased synthetic data\textit{alone} is not faster than pre-training on natural web texts; whilepre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web textscan speed up 5-10x (to reach the same validation loss) at larger data budgets.Pre-training on textbook-style synthetic data \textit{alone} results in notablyhigher loss on many downstream domains especially at small data budgets. "Good"ratios of synthetic data in training data mixtures depend on the model size anddata budget, empirically converging to ~30% for rephrased synthetic data.Larger generator models do not necessarily yield better pre-training data than~8B-param models. These results contribute mixed evidence on "model collapse"during large-scale single-round (n=1) model training on syntheticdata--training on rephrased synthetic data shows no degradation in performancein foreseeable scales whereas training on mixtures of textbook-stylepure-generated synthetic data shows patterns predicted by "model collapse". Ourwork demystifies synthetic data in pre-training, validates its conditionalbenefits, and offers practical guidance.
Link: http://arxiv.org/abs/2510.01631v1
Updated: 2025-10-02T03:24:42Z

16: NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with  BERT
Authors: ['John Hawkins', 'Aditya Pramar', 'Rodney Beard', 'Rohitash Chandra']
Summary: Large Language Models (LLMs) suffer from a range of vulnerabilities thatallow malicious users to solicit undesirable responses through manipulation ofthe input text. These so-called jailbreak prompts are designed to trick the LLMinto circumventing the safety guardrails put in place to keep responsesacceptable to the developer's policies. In this study, we analyse the abilityof different machine learning models to distinguish jailbreak prompts fromgenuine uses, including looking at our ability to identify jailbreaks that usepreviously unseen strategies. Our results indicate that using current datasetsthe best performance is achieved by fine tuning a Bidirectional EncoderRepresentations from Transformers (BERT) model end-to-end for identifyingjailbreaks. We visualise the keywords that distinguish jailbreak from genuineprompts and conclude that explicit reflexivity in prompt structure could be asignal of jailbreak intention.
Link: http://arxiv.org/abs/2510.01644v1
Updated: 2025-10-02T03:55:29Z

17: Position: Privacy Is Not Just Memorization!
Authors: ['Niloofar Mireshghallah', 'Tianshi Li']
Summary: The discourse on privacy risks in Large Language Models (LLMs) hasdisproportionately focused on verbatim memorization of training data, while aconstellation of more immediate and scalable privacy threats remainunderexplored. This position paper argues that the privacy landscape of LLMsystems extends far beyond training data extraction, encompassing risks fromdata collection practices, inference-time context leakage, autonomous agentcapabilities, and the democratization of surveillance through deep inferenceattacks. We present a comprehensive taxonomy of privacy risks across the LLMlifecycle -- from data collection through deployment -- and demonstrate throughcase studies how current privacy frameworks fail to address these multifacetedthreats. Through a longitudinal analysis of 1,322 AI/ML privacy paperspublished at leading conferences over the past decade (2016--2025), we revealthat while memorization receives outsized attention in technical research, themost pressing privacy harms lie elsewhere, where current technical approachesoffer little traction and viable paths forward remain unclear. We call for afundamental shift in how the research community approaches LLM privacy, movingbeyond the narrow focus of current technical solutions and embracinginterdisciplinary approaches that address the sociotechnical nature of theseemerging threats.
Link: http://arxiv.org/abs/2510.01645v1
Updated: 2025-10-02T04:02:06Z

18: Learning to Look at the Other Side: A Semantic Probing Study of Word  Embeddings in LLMs with Enabled Bidirectional Attention
Authors: ['Zhaoxin Feng', 'Jianfei Ma', 'Emmanuele Chersoni', 'Xiaojing Zhao', 'Xiaoyi Bao']
Summary: Autoregressive Large Language Models (LLMs) demonstrate exceptionalperformance in language understanding and generation. However, theirapplication in text embedding tasks has been relatively slow, along with theanalysis of their semantic representation in probing tasks, due to theconstraints of the unidirectional attention mechanism.  This paper aims to explore whether such constraints can be overcome byenabling bidirectional attention in LLMs. We tested different variants of theLlama architecture through additional training steps, progressively enablingbidirectional attention and unsupervised/supervised contrastive learning.
Link: http://arxiv.org/abs/2510.01652v1
Updated: 2025-10-02T04:18:13Z

19: SoK: Measuring What Matters for Closed-Loop Security Agents
Authors: ['Mudita Khurana', 'Raunak Jain']
Summary: Cybersecurity is a relentless arms race, with AI driven offensive systemsevolving faster than traditional defenses can adapt. Research and toolingremain fragmented across isolated defensive functions, creating blind spotsthat adversaries exploit. Autonomous agents capable of integrating, exploitconfirmation, remediation, and validation into a single closed loop offerpromise, but the field lacks three essentials: a framework defining the agenticcapabilities of security systems across security life cycle, a principledmethod for evaluating closed loop agents, and a benchmark for measuring theirperformance in practice. We introduce CLASP: the Closed-Loop AutonomousSecurity Performance framework which aligns the security lifecycle(reconnaissance, exploitation, root cause analysis, patch synthesis,validation) with core agentic capabilities (planning, tool use, memory,reasoning, reflection & perception) providing a common vocabulary and rubricfor assessing agentic capabilities in security tasks. By applying CLASP to 21representative works, we map where systems demonstrate strengths, and wherecapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,a composite metric quantifying both degree of loop closure and operationaleffectiveness, and outline the requirements for a closed loop benchmark.Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, andmeasurements needed to advance both function level performance and measureclosed loop security agents.
Link: http://arxiv.org/abs/2510.01654v1
Updated: 2025-10-02T04:20:35Z

20: MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue  Summarization
Authors: ['Yinhong Liu', 'Jianfeng He', 'Hang Su', 'Ruixue Lian', 'Yi Nian', 'Jake Vincent', 'Srikanth Vishnubhotla', 'Robinson Piramuthu', 'Saab Mansour']
Summary: Multimodal Dialogue Summarization (MDS) is a critical task with wide-rangingapplications. To support the development of effective MDS models, robustautomatic evaluation methods are essential for reducing both cost and humaneffort. However, such methods require a strong meta-evaluation benchmarkgrounded in human annotations. In this work, we introduce MDSEval, the firstmeta-evaluation benchmark for MDS, consisting image-sharing dialogues,corresponding summaries, and human judgments across eight well-defined qualityaspects. To ensure data quality and richfulness, we propose a novel filteringframework leveraging Mutually Exclusive Key Information (MEKI) acrossmodalities. Our work is the first to identify and formalize key evaluationdimensions specific to MDS. We benchmark state-of-the-art modal evaluationmethods, revealing their limitations in distinguishing summaries from advancedMLLMs and their susceptibility to various bias.
Link: http://arxiv.org/abs/2510.01659v1
Updated: 2025-10-02T04:38:27Z

21: Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness
Authors: ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet']
Summary: Computer-Use Agents (CUAs) are an increasingly deployed class of agents thattake actions on GUIs to accomplish user goals. In this paper, we show that CUAsconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goalsregardless of feasibility, safety, reliability, or context. We characterizethree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)assumptions and decisions under ambiguity, and (iii) contradictory orinfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing thesethree patterns. Built on OSWorld, BLIND-ACT provides realistic environments andemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreementwith human annotations. We use BLIND-ACT to evaluate nine frontier models,including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observinghigh average BGD rates (80.8%) across them. We show that BGD exposes subtlerisks that arise even when inputs are not directly harmful. Whileprompting-based interventions lower BGD levels, substantial risk persists,highlighting the need for stronger training- or inference-time interventions.Qualitative analysis reveals observed failure modes: execution-first bias(focusing on how to act over whether to act), thought-action disconnect(execution diverging from reasoning), and request-primacy (justifying actionsdue to user request). Identifying BGD and introducing BLIND-ACT establishes afoundation for future research on studying and mitigating this fundamental riskand ensuring safe CUA deployment.
Link: http://arxiv.org/abs/2510.01670v1
Updated: 2025-10-02T04:52:15Z

22: FOR-Prompting: From Objection to Revision via an Asymmetric Prompting  Protocol
Authors: ['He Zhang', 'Anzhou Zhang', 'Jian Dai']
Summary: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)organize internal deliberation but lack an explicit mechanism for externalquestioning that elicits self-revision. We present FOR-Prompting (FromObjection to Revision Prompting), an asymmetric protocol where a Defenderproposes an answer, an Objectioner raises question-style objections with nodirect fixes, and a Host enforces consistency and closure. On GSM8K we observeabout a 22% point gain over single-prompt and accuracy on par with CoT, withmore than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1judge. FOR-Prompting also corrects mistakes without tools or human supervisionon tricky queries, and improves performance for small-scale model (approx. 19%accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise forsmall models and on personal device use. Beyond factual QA, qualitativeanalyses on open-ended tasks show enhanced exploration and refinement, withdialogue traces that make assumptions and trade-offs explicit. The protocol ismodel agnostic and operates purely at the prompt level through role-structuredturns, so it works with hosted and local models of different sizes withoutretraining, and it supports large-scale study of objection-guided reasoning.
Link: http://arxiv.org/abs/2510.01674v1
Updated: 2025-10-02T04:57:58Z

23: How Do Language Models Compose Functions?
Authors: ['Apoorv Khandelwal', 'Ellie Pavlick']
Summary: While large language models (LLMs) appear to be increasingly capable ofsolving compositional tasks, it is an open question whether they do so usingcompositional mechanisms. In this work, we investigate how feedforward LLMssolve two-hop factual recall tasks, which can be expressed compositionally as$g(f(x))$. We first confirm that modern LLMs continue to suffer from the"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.Then, using logit lens on their residual stream activations, we identify twoprocessing mechanisms, one which solves tasks $\textit{compositionally}$,computing $f(x)$ along the way to computing $g(f(x))$, and one which solvesthem $\textit{directly}$, without any detectable signature of the intermediatevariable $f(x)$. Finally, we find that which mechanism is employed appears tobe related to the embedding space geometry, with the idiomatic mechanism beingdominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ inthe embedding spaces. We fully release our data and code at:https://github.com/apoorvkh/composing-functions .
Link: http://arxiv.org/abs/2510.01685v1
Updated: 2025-10-02T05:21:34Z

24: Improving AGI Evaluation: A Data Science Perspective
Authors: ['John Hawkins']
Summary: Evaluation of potential AGI systems and methods is difficult due to thebreadth of the engineering goal. We have no methods for perfect evaluation ofthe end state, and instead measure performance on small tests designed toprovide directional indication that we are approaching AGI. In this work weargue that AGI evaluation methods have been dominated by a design philosophythat uses our intuitions of what intelligence is to create synthetic tasks,that have performed poorly in the history of AI. Instead we argue for analternative design philosophy focused on evaluating robust task execution thatseeks to demonstrate AGI through competence. This perspective is developed fromcommon practices in data science that are used to show that a system can bereliably deployed. We provide practical examples of what this would mean forAGI evaluation.
Link: http://arxiv.org/abs/2510.01687v1
Updated: 2025-10-02T05:27:29Z

25: Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation
Authors: ['Seungseop Lim', 'Gibaeg Kim', 'Wooseok Han', 'Jean Seo', 'Hyunkyung Lee', 'Jaehyo Yoo', 'Eunho Yang']
Summary: Recent advances in Large Language Models (LLMs) have brought significantimprovements to various service domains, including chatbots and medicalpre-consultation applications. In the healthcare domain, the most commonapproach for adapting LLMs to multi-turn dialogue generation is SupervisedFine-Tuning (SFT). However, datasets for SFT in tasks like medicalpre-consultation typically exhibit a skewed turn-count distribution. Trainingon such data induces a novel failure mechanism we term **Format Inertia**,where models tend to generate repetitive, format-correct, but diagnosticallyuninformative questions in long medical dialogues. To mitigate this observedfailure mechanism, we adopt a simple, data-centric method that rebalances theturn-count distribution of the training dataset. Experimental results show thatour approach substantially alleviates Format Inertia in medicalpre-consultation.
Link: http://arxiv.org/abs/2510.01688v1
Updated: 2025-10-02T05:29:38Z

26: What MLLMs Learn about When they Learn about Multimodal Reasoning:  Perception, Reasoning, or their Integration?
Authors: ['Jiwan Chung', 'Neel Joshi', 'Pratyusha Sharma', 'Youngjae Yu', 'Vibhav Vineet']
Summary: Multimodal reasoning models have recently shown promise on challengingdomains such as olympiad-level geometry, yet their evaluation remains dominatedby aggregate accuracy, a single score that obscures where and how models areimproving. We introduce MathLens, a benchmark designed to disentangle thesubskills of multimodal reasoning while preserving the complexity oftextbook-style geometry problems. The benchmark separates performance intothree components: Perception: extracting information from raw inputs,Reasoning: operating on available information, and Integration: selectingrelevant perceptual evidence and applying it within reasoning. To support eachtest, we provide annotations: visual diagrams, textual descriptions to evaluatereasoning in isolation, controlled questions that require both modalities, andprobes for fine-grained perceptual skills, all derived from symbolicspecifications of the problems to ensure consistency and robustness. Ouranalysis reveals that different training approaches have uneven effects: First,reinforcement learning chiefly strengthens perception, especially whensupported by textual supervision, while textual SFT indirectly improvesperception through reflective reasoning. Second, reasoning improves only intandem with perception. Third, integration remains the weakest capacity, withresidual errors concentrated there once other skills advance. Finally,robustness diverges: RL improves consistency under diagram variation, whereasmultimodal SFT reduces it through overfitting. We will release all data andexperimental logs.
Link: http://arxiv.org/abs/2510.01719v1
Updated: 2025-10-02T06:58:29Z

27: Machine-interpretable Engineering Design Standards for Valve  Specification
Authors: ['Anders Gjerver', 'Rune Frostad', 'Vedrana Barisic', 'Melinda Hodkiewicz', 'Caitlin Woods', 'Mihaly Fekete', 'Arild Braathen Torjusen', 'Johan Wilhelm Kluwer']
Summary: Engineering design processes use technical specifications and must complywith standards. Product specifications, product type data sheets, and designstandards are still mainly document-centric despite the ambition to digitalizeindustrial work. In this paper, we demonstrate how to transform informationheld in engineering design standards into modular, reusable,machine-interpretable ontologies and use the ontologies in quality assurance ofthe plant design and equipment selection process. We use modelling patterns tocreate modular ontologies for knowledge captured in the text and in frequentlyreferenced tables in International Standards for piping, material and valvedesign. These modules are exchangeable, as stored in a W3C compliant format,and interoperable as they are aligned with the top-level ontology ISO DIS23726-3: Industrial Data Ontology (IDO).  We test these ontologies, created based on international material and pipingstandards and industry norms, on a valve selection process. Valves areinstantiated in semantic asset models as individuals along with a semanticrepresentation of the environmental condition at their location on the asset.We create "functional location tags" as OWL individuals that become instancesof OWL class Valve Data Sheet (VDS) specified valves. Similarly we createinstances of manufacturer product type. Our approach enables automatedvalidation that a specific VDS is compliant with relevant industry standards.Using semantic reasoning and executable design rules, we also determine whetherthe product type meets the valve specification. Creation of shared, reusableIDO-based modular ontologies for design standards enables semantic reasoning tobe applied to equipment selection processes and demonstrates the potential ofthis approach for Standards Bodies wanting to transition to digitized SmartStandards.
Link: http://arxiv.org/abs/2510.01736v1
Updated: 2025-10-02T07:20:37Z

28: Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware  Refusal in Factual Tasks
Authors: ['Wenbo Pan', 'Jie Xu', 'Qiguang Chen', 'Junhao Dong', 'Libo Qin', 'Xinfeng Li', 'Haining Yu', 'Xiaohua Jia']
Summary: Large Language Models (LLMs) should refuse to answer questions beyond theirknowledge. This capability, which we term knowledge-aware refusal, is crucialfor factual reliability. However, existing metrics fail to faithfully measurethis ability. On the one hand, simple refusal-based metrics are biased byrefusal rates and yield inconsistent scores when models exhibit differentrefusal tendencies. On the other hand, existing calibration metrics areproxy-based, capturing the performance of auxiliary calibration processesrather than the model's actual refusal behavior. In this work, we propose theRefusal Index (RI), a principled metric that measures how accurately LLMsrefuse questions they do not know. We define RI as Spearman's rank correlationbetween refusal probability and error probability. To make RI practicallymeasurable, we design a lightweight two-pass evaluation method that efficientlyestimates RI from observed refusal rates across two standard evaluation runs.Extensive experiments across 16 models and 5 datasets demonstrate that RIaccurately quantifies a model's intrinsic knowledge-aware refusal capability infactual tasks. Notably, RI remains stable across different refusal rates andprovides consistent model rankings independent of a model's overall accuracyand refusal rates. More importantly, RI provides insight into an important butpreviously overlooked aspect of LLM factuality: while LLMs achieve highaccuracy on factual tasks, their refusal behavior can be unreliable andfragile. This finding highlights the need to complement traditional accuracymetrics with the Refusal Index for comprehensive factuality evaluation.
Link: http://arxiv.org/abs/2510.01782v1
Updated: 2025-10-02T08:20:36Z

29: Comparison of Unsupervised Metrics for Evaluating Judicial Decision  Extraction
Authors: ['Ivan Leonidovich Litvak', 'Anton Kostin', 'Fedor Lashkin', 'Tatiana Maksiyan', 'Sergey Lagutin']
Summary: The rapid advancement of artificial intelligence in legal natural languageprocessing demands scalable methods for evaluating text extraction fromjudicial decisions. This study evaluates 16 unsupervised metrics, includingnovel formulations, to assess the quality of extracting seven semantic blocksfrom 1,000 anonymized Russian judicial decisions, validated against 7,168expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,semantic, structural, pseudo-ground truth, and legal-specific categories,operate without pre-annotated ground truth. Bootstrapped correlations, Lin'sconcordance correlation coefficient (CCC), and mean absolute error (MAE) revealthat Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negativecorrelations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, LinCCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, usinggpt-4.1-mini via g4f, suggests limited specialization for legal textse. Thesefindings highlight that unsupervised metrics, including LLM-based approaches,enable scalable screening but, with moderate correlations and low CCC values,cannot fully replace human judgment in high-stakes legal contexts. This workadvances legal NLP by providing annotation-free evaluation tools, withimplications for judicial analytics and ethical AI deployment.
Link: http://arxiv.org/abs/2510.01792v1
Updated: 2025-10-02T08:32:16Z

30: Detecting LLM-Generated Spam Reviews by Integrating Language Model  Embeddings and Graph Neural Network
Authors: ['Xin Liu', 'Rongwu Xu', 'Xinyi Jia', 'Jason Liao', 'Jiao Sun', 'Ling Huang', 'Wei Xu']
Summary: The rise of large language models (LLMs) has enabled the generation of highlypersuasive spam reviews that closely mimic human writing. These reviews posesignificant challenges for existing detection systems and threaten thecredibility of online platforms. In this work, we first create three realisticLLM-generated spam review datasets using three distinct LLMs, each guided byproduct metadata and genuine reference reviews. Evaluations by GPT-4.1 confirmthe high persuasion and deceptive potential of these reviews. To address thisthreat, we propose FraudSquad, a hybrid detection model that integrates textembeddings from a pre-trained language model with a gated graph transformer forspam node classification. FraudSquad captures both semantic and behavioralsignals without relying on manual feature engineering or massive trainingresources. Experiments show that FraudSquad outperforms state-of-the-artbaselines by up to 44.22% in precision and 43.01% in recall on threeLLM-generated datasets, while also achieving promising results on twohuman-written spam datasets. Furthermore, FraudSquad maintains a modest modelsize and requires minimal labeled training data, making it a practical solutionfor real-world applications. Our contributions include new synthetic datasets,a practical detection framework, and empirical evidence highlighting theurgency of adapting spam detection to the LLM era. Our code and datasets areavailable at: https://anonymous.4open.science/r/FraudSquad-5389/.
Link: http://arxiv.org/abs/2510.01801v1
Updated: 2025-10-02T08:42:35Z

31: Sparse Query Attention (SQA): A Computationally Efficient Attention  Mechanism with Query Heads Reduction
Authors: ['Adam Filipek']
Summary: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)mechanism, has become the de facto standard for state-of-the-art models inartificial intelligence. However, the quadratic computational complexity of MHAwith respect to sequence length presents a significant barrier to scaling,particularly for applications involving long contexts. Prevailing solutions,such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), haveeffectively addressed the memory bandwidth bottleneck that dominatesautoregressive inference latency by sharing Key and Value projections. Whilehighly successful, these methods do not reduce the fundamental number offloating-point operations (FLOPs) required for the attention score computation,which remains a critical bottleneck for training and full-sequence processing.This paper introduces Sparse Query Attention (SQA), a novel attentionarchitecture that pursues an alternative and complementary optimization path.Instead of reducing Key/Value heads, SQA reduces the number of Query heads.This architectural modification directly decreases the computational complexityof the attention mechanism by a factor proportional to the reduction in queryheads, thereby lowering the overall FLOPs. This work presents the theoreticalfoundation of SQA, its mathematical formulation, and a family of architecturalvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstratethat SQA can achieve significant throughput improvements of up to 3x incomputation-bound scenarios such as model pre-training, fine-tuning, andencoder-based tasks, with only a minimal impact on model quality in preliminarysmallscale experiments. SQA was discovered serendipitously during thedevelopment of the upcoming Reactive Transformer architecture, suggesting itspotential as a powerful tool for building more efficient and scalable models
Link: http://arxiv.org/abs/2510.01817v1
Updated: 2025-10-02T09:01:38Z

32: Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical  Errors
Authors: ['Dane Williamson', 'Yangfeng Ji', 'Matthew Dwyer']
Summary: Large Language Models (LLMs) demonstrate strong mathematical problem-solvingabilities but frequently fail on problems that deviate syntactically from theirtraining distribution. We identify a systematic failure mode, syntactic blindspots, in which models misapply familiar reasoning strategies to problems thatare semantically straightforward but phrased in unfamiliar ways. These errorsare not due to gaps in mathematical competence, but rather reflect a brittlecoupling between surface form and internal representation. To test this, werephrase incorrectly answered questions using syntactic templates drawn fromcorrect examples. These rephrasings, which preserve semantics while reducingstructural complexity, often lead to correct answers. We quantify syntacticcomplexity using a metric based on Dependency Locality Theory (DLT), and showthat higher DLT scores are associated with increased failure rates acrossmultiple datasets. Our findings suggest that many reasoning errors stem fromstructural misalignment rather than conceptual difficulty, and thatsyntax-aware interventions can reveal and mitigate these inductive failures.
Link: http://arxiv.org/abs/2510.01831v1
Updated: 2025-10-02T09:26:26Z

33: SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with  Reinforcement Learning
Authors: ['Shicheng Liu', 'Kai Sun', 'Lisheng Fu', 'Xilun Chen', 'Xinyuan Zhang', 'Zhaojiang Lin', 'Rulin Shao', 'Yue Liu', 'Anuj Kumar', 'Wen-tau Yih', 'Xin Luna Dong']
Summary: Semi-structured content in HTML tables, lists, and infoboxes accounts for asubstantial share of factual data on the web, yet the formatting complicatesusage, and reliably extracting structured information from them remainschallenging. Existing methods either lack generalization or areresource-intensive due to per-page LLM inference. In this paper, we introduceSCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novelreinforcement learning framework that leverages layout similarity acrosswebpages within the same site as a reward signal. Instead of processing eachpage individually, SCRIBES generates reusable extraction scripts that can beapplied to groups of structurally similar webpages. Our approach furtherimproves by iteratively training on synthetic annotations from in-the-wildCommonCrawl data. Experiments show that our approach outperforms strongbaselines by over 13% in script quality and boosts downstream questionanswering accuracy by more than 4% for GPT-4o, enabling scalable andresource-efficient web information extraction.
Link: http://arxiv.org/abs/2510.01832v1
Updated: 2025-10-02T09:27:15Z

34: Plan Then Action:High-Level Planning Guidance Reinforcement Learning for  LLM Reasoning
Authors: ['Zhihao Dou', 'Qinjian Zhao', 'Zhongwei Wan', 'Dinggen Zhang', 'Weida Wang', 'Towsif Raiyan', 'Benteng Chen', 'Qingtao Pan', 'Yang Ouyang', 'Zhiqiang Gao', 'Shufei Zhang', 'Sumon Biswas']
Summary: Large language models (LLMs) have demonstrated remarkable reasoning abilitiesin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,due to their autoregressive token-level generation, the reasoning process islargely constrained to local decision-making and lacks global planning. Thislimitation frequently results in redundant, incoherent, or inaccuratereasoning, which significantly degrades overall performance. Existingapproaches, such as tree-based algorithms and reinforcement learning (RL),attempt to address this issue but suffer from high computational costs andoften fail to produce optimal reasoning trajectories. To tackle this challenge,we propose Plan-Then-Action Enhanced Reasoning with Group Relative PolicyOptimization PTA-GRPO, a two-stage framework designed to improve bothhigh-level planning and fine-grained CoT reasoning. In the first stage, weleverage advanced LLMs to distill CoT into compact high-level guidance, whichis then used for supervised fine-tuning (SFT). In the second stage, weintroduce a guidance-aware RL method that jointly optimizes the final outputand the quality of high-level guidance, thereby enhancing reasoningeffectiveness. We conduct extensive experiments on multiple mathematicalreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, acrossdiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, andLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistentlyachieves stable and significant improvements across different models and tasks,validating its effectiveness and generalization.
Link: http://arxiv.org/abs/2510.01833v1
Updated: 2025-10-02T09:28:13Z

35: Model Merging to Maintain Language-Only Performance in Developmentally  Plausible Multimodal Models
Authors: ['Ece Takmaz', 'Lisa Bylinina', 'Jakub Dotlacil']
Summary: State-of-the-art vision-and-language models consist of many parameters andlearn from enormous datasets, surpassing the amounts of linguistic data thatchildren are exposed to as they acquire a language. This paper presents ourapproach to the multimodal track of the BabyLM challenge addressing thisdiscrepancy. We develop language-only and multimodal models in low-resourcesettings using developmentally plausible datasets, with our multimodal modelsoutperforming previous BabyLM baselines. One finding in the multimodal languagemodel literature is that these models tend to underperform in\textit{language-only} tasks. Therefore, we focus on maintaining language-onlyabilities in multimodal models. To this end, we experiment with \textit{modelmerging}, where we fuse the parameters of multimodal models with those oflanguage-only models using weighted linear interpolation. Our resultscorroborate the findings that multimodal models underperform in language-onlybenchmarks that focus on grammar, and model merging with text-only models canhelp alleviate this problem to some extent, while maintaining multimodalperformance.
Link: http://arxiv.org/abs/2510.01845v1
Updated: 2025-10-02T09:38:25Z

36: REPAIR: Robust Editing via Progressive Adaptive Intervention and  Reintegration
Authors: ['Yisu Wang', 'Ming Wang', 'Haoyuan Song', 'Wenjie Huang', 'Chaozheng Wang', 'Yi Xie', 'Xuming Ran']
Summary: Post-training for large language models (LLMs) is constrained by the highcost of acquiring new knowledge or correcting errors and by the unintended sideeffects that frequently arise from retraining. To address these issues, weintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention andReintegration), a lifelong editing framework designed to support precise andlow-cost model updates while preserving non-target knowledge. REPAIR mitigatesthe instability and conflicts of large-scale sequential edits through aclosed-loop feedback mechanism coupled with dynamic memory management.Furthermore, by incorporating frequent knowledge fusion and enforcing stronglocality guards, REPAIR effectively addresses the shortcomings of traditionaldistribution-agnostic approaches that often overlook unintended ripple effects.Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%across multiple model families and significantly reduces knowledge forgetting.This work introduces a robust framework for developing reliable, scalable, andcontinually evolving LLMs.
Link: http://arxiv.org/abs/2510.01879v1
Updated: 2025-10-02T10:35:39Z

37: Constrained Adaptive Rejection Sampling
Authors: ['Paweł Parys', 'Sairam Vaidya', 'Taylor Berg-Kirkpatrick', "Loris D'Antoni"]
Summary: Language Models (LMs) are increasingly used in applications where generatedoutputs must satisfy strict semantic or syntactic constraints. Existingapproaches to constrained generation fall along a spectrum: greedy constraineddecoding methods enforce validity during decoding but distort the LM'sdistribution, while rejection sampling (RS) preserves fidelity but wastescomputation by discarding invalid outputs. Both extremes are problematic indomains such as program fuzzing, where both validity and diversity of samplesare essential. We present Constrained Adaptive Rejection Sampling (CARS), anapproach that strictly improves the sample-efficiency of RS withoutdistributional distortion. CARS begins with unconstrained LM sampling andadaptively rules out constraint-violating continuations by recording them in atrie and subtracting their probability mass from future draws. This adaptivepruning ensures that prefixes proven invalid are never revisited, acceptancerates improve monotonically, and the resulting samples exactly follow theconstrained distribution. In experiments on a variety of domains -- e.g.,program fuzzing and molecular generation -- CARS consistently achieves higherefficiency -- measured in the number of LM forward passes per valid sample --while also producing stronger sample diversity than both GCD and methods thatapproximate the LM's distribution.
Link: http://arxiv.org/abs/2510.01902v1
Updated: 2025-10-02T11:17:26Z

38: Enhancing Large Language Model Reasoning with Reward Models: An  Analytical Survey
Authors: ['Qiyuan Liu', 'Hao Xu', 'Xuhong Chen', 'Wei Chen', 'Yee Whye Teh', 'Ning Miao']
Summary: Reward models (RMs) play a critical role in enhancing the reasoningperformance of LLMs. For example, they can provide training signals to finetuneLLMs during reinforcement learning (RL) and help select the best answer frommultiple candidates during inference. In this paper, we provide a systematicintroduction to RMs, along with a comprehensive survey of their applications inLLM reasoning. We first review fundamental concepts of RMs, including theirarchitectures, training methodologies, and evaluation techniques. Then, weexplore their key applications: (1) guiding generation and selecting optimaloutputs during LLM inference, (2) facilitating data synthesis and iterativeself-improvement for LLMs, and (3) providing training signals in RL-basedfinetuning. Finally, we address critical open questions regarding theselection, generalization, evaluation, and enhancement of RMs, based onexisting research and our own empirical findings. Our analysis aims to provideactionable insights for the effective deployment and advancement of RMs for LLMreasoning.
Link: http://arxiv.org/abs/2510.01925v1
Updated: 2025-10-02T11:42:17Z

39: Inverse Language Modeling towards Robust and Grounded LLMs
Authors: ['Davide Gabrielli', 'Simone Sestito', 'Iacopo Masi']
Summary: The current landscape of defensive mechanisms for LLMs is fragmented andunderdeveloped, unlike prior work on classifiers. To further promoteadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), aunified framework that simultaneously 1) improves the robustness of LLMs toinput perturbations, and, at the same time, 2) enables native grounding byinverting model outputs to identify potentially toxic or unsafe input triggers.ILM transforms LLMs from static generators into analyzable and robust systems,potentially helping RED teaming. ILM can lay the foundation for next-generationLLMs that are not only robust and grounded but also fundamentally morecontrollable and trustworthy. The code is publicly available atgithub.com/davegabe/pag-llm.
Link: http://arxiv.org/abs/2510.01929v1
Updated: 2025-10-02T11:47:18Z

40: Veri-R1: Toward Precise and Faithful Claim Verification via Online  Reinforcement Learning
Authors: ['Qi He', 'Cheng Qian', 'Xiusi Chen', 'Bingxiang He', 'Yi R.', 'Fung', 'Heng Ji']
Summary: Claim verification with large language models (LLMs) has recently attractedconsiderable attention, owing to their superior reasoning capabilities andtransparent verification pathways compared to traditional answer-onlyjudgments. Online claim verification requires iterative evidence retrieval andreasoning, yet existing approaches mainly rely on prompt engineering orpredesigned reasoning workflows without offering a unified training paradigm toimprove necessary skills. Therefore, we introduce Veri-R1, an onlinereinforcement learning (RL) framework that enables an LLM to interact with asearch engine and to receive reward signals that explicitly shape its planning,retrieval, and reasoning behaviors. The dynamic interaction between models andretrieval systems more accurately reflects real-world verification scenariosand fosters comprehensive verification skills. Empirical results show thatVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, oftensurpassing larger-scale counterparts. Ablation studies further reveal theimpact of reward components and the link between output logits and labelaccuracy. Our results highlight the effectiveness of online RL for precise andfaithful claim verification and provide a foundation for future research. Werelease our code to support community progress in LLM empowered claimverification.
Link: http://arxiv.org/abs/2510.01932v1
Updated: 2025-10-02T11:49:48Z

41: Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion,  Argument, and Topic Annotations
Authors: ['Adina Nicola Dobrinoiu', 'Ana Cristiana Marcu', 'Amir Homayounirad', 'Luciano Cavalcante Siebert', 'Enrico Liscio']
Summary: Our interpretation of value concepts is shaped by our socioculturalbackground and lived experiences, and is thus subjective. Recognizingindividual value interpretations is important for developing AI systems thatcan align with diverse human perspectives and avoid bias toward majorityviewpoints. To this end, we investigate whether a language model can predictindividual value interpretations by leveraging multi-dimensional subjectiveannotations as a proxy for their interpretive lens. That is, we evaluatewhether providing examples of how an individual annotates Sentiment, Emotion,Argument, and Topics (SEAT dimensions) helps a language model in predictingtheir value interpretations. Our experiment across different zero- and few-shotsettings demonstrates that providing all SEAT dimensions simultaneously yieldssuperior performance compared to individual dimensions and a baseline where noinformation about the individual is provided. Furthermore, individualvariations across annotators highlight the importance of accounting for theincorporation of individual subjective annotators. To the best of ourknowledge, this controlled setting, although small in size, is the firstattempt to go beyond demographics and investigate the impact of annotationbehavior on value prediction, providing a solid foundation for futurelarge-scale validation.
Link: http://arxiv.org/abs/2510.01976v1
Updated: 2025-10-02T12:51:33Z

42: Exploring Database Normalization Effects on SQL Generation
Authors: ['Ryosuke Kohita']
Summary: Schema design, particularly normalization, is a critical yet often overlookedfactor in natural language to SQL (NL2SQL) systems. Most prior researchevaluates models on fixed schemas, overlooking the influence of design onperformance. We present the first systematic study of schema normalization'simpact, evaluating eight leading large language models on synthetic andreal-world datasets with varied normalization levels. We construct controlledsynthetic datasets with formal normalization (1NF-3NF) and real academic paperdatasets with practical schemes. Our results show that denormalized schemasoffer high accuracy on simple retrieval queries, even with cost-effectivemodels in zero-shot settings. In contrast, normalized schemas (2NF/3NF)introduce challenges such as errors in base table selection and join typeprediction; however, these issues are substantially mitigated by providingfew-shot examples. For aggregation queries, normalized schemas yielded betterperformance, mainly due to their robustness against the data duplication andNULL value issues that cause errors in denormalized schemas. These findingssuggest that the optimal schema design for NL2SQL applications depends on thetypes of queries to be supported. Our study demonstrates the importance ofconsidering schema design when developing NL2SQL interfaces and integratingadaptive schema selection for real-world scenarios.
Link: http://arxiv.org/abs/2510.01989v1
Updated: 2025-10-02T13:11:30Z

43: LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and  Target
Authors: ['Md Arid Hasan', 'Firoj Alam', 'Md Fahad Hossain', 'Usman Naseem', 'Syed Ishtiaque Ahmed']
Summary: Online social media platforms are central to everyday communication andinformation seeking. While these platforms serve positive purposes, they alsoprovide fertile ground for the spread of hate speech, offensive language, andbullying content targeting individuals, organizations, and communities. Suchcontent undermines safety, participation, and equity online. Reliable detectionsystems are therefore needed, especially for low-resource languages wheremoderation tools are limited. In Bangla, prior work has contributed resourcesand models, but most are single-task (e.g., binary hate/offense) with limitedcoverage of multi-facet signals (type, severity, target). We address these gapsby introducing the first multi-task Bangla hate-speech dataset,BanglaMultiHate, one of the largest manually annotated corpus to date. Buildingon this resource, we conduct a comprehensive, controlled comparison spanningclassical baselines, monolingual pretrained models, and LLMs under zero-shotprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in alow-resource setting and reveal a consistent trend: although LoRA-tuned LLMsare competitive with BanglaBERT, culturally and linguistically groundedpretraining remains critical for robust performance. Together, our dataset andfindings establish a stronger benchmark for developing culturally alignedmoderation tools in low-resource contexts. For reproducibility, we will releasethe dataset and all related scripts.
Link: http://arxiv.org/abs/2510.01995v1
Updated: 2025-10-02T13:17:11Z

44: Style Over Story: A Process-Oriented Study of Authorial Creativity in  Large Language Models
Authors: ['Donghoon Jung', 'Jiwoo Choi', 'Songeun Chae', 'Seohyon Jung']
Summary: Evaluations of large language models (LLMs)' creativity have focusedprimarily on the quality of their outputs rather than the processes that shapethem. This study takes a process-oriented approach, drawing on narratology toexamine LLMs as computational authors. We introduce constraint-baseddecision-making as a lens for authorial creativity. Using controlled promptingto assign authorial personas, we analyze the creative preferences of themodels. Our findings show that LLMs consistently emphasize Style over otherelements, including Character, Event, and Setting. By also probing thereasoning the models provide for their choices, we show that distinctiveprofiles emerge across models and argue that our approach provides a novelsystematic tool for analyzing AI's authorial creativity.
Link: http://arxiv.org/abs/2510.02025v1
Updated: 2025-10-02T13:57:14Z

45: Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming  Tool Usage
Authors: ['Siddhant Arora', 'Haidar Khan', 'Kai Sun', 'Xin Luna Dong', 'Sajal Choudhary', 'Seungwhan Moon', 'Xinyuan Zhang', 'Adithya Sagar', 'Surya Teja Appini', 'Kaushik Patnaik', 'Sanat Sharma', 'Shinji Watanabe', 'Anuj Kumar', 'Ahmed Aly', 'Yue Liu', 'Florian Metze', 'Zhaojiang Lin']
Summary: End-to-end speech-in speech-out dialogue systems are emerging as a powerfulalternative to traditional ASR-LLM-TTS pipelines, generating more natural,expressive responses with significantly lower latency. However, these systemsremain prone to hallucinations due to limited factual grounding. Whiletext-based dialogue systems address this challenge by integrating tools such asweb search and knowledge graph APIs, we introduce the first approach to extendtool use directly into speech-in speech-out systems. A key challenge is thattool integration substantially increases response latency, disruptingconversational flow. To mitigate this, we propose Streaming Retrieval-AugmentedGeneration (Streaming RAG), a novel framework that reduces user-perceivedlatency by predicting tool queries in parallel with user speech, even beforethe user finishes speaking. Specifically, we develop a post-training pipelinethat teaches the model when to issue tool calls during ongoing speech and howto generate spoken summaries that fuse audio queries with retrieved textresults, thereby improving both accuracy and responsiveness. To evaluate ourapproach, we construct AudioCRAG, a benchmark created by converting queriesfrom the publicly available CRAG dataset into speech form. Experimental resultsdemonstrate that our streaming RAG approach increases QA accuracy by up to 200%relative (from 11.1% to 34.2% absolute) and further enhances user experience byreducing tool use latency by 20%. Importantly, our streaming RAG approach ismodality-agnostic and can be applied equally to typed input, paving the way formore agentic, real-time AI assistants.
Link: http://arxiv.org/abs/2510.02044v1
Updated: 2025-10-02T14:18:20Z

46: Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken  Dialogue Systems
Authors: ['Siddhant Arora', 'Jinchuan Tian', 'Hayato Futami', 'Jiatong Shi', 'Yosuke Kashiwagi', 'Emiru Tsunoo', 'Shinji Watanabe']
Summary: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activitydetection (VAD) for turn-taking, but VAD fails to distinguish between pausesand turn completions. Duplex SDS models address this by predicting outputcontinuously, including silence tokens, thus removing the need for explicitVAD. However, they often have complex dual-channel architecture and lag behindcascaded models in semantic reasoning. To overcome these challenges, we proposeSCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternatingbetween processing fixed-duration user input and generating responses in ablockwise manner. Using frame-level alignments, we create intermediatetargets-aligned user transcripts and system responses for each block.Experiments show that our approach produces more coherent and interpretableresponses than existing duplex methods while supporting lower-latency andoverlapping interactions compared to turn-by-turn systems.
Link: http://arxiv.org/abs/2510.02066v1
Updated: 2025-10-02T14:33:05Z

47: Do AI Models Perform Human-like Abstract Reasoning Across Modalities?
Authors: ['Claas Beger', 'Ryan Yi', 'Shuhao Fu', 'Arseny Moskvichev', 'Sarah W. Tsai', 'Sivasankaran Rajamanickam', 'Melanie Mitchell']
Summary: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGIbenchmark, but does that mean state-of-the-art models recognize and reason withthe abstractions that the task creators intended? We investigate models'abstraction abilities on ConceptARC. We evaluate models under settings thatvary the input modality (textual vs. visual), whether the model is permitted touse external Python tools, and, for reasoning models, the amount of reasoningeffort. In addition to measuring output accuracy, we perform fine-grainedevaluation of the natural-language rules that models generate to explain theirsolutions. This dual evaluation lets us assess whether models solve tasks usingthe abstractions ConceptARC was designed to elicit, rather than relying onsurface-level patterns. Our results show that, while some models usingtext-based representations match human output accuracy, the best models' rulesare often based on surface-level ``shortcuts'' and capture intendedabstractions far less often than humans. Thus their capabilities for generalabstract reasoning may be overestimated by evaluations based on accuracy alone.In the visual modality, AI models' output accuracy drops sharply, yet ourrule-level analysis reveals that models might be underestimated, as they stillexhibit a substantial share of rules that capture intended abstractions, butare often unable to correctly apply these rules. In short, our results showthat models still lag humans in abstract reasoning, and that using accuracyalone to evaluate abstract reasoning on ARC-like tasks may overestimateabstract-reasoning capabilities in textual modalities and underestimate it invisual modalities. We believe that our evaluation framework offers a morefaithful picture of multimodal models' abstract reasoning abilities and a moreprincipled way to track progress toward human-like, abstraction-centeredintelligence.
Link: http://arxiv.org/abs/2510.02125v1
Updated: 2025-10-02T15:35:10Z

48: The Disparate Impacts of Speculative Decoding
Authors: ['Jameson Sandler', 'Ahmet Üstün', 'Marco Romanelli', 'Sara Hooker', 'Ferdinando Fioretto']
Summary: The practice of speculative decoding, whereby inference is probabilisticallysupported by a smaller, cheaper, ``drafter'' model, has become a standardtechnique for systematically reducing the decoding time of large languagemodels. This paper conducts an analysis of speculative decoding through thelens of its potential disparate speed-up rates across tasks. Crucially, thepaper shows that speed-up gained from speculative decoding is not uniformlydistributed across tasks, consistently diminishing for under-fit, and oftenunderrepresented tasks. To better understand this phenomenon, we derive ananalysis to quantify this observed ``unfairness'' and draw attention to thefactors that motivate such disparate speed-ups to emerge. Further, guided bythese insights, the paper proposes a mitigation strategy designed to reducespeed-up disparities and validates the approach across several model pairs,revealing on average a 12% improvement in our fairness metric.
Link: http://arxiv.org/abs/2510.02128v1
Updated: 2025-10-02T15:38:57Z

49: RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with  Self-Penalization
Authors: ['Zhaoning Yu', 'Will Su', 'Leitian Tao', 'Haozhu Wang', 'Aashu Singh', 'Hanchao Yu', 'Jianyu Wang', 'Hongyang Gao', 'Weizhe Yuan', 'Jason Weston', 'Ping Yu', 'Jing Xu']
Summary: Reinforcement learning with human-annotated data has boosted chain-of-thoughtreasoning in large reasoning models, but these gains come at high costs inlabeled data while faltering on harder tasks. A natural next step isexperience-driven learning, where models improve without curated labels byadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning withSelf-restraint), a self-penalizing RL framework that converts the absence ofgold labels into a useful learning signal. Instead of overcommitting tospurious majority votes, RESTRAIN exploits signals from the model's entireanswer distribution: penalizing overconfident rollouts and low-consistencyexamples while preserving promising reasoning chains. The self-penalizationmechanism integrates seamlessly into policy optimization methods such as GRPO,enabling continual self-improvement without supervision. On challengingreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent onGPQA-Diamond, nearly matching gold-label training while using no gold labels.These results demonstrate that RESTRAIN establishes a scalable path towardstronger reasoning without gold labels.
Link: http://arxiv.org/abs/2510.02172v1
Updated: 2025-10-02T16:24:01Z

50: Learning to Reason for Hallucination Span Detection
Authors: ['Hsuan Su', 'Ting-Yao Hu', 'Hema Swetha Koppula', 'Kundan Krishna', 'Hadi Pouransari', 'Cheng-Yu Hsieh', 'Cem Koc', 'Joseph Yitan Cheng', 'Oncel Tuzel', 'Raviteja Vemulapalli']
Summary: Large language models (LLMs) often generate hallucinations -- unsupportedcontent that undermines reliability. While most prior works frame hallucinationdetection as a binary task, many real-world applications require identifyinghallucinated spans, which is a multi-step decision making process. Thisnaturally raises the question of whether explicit reasoning can help thecomplex task of detecting hallucination spans. To answer this question, wefirst evaluate pretrained models with and without Chain-of-Thought (CoT)reasoning, and show that CoT reasoning has the potential to generate at leastone correct answer when sampled multiple times. Motivated by this, we proposeRL4HS, a reinforcement learning framework that incentivizes reasoning with aspan-level reward function. RL4HS builds on Group Relative Policy Optimizationand introduces Class-Aware Policy Optimization to mitigate reward imbalanceissue. Experiments on the RAGTruth benchmark (summarization, questionanswering, data-to-text) show that RL4HS surpasses pretrained reasoning modelsand supervised fine-tuning, demonstrating the necessity of reinforcementlearning with span-level rewards for detecting hallucination spans.
Link: http://arxiv.org/abs/2510.02173v1
Updated: 2025-10-02T16:24:28Z

51: A Rigorous Benchmark with Multidimensional Evaluation for Deep Research  Agents: From Answers to Reports
Authors: ['Yang Yao', 'Yixu Wang', 'Yuxuan Zhang', 'Yi Lu', 'Tianle Gu', 'Lingyu Li', 'Dingyi Zhao', 'Keming Wu', 'Haozhe Wang', 'Ping Nie', 'Yan Teng', 'Yingchun Wang']
Summary: Artificial intelligence is undergoing the paradigm shift from closed languagemodels to interconnected agent systems capable of external perception andinformation integration. As a representative embodiment, Deep Research Agents(DRAs) systematically exhibit the capabilities for task decomposition,cross-source retrieval, multi-stage reasoning, and structured output, whichmarkedly enhance performance on complex and open-ended tasks. However, existingbenchmarks remain deficient in evaluation dimensions, response formatting, andscoring mechanisms, limiting their capacity to assess such systems effectively.This paper introduces a rigorous benchmark and a multidimensional evaluationframework tailored to DRAs and report-style responses. The benchmark comprises214 expert-curated challenging queries distributed across 10 broad thematicdomains, each accompanied by manually constructed reference bundles to supportcomposite evaluation. The framework enables comprehensive evaluation oflong-form reports generated by DRAs, incorporating integrated scoring metricsfor semantic quality, topical focus, and retrieval trustworthiness. Extensiveexperimentation confirms the superior performance of mainstream DRAs overweb-search-tool-augmented reasoning models, yet reveals considerable scope forfurther improvement. This study provides a robust foundation for capabilityassessment, architectural refinement, and paradigm advancement in DRA systems.
Link: http://arxiv.org/abs/2510.02190v1
Updated: 2025-10-02T16:40:02Z

52: ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge  Graph Exploration Utilities
Authors: ['Felix Brei', 'Lorenz Bühmann', 'Johannes Frey', 'Daniel Gerber', 'Lars-Peter Meyer', 'Claus Stadler', 'Kirill Bulert']
Summary: Interacting with knowledge graphs can be a daunting task for people without abackground in computer science since the query language that is used (SPARQL)has a high barrier of entry. Large language models (LLMs) can lower thatbarrier by providing support in the form of Text2SPARQL translation. In thispaper we introduce a generalized method based on SPINACH, an LLM backed agentthat translates natural language questions to SPARQL queries not in a singleshot, but as an iterative process of exploration and execution. We describe theoverall architecture and reasoning behind our design decisions, and alsoconduct a thorough analysis of the agent behavior to gain insights into futureareas for targeted improvements. This work was motivated by the Text2SPARQLchallenge, a challenge that was held to facilitate improvements in theText2SPARQL domain.
Link: http://arxiv.org/abs/2510.02200v1
Updated: 2025-10-02T16:49:27Z

53: Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in  VLM-Powered Mobile-Use Agents
Authors: ['Lingzhong Dong', 'Ziqi Zhou', 'Shuaibo Yang', 'Haiyue Sheng', 'Pengzhou Cheng', 'Zongru Wu', 'Zheng Wu', 'Gongshen Liu', 'Zhuosheng Zhang']
Summary: Mobile-use agents powered by vision-language models (VLMs) have shown greatpotential in interpreting natural language instructions and generatingcorresponding actions based on mobile graphical user interface. Recent studiessuggest that incorporating chain-of-thought (CoT) reasoning tends to improvethe execution accuracy. However, existing evaluations emphasize executionaccuracy while neglecting whether CoT reasoning aligns with ground-truthactions. This oversight fails to assess potential reasoning-execution gaps,which in turn foster over-trust: users relying on seemingly plausible CoTs mayunknowingly authorize harmful actions, potentially resulting in financial lossor trust crisis. In this work, we introduce a new evaluation framework todiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment(GTA), which measures whether the action implied by a CoT matches theground-truth action. By combining GTA with the standard Exact Match (EM)metric, we jointly assess both the reasoning accuracy and execution accuracy.This joint perspective reveals two types of reasoning-execution gaps: (i)Execution Gap (EG), where the reasoning correctly identifies the correct actionbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds butreasoning process conflicts with the actual execution. Experimental resultsacross a wide range of mobile interaction tasks reveal that reasoning-executiongaps are prevalent, with execution gaps occurring more frequently thanreasoning gaps. Moreover, while scaling up model size reduces the overall gap,sizable execution gaps persist even in the largest models. Further analysisshows that our framework reliably reflects systematic EG/RG patterns instate-of-the-art models. These findings offer concrete diagnostics and supportthe development of more trustworthy mobile-use agents.
Link: http://arxiv.org/abs/2510.02204v1
Updated: 2025-10-02T16:51:19Z

54: StockBench: Can LLM Agents Trade Stocks Profitably In Real-world  Markets?
Authors: ['Yanxu Chen', 'Zijun Yao', 'Yantao Liu', 'Jin Ye', 'Jianing Yu', 'Lei Hou', 'Juanzi Li']
Summary: Large language models (LLMs) have recently demonstrated strong capabilitiesas autonomous agents, showing promise in reasoning, tool use, and sequentialdecision-making. While prior benchmarks have evaluated LLM agents in domainssuch as software engineering and scientific discovery, the finance domainremains underexplored, despite its direct relevance to economic value andhigh-stakes decision-making. Existing financial benchmarks primarily teststatic knowledge through question answering, but they fall short of capturingthe dynamic and iterative nature of trading. To address this gap, we introduceStockBench, a contamination-free benchmark designed to evaluate LLM agents inrealistic, multi-month stock trading environments. Agents receive daily marketsignals -- including prices, fundamentals, and news -- and must make sequentialbuy, sell, or hold decisions. Performance is assessed using financial metricssuch as cumulative return, maximum drawdown, and the Sortino ratio. Ourevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) andopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLMagents struggle to outperform the simple buy-and-hold baseline, several modelsdemonstrate the potential to deliver higher returns and manage risk moreeffectively. These findings highlight both the challenges and opportunities indeveloping LLM-powered financial agents, showing that excelling at staticfinancial knowledge tasks does not necessarily translate into successfultrading strategies. We release StockBench as an open-source resource to supportreproducibility and advance future research in this domain.
Link: http://arxiv.org/abs/2510.02209v1
Updated: 2025-10-02T16:54:57Z

55: More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for  Diverse Exploration
Authors: ['Xiaoyang Yuan', 'Yujuan Ding', 'Yi Bin', 'Wenqi Shao', 'Jinyu Cai', 'Jingkuan Song', 'Yang Yang', 'Hengtao Shen']
Summary: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigmfor enhancing the reasoning ability in Large Language Models (LLMs). However,prevailing methods primarily rely on self-exploration or a single off-policyteacher to elicit long chain-of-thought (LongCoT) reasoning, which mayintroduce intrinsic model biases and restrict exploration, ultimately limitingreasoning diversity and performance. Drawing inspiration from multi-teacherstrategies in knowledge distillation, we introduce Adaptive Multi-GuidancePolicy Optimization (AMPO), a novel framework that adaptively leveragesguidance from multiple proficient teacher models, but only when the on-policymodel fails to generate correct solutions. This "guidance-on-demand" approachexpands exploration while preserving the value of self-discovery. Moreover,AMPO incorporates a comprehension-based selection mechanism, prompting thestudent to learn from the reasoning paths that it is most likely to comprehend,thus balancing broad exploration with effective exploitation. Extensiveexperiments show AMPO substantially outperforms a strong baseline (GRPO), witha 4.3% improvement on mathematical reasoning tasks and 12.2% onout-of-distribution tasks, while significantly boosting Pass@k performance andenabling more diverse exploration. Notably, using four peer-sized teachers, ourmethod achieves comparable results to approaches that leverage a single, morepowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstratea more efficient and scalable path to superior reasoning and generalizability.Our code is available at https://github.com/SII-Enigma/AMPO.
Link: http://arxiv.org/abs/2510.02227v1
Updated: 2025-10-02T17:14:00Z

56: The Reasoning Boundary Paradox: How Reinforcement Learning Constrains  Language Models
Authors: ['Phuc Minh Nguyen', 'Chinh D. La', 'Duy M. H. Nguyen', 'Nitesh V. Chawla', 'Binh T. Nguyen', 'Khoa D. Doan']
Summary: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a keymethod for improving Large Language Models' reasoning capabilities, yet recentevidence suggests it may paradoxically shrink the reasoning boundary ratherthan expand it. This paper investigates the shrinkage issue of RLVR byanalyzing its learning dynamics and reveals two critical phenomena that explainthis failure. First, we expose negative interference in RLVR, where learning tosolve certain training problems actively reduces the likelihood of correctsolutions for others, leading to the decline of Pass@$k$ performance, or theprobability of generating a correct solution within $k$ attempts. Second, weuncover the winner-take-all phenomenon: RLVR disproportionately reinforcesproblems with high likelihood, correct solutions, under the base model, whilesuppressing other initially low-likelihood ones. Through extensive theoreticaland empirical analysis on multiple mathematical reasoning benchmarks, we showthat this effect arises from the inherent on-policy sampling in standard RLobjectives, causing the model to converge toward narrow solution strategies.Based on these insights, we propose a simple yet effective data curationalgorithm that focuses RLVR learning on low-likelihood problems, achievingnotable improvement in Pass@$k$ performance. Our code is available athttps://github.com/mail-research/SELF-llm-interference.
Link: http://arxiv.org/abs/2510.02230v1
Updated: 2025-10-02T17:17:27Z

57: Enhanced Arabic-language cyberbullying detection: deep embedding and  transformer (BERT) approaches
Authors: ['Ebtesam Jaber Aljohani', 'Wael M. S. Yafoo']
Summary: Recent technological advances in smartphones and communications, includingthe growth of such online platforms as massive social media networks such as X(formerly known as Twitter) endangers young people and their emotionalwell-being by exposing them to cyberbullying, taunting, and bullying content.Most proposed approaches for automatically detecting cyberbullying have beendeveloped around the English language, and methods for detectingArabic-language cyberbullying are scarce. Methods for detecting Arabic-languagecyberbullying are especially scarce. This paper aims to enhance theeffectiveness of methods for detecting cyberbullying in Arabic-languagecontent. We assembled a dataset of 10,662 X posts, pre-processed the data, andused the kappa tool to verify and enhance the quality of our annotations. Weconducted four experiments to test numerous deep learning models forautomatically detecting Arabic-language cyberbullying. We first tested a longshort-term memory (LSTM) model and a bidirectional long short-term memory(Bi-LSTM) model with several experimental word embeddings. We also tested theLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder fromrepresentations (BERT) and then tested them on a different experimental modelsBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTMwith FastText embedding word performed even better, achieving 98% accuracy. Asa result, the outcomes are generalize
Link: http://dx.doi.org/10.11591/ijai.v14.i3.pp2258-2269
Updated: 2025-10-02T17:20:02Z

58: Study on LLMs for Promptagator-Style Dense Retriever Training
Authors: ['Daniel Gwon', 'Nour Jedidi', 'Jimmy Lin']
Summary: Promptagator demonstrated that Large Language Models (LLMs) with few-shotprompts can be used as task-specific query generators for fine-tuningdomain-specialized dense retrieval models. However, the original Promptagatorapproach relied on proprietary and large-scale LLMs which users may not haveaccess to or may be prohibited from using with sensitive data. In this work, westudy the impact of open-source LLMs at accessible scales ($\leq$14Bparameters) as an alternative. Our results demonstrate that open-source LLMs assmall as 3B parameters can serve as effective Promptagator-style querygenerators. We hope our work will inform practitioners with reliablealternatives for synthetic data generation and give insights to maximizefine-tuning results for domain-specific applications.
Link: http://arxiv.org/abs/2510.02241v1
Updated: 2025-10-02T17:29:51Z

59: AccurateRAG: A Framework for Building Accurate Retrieval-Augmented  Question-Answering Applications
Authors: ['Linh The Nguyen', 'Chi Tran', 'Dung Ngoc Nguyen', 'Van-Cuong Pham', 'Hoang Ngo', 'Dat Quoc Nguyen']
Summary: We introduce AccurateRAG -- a novel framework for constructinghigh-performance question-answering applications based on retrieval-augmentedgeneration (RAG). Our framework offers a pipeline for development efficiencywith tools for raw dataset processing, fine-tuning data generation, textembedding & LLM fine-tuning, output evaluation, and building RAG systemslocally. Experimental results show that our framework outperforms previousstrong baselines and obtains new state-of-the-art question-answeringperformance on benchmark datasets.
Link: http://arxiv.org/abs/2510.02243v1
Updated: 2025-10-02T17:30:08Z

60: ExGRPO: Learning to Reason from Experience
Authors: ['Runzhe Zhan', 'Yafu Li', 'Zhi Wang', 'Xiaoye Qu', 'Dongrui Liu', 'Jing Shao', 'Derek F. Wong', 'Yu Cheng']
Summary: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigmfor improving the reasoning ability of large language models. However, standardon-policy training discards rollout experiences after a single update, leadingto computational inefficiency and instability. While prior work on RL hashighlighted the benefits of reusing past experience, the role of experiencecharacteristics in shaping learning dynamics of large reasoning models remainsunderexplored. In this paper, we are the first to investigate what makes areasoning experience valuable and identify rollout correctness and entropy aseffective indicators of experience value. Based on these insights, we proposeExGRPO (Experiential Group Relative Policy Optimization), a framework thatorganizes and prioritizes valuable experiences, and employs a mixed-policyobjective to balance exploration with experience exploitation. Experiments onfive backbone models (1.5B-8B parameters) show that ExGRPO consistentlyimproves reasoning performance on mathematical/general benchmarks, with anaverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPOstabilizes training on both stronger and weaker models where on-policy methodsfail. These results highlight principled experience management as a keyingredient for efficient and scalable RLVR.
Link: http://arxiv.org/abs/2510.02245v1
Updated: 2025-10-02T17:31:30Z

61: Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative  Entropy Regulation
Authors: ['Tianyi Jiang', 'Yi Bin', 'Yujuan Ding', 'Kainian Zhu', 'Fei Ma', 'Jingkuan Song', 'Heng Tao Shen']
Summary: Large Language Models (LLMs) have demonstrated remarkable reasoning abilitieson complex problems using long Chain-of-Thought (CoT) reasoning. However, theyoften suffer from overthinking, meaning generating unnecessarily lengthyreasoning steps for simpler problems. This issue may degrade the efficiency ofthe models and make them difficult to adapt the reasoning depth to thecomplexity of problems. To address this, we introduce a novel metric TokenEntropy Cumulative Average (TECA), which measures the extent of explorationthroughout the reasoning process. We further propose a novel reasoning paradigm-- Explore Briefly, Then Decide -- with an associated Cumulative EntropyRegulation (CER) mechanism. This paradigm leverages TECA to help the modeldynamically determine the optimal point to conclude its thought process andprovide a final answer, thus achieving efficient reasoning. Experimentalresults across diverse mathematical benchmarks show that our approachsubstantially mitigates overthinking without sacrificing problem-solvingability. With our thinking paradigm, the average response length decreases byup to 71% on simpler datasets, demonstrating the effectiveness of our method increating a more efficient and adaptive reasoning process.
Link: http://arxiv.org/abs/2510.02249v1
Updated: 2025-10-02T17:36:50Z

62: The Unreasonable Effectiveness of Scaling Agents for Computer Use
Authors: ['Gonzalo Gonzalez-Pumariega', 'Vincent Tu', 'Chih-Lun Lee', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang']
Summary: Computer-use agents (CUAs) hold promise for automating everyday digitaltasks, but their unreliability and high variance hinder their application tolong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a methodthat scales over agents by generating multiple rollouts and selecting amongthem using behavior narratives that describe the agents' rollouts. It enablesboth wide exploration and principled trajectory selection, substantiallyimproving robustness and success rates. On OSWorld, our bBoN scaling methodestablishes a new state of the art (SoTA) at 69.9%, significantly outperformingprior methods and approaching human-level performance at 72%, withcomprehensive ablations validating key design choices. We further demonstratestrong generalization results to different operating systems onWindowsAgentArena and AndroidWorld. Crucially, our results highlight theunreasonable effectiveness of scaling CUAs, when you do it right: effectivescaling requires structured trajectory understanding and selection, and bBoNprovides a practical framework to achieve this.
Link: http://arxiv.org/abs/2510.02250v1
Updated: 2025-10-02T17:37:08Z

63: RLAD: Training LLMs to Discover Abstractions for Solving Reasoning  Problems
Authors: ['Yuxiao Qu', 'Anikait Singh', 'Yoonho Lee', 'Amrith Setlur', 'Ruslan Salakhutdinov', 'Chelsea Finn', 'Aviral Kumar']
Summary: Reasoning requires going beyond pattern matching or memorization of solutionsto identify and implement "algorithmic procedures" that can be used to deduceanswers to hard problems. Doing so requires realizing the most relevantprimitives, intermediate results, or shared procedures, and building upon them.While RL post-training on long chains of thought ultimately aims to uncoverthis kind of algorithmic behavior, most reasoning traces learned by largemodels fail to consistently capture or reuse procedures, instead drifting intoverbose and degenerate exploration. To address more effective reasoning, weintroduce reasoning abstractions: concise natural language descriptions ofprocedural and factual knowledge that guide the model toward learningsuccessful reasoning. We train models to be capable of proposing multipleabstractions given a problem, followed by RL that incentivizes building asolution while using the information provided by these abstractions. Thisresults in a two-player RL training paradigm, abbreviated as RLAD, that jointlytrains an abstraction generator and a solution generator. This setupeffectively enables structured exploration, decouples learning signals ofabstraction proposal and solution generation, and improves generalization toharder problems. We also show that allocating more test-time compute togenerating abstractions is more beneficial for performance than generating moresolutions at large test budgets, illustrating the role of abstractions inguiding meaningful exploration.
Link: http://arxiv.org/abs/2510.02263v1
Updated: 2025-10-02T17:44:23Z

64: InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in  Tool-Augmented Agents
Authors: ['Yaxin Du', 'Yuanshuo Zhang', 'Xiyuan Yang', 'Yifan Zhou', 'Cheng Wang', 'Gongyi Zou', 'Xianghe Pang', 'Wenhao Wang', 'Menglan Chen', 'Shuo Tang', 'Zhiyu Li', 'Siheng Chen']
Summary: Information seeking is a fundamental requirement for humans. However,existing LLM agents rely heavily on open-web search, which exposes twofundamental weaknesses: online content is noisy and unreliable, and manyreal-world tasks require precise, domain-specific knowledge unavailable fromthe web. The emergence of the Model Context Protocol (MCP) now allows agents tointerface with thousands of specialized tools, seemingly resolving thislimitation. Yet it remains unclear whether agents can effectively leverage suchtools -- and more importantly, whether they can integrate them withgeneral-purpose search to solve complex tasks. Therefore, we introduceInfoMosaic-Bench, the first benchmark dedicated to multi-source informationseeking in tool-augmented agents. Covering six representative domains(medicine, finance, maps, video, web, and multi-domain integration),InfoMosaic-Bench requires agents to combine general-purpose search withdomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalablepipeline that grounds task conditions in verified tool outputs, enforcescross-source dependencies, and filters out shortcut cases solvable by triviallookup. This design guarantees both reliability and non-triviality. Experimentswith 14 state-of-the-art LLM agents reveal three findings: (i) web informationalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% passrate; (ii) domain tools provide selective but inconsistent benefits, improvingsome domains while degrading others; and (iii) 22.4% of failures arise fromincorrect tool usage or selection, highlighting that current LLMs stillstruggle with even basic tool handling.
Link: http://arxiv.org/abs/2510.02271v1
Updated: 2025-10-02T17:48:03Z

65: Parallel Scaling Law: Unveiling Reasoning Generalization through A  Cross-Linguistic Perspective
Authors: ['Wen Yang', 'Junhong Wu', 'Chong Li', 'Chengqing Zong', 'Jiajun Zhang']
Summary: Recent advancements in Reinforcement Post-Training (RPT) have significantlyenhanced the capabilities of Large Reasoning Models (LRMs), sparking increasedinterest in the generalization of RL-based reasoning. While existing work hasprimarily focused on investigating its generalization across tasks ormodalities, this study proposes a novel cross-linguistic perspective toinvestigate reasoning generalization. This raises a crucial question:$\textit{Does the reasoning capability achieved from English RPT effectivelytransfer to other languages?}$ We address this by systematically evaluatingEnglish-centric LRMs on multilingual reasoning benchmarks and introducing ametric to quantify cross-lingual transferability. Our findings reveal thatcross-lingual transferability varies significantly across initial model, targetlanguage, and training paradigm. Through interventional studies, we find thatmodels with stronger initial English capabilities tend to over-rely onEnglish-specific patterns, leading to diminished cross-lingual generalization.To address this, we conduct a thorough parallel training study. Experimentalresults yield three key findings: $\textbf{First-Parallel Leap}$, a substantialleap in performance when transitioning from monolingual to just a singleparallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealingthat cross-lingual reasoning transfer follows a power-law with the number oftraining parallel languages. Moreover, we identify the discrepancy betweenactual monolingual performance and the power-law prediction as$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMsfail to fully generalize across languages. Our study challenges the assumptionthat LRM reasoning mirrors human cognition, providing critical insights for thedevelopment of more language-agnostic LRMs.
Link: http://arxiv.org/abs/2510.02272v1
Updated: 2025-10-02T17:49:49Z

66: Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming  Attacks
Authors: ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth']
Summary: Despite recent rapid progress in AI safety, current large language modelsremain vulnerable to adversarial attacks in multi-turn interaction settings,where attackers strategically adapt their prompts across conversation turns andpose a more critical yet realistic challenge. Existing approaches that discoversafety vulnerabilities either rely on manual red-teaming with human experts oremploy automated methods using pre-defined templates and human-curated attackdata, with most focusing on single-turn attacks. However, these methods did notexplore the vast space of possible multi-turn attacks, failing to considernovel attack trajectories that emerge from complex dialogue dynamics andstrategic conversation planning. This gap is particularly critical given recentfindings that LLMs exhibit significantly higher vulnerability to multi-turnattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policyreinforcement learning framework integrated with tree search that autonomouslydiscovers diverse multi-turn attack strategies by treating the dialogue as asequential decision-making problem, enabling systematic exploration withoutmanually curated data. Through extensive experiments, our approach not onlyachieves more than 25.9% higher ASR across 10 target models compared toprevious state-of-the-art approaches, but also effectively uncovers new attackstrategies by learning optimal dialogue policies that maximize attack successacross multiple turns.
Link: http://arxiv.org/abs/2510.02286v1
Updated: 2025-10-02T17:57:05Z

67: From Behavioral Performance to Internal Competence: Interpreting  Vision-Language Models with VLM-Lens
Authors: ['Hala Sheta', 'Eric Huang', 'Shuyu Wu', 'Ilia Alenabi', 'Jiajun Hong', 'Ryker Lin', 'Ruoxi Ning', 'Daniel Wei', 'Jialin Yang', 'Jiawei Zhou', 'Ziqiao Ma', 'Freda Shi']
Summary: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,analysis, and interpretation of vision-language models (VLMs) by supporting theextraction of intermediate outputs from any layer during the forward pass ofopen-source VLMs. VLM-Lens provides a unified, YAML-configurable interface thatabstracts away model-specific complexities and supports user-friendly operationacross diverse VLMs. It currently supports 16 state-of-the-art base VLMs andtheir over 30 variants, and is extensible to accommodate new models withoutchanging the core logic.  The toolkit integrates easily with various interpretability and analysismethods. We demonstrate its usage with two simple analytical experiments,revealing systematic differences in the hidden representations of VLMs acrosslayers and target concepts. VLM-Lens is released as an open-sourced project toaccelerate community efforts in understanding and improving VLMs.
Link: http://arxiv.org/abs/2510.02292v1
Updated: 2025-10-02T17:58:41Z

68: F2LLM Technical Report: Matching SOTA Embedding Performance with 6  Million Open-Source Data
Authors: ['Ziyin Zhang', 'Zihan Liao', 'Hang Yu', 'Peng Di', 'Rui Wang']
Summary: We introduce F2LLM - Foundation to Feature Large Language Models, a suite ofstate-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlikeprevious top-ranking embedding models that require massive contrastivepretraining, sophisticated training pipelines, and costly synthetic trainingdata, F2LLM is directly finetuned from foundation models on 6 millionquery-document-negative tuples curated from open-source, non-syntheticdatasets, striking a strong balance between training cost, model size, andembedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2ndamong models with approximately 4B parameters and 7th overall, while F2LLM-1.7Branks 1st among models in the 1B-2B size range. To facilitate future researchin the field, we release the models, training dataset, and code, positioningF2LLM as a strong, reproducible, and budget-friendly baseline for future works.
Link: http://arxiv.org/abs/2510.02294v1
Updated: 2025-10-02T17:58:49Z

69: Interactive Training: Feedback-Driven Neural Network Optimization
Authors: ['Wentao Zhang', 'Yang Young Lu', 'Yuntian Deng']
Summary: Traditional neural network training typically follows fixed, predefinedoptimization recipes, lacking the flexibility to dynamically respond toinstabilities or emerging training issues. In this paper, we introduceInteractive Training, an open-source framework that enables real-time,feedback-driven intervention during neural network training by human experts orautomated AI agents. At its core, Interactive Training uses a control server tomediate communication between users or agents and the ongoing training process,allowing users to dynamically adjust optimizer hyperparameters, training data,and model checkpoints. Through three case studies, we demonstrate thatInteractive Training achieves superior training stability, reduced sensitivityto initial hyperparameters, and improved adaptability to evolving user needs,paving the way toward a future training paradigm where AI agents autonomouslymonitor training logs, proactively resolve instabilities, and optimize trainingdynamics.
Link: http://arxiv.org/abs/2510.02297v1
Updated: 2025-10-02T17:59:00Z

70: Drawing Conclusions from Draws: Rethinking Preference Semantics in  Arena-Style LLM Evaluation
Authors: ['Raphael Tang', 'Crystina Zhang', 'Wenyan Li', 'Carmen Lai', 'Pontus Stenetorp', 'Yao Lu']
Summary: In arena-style evaluation of large language models (LLMs), two LLMs respondto a user query, and the user chooses the winning response or deems the"battle" a draw, resulting in an adjustment to the ratings of both models. Theprevailing approach for modeling these rating dynamics is to view battles astwo-player game matches, as in chess, and apply the Elo rating system and itsderivatives. In this paper, we critically examine this paradigm. Specifically,we question whether a draw genuinely means that the two models are equal andhence whether their ratings should be equalized. Instead, we conjecture thatdraws are more indicative of query difficulty: if the query is too easy, thenboth models are more likely to succeed equally. On three real-world arenadatasets, we show that ignoring rating updates for draws yields a 1-3% relativeincrease in battle outcome prediction accuracy (which includes draws) for allfour rating systems studied. Further analyses suggest that draws occur more forqueries rated as very easy and those as highly objective, with risk ratios of1.37 and 1.35, respectively. We recommend future rating systems to reconsiderexisting draw semantics and to account for query properties in rating updates.
Link: http://arxiv.org/abs/2510.02306v1
Updated: 2025-10-02T17:59:41Z

